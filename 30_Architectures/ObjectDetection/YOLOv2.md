
### **1. 개요 (Overview)**

[[YOLOv2]]는 [[YOLOv1]]의 후속 버전으로, 2016년 Joseph Redmon 등이 "YOLO9000: Better, Faster, Stronger"라는 논문에서 발표했다. [[YOLOv1]]의 단점이었던 낮은 정확도(특히 재현율, Recall)와 위치 부정확성을 개선하는 데 중점을 두었으며, 속도는 유지하면서 성능을 대폭 향상시켰다. 또한, 9000개 이상의 클래스를 동시에 탐지할 수 있는 **YOLO9000** 모델을 함께 제안하여 객체 탐지의 범위를 크게 확장했다.

---

### **2. YOLOv2: Better, Faster**

YOLOv2는 "더 좋고, 더 빠르게" 만들기 위해 다양한 기법들을 실험하고 조합했다.

#### **2.1. Better (정확도 향상 기법)**

1.  **배치 정규화 (Batch Normalization)**
    -   모든 컨볼루션 레이어에 배치 정규화를 적용하여 학습을 안정화하고, 규제(regularization) 효과를 주었다. 이를 통해 mAP가 약 2% 향상되었다.

2.  **고해상도 분류기 (High Resolution Classifier)**
    -   기존에는 224x224 크기의 ImageNet으로 사전 학습한 후, 448x448 크기의 탐지 데이터로 학습했다. YOLOv2에서는 사전 학습 마지막에 448x448 고해상도 ImageNet 이미지로 10 에폭 정도 추가 미세 조정(fine-tuning)을 수행했다. 이를 통해 네트워크가 고해상도 입력에 더 잘 적응하게 하여 mAP를 약 4% 향상시켰다.

3.  **앵커 박스를 사용한 컨볼루션 (Convolutional with Anchor Boxes)**
    -   [[YOLOv1]]이 그리드 셀에서 직접 좌표를 예측하던 방식 대신, [[Faster R-CNN]]처럼 **앵커 박스(Anchor Box)** 개념을 도입했다.
    -   마지막 완전 연결 계층을 제거하고, 컨볼루션 레이어가 특징 맵의 각 위치에서 앵커 박스를 기준으로 오프셋(offset)을 예측하도록 변경했다. 이로 인해 재현율(Recall)이 크게 향상되었다.
    -   또한, 입력 이미지를 448x448에서 416x416으로 변경했다. 이는 다운샘플링(downsampling) 후 최종 특징 맵의 크기를 13x13으로 만들기 위함이다. 특징 맵의 가로, 세로가 홀수이면 중앙에 하나의 셀이 존재하게 되는데, 이는 이미지 중앙에 위치하는 큰 객체를 탐지하는 데 유리하기 때문이다.

4.  **차원 클러스터링 (Dimension Clusters)**
    -   앵커 박스의 크기와 비율을 수동으로 정하는 대신, 학습 데이터셋의 실제 바운딩 박스들을 대상으로 **k-means 클러스터링**을 수행하여 최적의 앵커 박스 형태(prior)를 자동으로 찾아냈다. 이는 더 나은 초기값을 제공하여 학습을 용이하게 했다.

5.  **직접적인 위치 예측 (Direct Location Prediction)**
    -   앵커 박스 오프셋을 예측할 때, [[Faster R-CNN]]의 방식이 학습 초기에 불안정하다고 판단하여 이를 개선했다. 로지스틱 활성화 함수(logistic activation)를 사용하여 예측값이 그리드 셀 경계 내에 머물도록 제한함으로써 학습을 안정화시켰다.

6.  **Passthrough 레이어 (Fine-Grained Features)**
    -   고해상도 특징 맵(더 이전 레이어의 특징 맵)을 가져와 저해상도 특징 맵과 합치는(concatenate) passthrough 레이어를 추가했다. 이를 통해 작은 객체 탐지에 필요한 세밀한 특징(fine-grained features)을 보존하여 성능을 약 1% 향상시켰다.

7.  **다중 스케일 학습 (Multi-Scale Training)**
    -   네트워크가 다양한 입력 이미지 크기에 강건해지도록, 학습 중에 몇 에폭마다 입력 이미지의 크기를 무작위로 변경(예: 320x320, 416x416, 608x608)했다. 이로 인해 단일 모델이 여러 해상도에서 좋은 성능을 낼 수 있게 되었다.

#### **2.2. Faster (속도)**

-   기존의 VGG-16 대신 **Darknet-19**라는 새로운 자체 네트워크를 설계했다. 19개의 컨볼루션 레이어와 5개의 맥스 풀링 레이어로 구성되어 있으며, 연산량을 줄이면서도 높은 성능을 유지했다.

---

### **3. YOLO9000: Stronger**

YOLO9000은 9000개 이상의 방대한 클래스를 탐지하기 위한 모델로, 다음과 같은 아이디어를 사용한다.

1.  **데이터셋 결합 학습 (Joint Training on COCO and ImageNet)**
    -   상대적으로 클래스 수는 적지만 바운딩 박스 레이블이 있는 **탐지 데이터셋(COCO)**과, 클래스 수는 방대하지만 이미지 전체 레이블만 있는 **분류 데이터셋(ImageNet)**을 함께 사용하여 학습한다.
    -   탐지 데이터셋 이미지로는 일반적인 탐지 손실(detection loss)을 계산하고, 분류 데이터셋 이미지로는 분류 손실(classification loss)만 계산하여 역전파한다.

2.  **계층적 분류 (Hierarchical Classification with WordTree)**
    -   ImageNet과 COCO의 클래스 레이블을 **WordNet**을 기반으로 한 계층적 트리 구조(WordTree)로 통합한다.
    -   예를 들어, "노리치 테리어"와 "요크셔 테리어"는 모두 "테리어"의 하위 클래스이고, "테리어"는 "사냥개", "사냥개"는 "개"의 하위 클래스가 된다.
    -   모델은 특정 하위 클래스에 대한 확신이 없더라도, 상위 클래스(예: "개")에 대한 예측은 높은 신뢰도로 수행할 수 있다.

---

### **4. 결론**

YOLOv2는 [[YOLOv1]]의 한계를 극복하고 **정확도와 속도** 모두에서 큰 발전을 이루었다. 특히 앵커 박스, 배치 정규화, 다중 스케일 학습 등 다양한 기법을 성공적으로 도입하여 1-Stage Detector의 성능을 한 단계 끌어올렸다. 더 나아가 YOLO9000은 데이터셋 결합과 계층적 분류라는 독창적인 아이디어로 객체 탐지의 스케일을 수천 개 클래스 단위로 확장하는 가능성을 보여주었다.


# 바운딩 박스(x, y, w, h)의 의미와 차이점

### **공통점: 바운딩 박스를 정의하는 4개의 요소**

모든 객체 탐지 아키텍처에서 `x, y, w, h`는 이미지 내의 사각형 영역, 즉 **바운딩 박스(Bounding Box)**를 정의하기 위한 4개의 요소를 의미한다는 점에서는 동일합니다.

-   `(x, y)`: 박스의 위치를 나타내는 좌표
-   `(w, h)`: 박스의 크기를 나타내는 너비와 높이

### **주요 차이점: 기준과 표현 방식**

하지만 이 값들을 어떻게 해석하고 계산하는지에 대한 세부 방식에서 중요한 차이가 발생합니다.

#### **1. `(x, y)` 좌표의 기준점**

-   **박스의 중심점 (Center Point)**: **YOLO, SSD** 계열의 1-Stage Detector에서 주로 사용합니다. `(x, y)`는 바운딩 박스의 정중앙 좌표를 의미합니다.
    ![[Bbox_center_xy.svg]]

-   **박스의 좌측 상단 꼭짓점 (Top-Left Corner)**: **R-CNN** 계열에서 일부 사용되며, 많은 그래픽 라이브러리에서 사각형을 그리는 표준 방식이기도 합니다. `(x, y)`는 바운딩 박스의 좌측 상단 좌표를 의미합니다.
    ![[Bbox_topleft_xy.svg]]

#### **2. 값의 스케일 (Normalization)**

-   **이미지 전체에 대한 상대값**: **YOLO** 계열은 `x, y, w, h` 값을 전체 이미지의 너비와 높이에 대한 비율(0~1 사이)로 정규화하여 사용합니다. 예를 들어, `x=0.5`는 이미지의 정중앙 x좌표를 의미합니다.
-   **특정 그리드 셀에 대한 상대값**: **YOLO**는 한 단계 더 나아가, 예측을 담당하는 그리드 셀(Grid Cell) 내부에서의 상대적인 위치를 예측하기도 합니다.
-   **절대적인 픽셀 값**: 최종적으로 사람이 보거나 시각화할 때는 절대적인 픽셀 좌표로 변환되지만, 네트워크가 직접 픽셀 값을 예측하는 경우는 드뭅니다.

#### **3. `(w, h)`의 변환 방식 (Transformation)**

-   **직접 예측**: 초기 모델들은 `w`와 `h`를 직접 예측하려 했지만, 크기가 다른 객체들에 대해 학습이 불안정해지는 문제가 있었습니다.
-   **로그 스페이스(Log-space) 변환**: **Faster R-CNN, YOLOv2** 이후 모델들은 너비와 높이에 로그를 씌운 값(`log(w)`, `log(h)`)을 예측합니다. 이는 작은 값의 변화에는 둔감하고 큰 값의 변화에는 민감하게 반응하여 학습을 안정시키는 효과가 있습니다.
-   **앵커 박스(Anchor Box)에 대한 상대적 스케일**: 현재 대부분의 모델(Faster R-CNN, YOLOv2/v3, SSD 등)은 사전에 정의된 **앵커 박스**를 기준으로 상대적인 크기 변화량을 예측합니다. 예를 들어, 예측값 `t_w`를 `anchor_width * exp(t_w)` 와 같은 수식으로 변환하여 최종 너비를 계산합니다.

### **아키텍처별 비교 요약**

| 아키텍처 | `(x, y)`의 의미 | `(w, h)`의 의미 | 기준 대상 |
| :--- | :--- | :--- | :--- |
| **R-CNN** | 좌측 상단 또는 중심 (구현에 따라 다름) | 실제 너비/높이 | 제안된 영역(Region Proposal) |
| **Faster R--CNN** | **앵커 박스** 중심점에서의 **이동량(offset)** | **앵커 박스** 너비/높이의 **로그 스케일 변화량** | 앵커 박스 |
| **YOLOv1** | **그리드 셀** 내부의 상대적 중심 좌표 | **전체 이미지**에 대한 상대적 너비/높이 | 그리드 셀, 전체 이미지 |
| **YOLOv2 / v3** | **그리드 셀** 내부 + **앵커 박스** 기준 중심 좌표 | **앵커 박스** 너비/높이의 **로그 스케일 변화량** | 그리드 셀, 앵커 박스 |

# Phase 1

## **1) 발전 흐름 요약**

- **R-CNN (2014)**
    
    외부 제안(Selective Search)으로 ~2k RoI 생성 → 각 RoI를 잘라 CNN 통과 → SVM 분류 + 별도 회귀기.
    
    장점: CNN을 객체 검출에 도입. 단점: 매우 느리고(수천 RoI × 다중 포워드), 학습/추론 파이프라인이 다단계.
    
- **SPPNet (2014)**
    
    **한 번**의 CNN 합성곱으로 **전역 특징맵** 생성 → RoI를 **SPP(Spatial Pyramid Pooling)** 로 고정 크기 특징으로 변환 → SVM/회귀.
    
    장점: R-CNN 대비 대폭 가속(특징 공유). 단점: 여전히 외부 제안 + 다단계 학습.
    
- **Fast R-CNN (2015)**
    
    전역 특징맵 + **RoI Pooling** → **단일 네트워크**에서 **Softmax 분류 + bbox 회귀**를 **동시에(end-to-end)** 학습.
    
    장점: 학습·추론 모두 크게 가속, 정확도 상승. 단점: **제안은 여전히 Selective Search**(병목).
    
- **Faster R-CNN (2015)**
    
    **RPN(Region Proposal Network)** 으로 제안까지 CNN으로 대체(앵커 기반)하며 **특징 공유**.
    
    장점: 제안 생성까지 GPU에서 end-to-end, 속도/정확도 동시 개선. 단점: 두 단계(RPN→RoI head) 구조 유지.
    
- **Mask R-CNN (2017)**
    
    Faster R-CNN에 **마스크 분지(branch)** 추가 + **RoIAlign**(정수 라운딩 없는 정밀 정렬).
    
    장점: **탐지 + 인스턴스 분할** 동시 수행, 정렬 오차 감소로 정밀도↑. 단점: 분지 추가로 계산량 증가.
    

---

# **2) 핵심 비교 표**

| **항목**       | **R-CNN**            | **SPPNet**                | **Fast R-CNN**      | **Faster R-CNN**     | **Mask R-CNN**             |
| ------------ | -------------------- | ------------------------- | ------------------- | -------------------- | -------------------------- |
| 연도/장소        | 2014                 | 2014                      | 2015 (ICCV)         | 2015 (NeurIPS)       | 2017 (ICCV)                |
| 제안(Proposal) | [[Selective Search]] | Selective Search          | Selective Search    | **RPN (앵커 기반)**      | RPN                        |
| 특징 계산        | **RoI별 CNN 재계산**     | **이미지 1회** CNN → 전역 특징 공유 | 전역 look 특징 공유       | 전역 특징 공유             | 전역 특징 공유(+ FPN 주로 사용)      |
| RoI 추출/정규화   | 잘라서 워핑               | **SPP** (다중 피라미드 풀링)      | **RoI Pooling**     | RoI Pooling          | **RoIAlign** (보간)          |
| 분류기          | **SVM** (별도)         | SVM (별도)                  | **Softmax(내장)**     | Softmax              | Softmax                    |
| BBox 회귀      | 별도 선형 회귀             | 별도                        | **내장(멀티태스크 로스)**    | 내장                   | 내장                         |
| 학습 방식        | 다단계(분리)              | 다단계(분리)                   | **단일 엔드투엔드(제안 제외)** | **엔드투엔드(제안 포함)**     | 엔드투엔드(멀티태스크: cls+box+mask) |
| 속도 병목        | RoI별 CNN             | 외부 제안                     | 외부 제안               | 해소(RPN)              | 일부 증가(마스크 분지)              |
| 대표 기여        | CNN+검출 연결            | 전역 특징 공유(SPP)             | RoI Pooling, 멀티태스크  | **RPN, 앵커, 완전 CNN화** | **RoIAlign, 인스턴스 마스크**     |
| 주요 한계        | 느림, 복잡               | 외부 제안, 다단계                | 제안 병목               | 2단계 구조               | 계산량↑                       |

---

# **3) 각 모델 파이프라인(연산 관점)**

  
## **[[R-CNN]]**

1. Selective Search로 RoI 생성 →
    
2. 각 RoI를 원본에서 크롭·워핑 → CNN 추출 →
    
3. 클래스별 SVM 분류 →
    
4. 클래스별 bbox 회귀(별도 학습) →
    
5. NMS.
    
## **[[SPPNet]]**

1. 전체 이미지 1회 CNN → 전역 특징맵 →
    
2. RoI 좌표로 **SPP** 수행(다단계 피라미드 빈을 통해 고정 길이 벡터) →
    
3. SVM + bbox 회귀(분리) → NMS.
    
    ※ SPP로 **RoI별 재계산 제거**, 속도↑.
    
## **[[30_Architectures/ObjectDetection/Fast R-CNN|Fast R-CNN]]**

1. 전체 이미지 1회 CNN →
    
2. RoI Pooling으로 RoI 특징 벡터화 →
    
3. **하나의 네트워크**에서 **Softmax 분류 + bbox 회귀**를 **공동 학습** → NMS.
    
    ※ 제안만 외부(Selective Search)라 병목 잔존.
## **[[Faster R-CNN]]**

1. 전역 특징맵 공유 →
    
2. **RPN**이 앵커마다 객체성 점수·bbox 오프셋 예측 → 상위 RoI 생성 →
    
3. RoI Pooling → 분류 + bbox 회귀 → NMS.
    
    ※ **제안까지 CNN 내재화**, 속도/정확도 개선.
    
## **[[Mask R-CNN]]**

1. Faster R-CNN 구조 기반 →
    
2. **RoIAlign**으로 정밀 정렬(양자화 제거, bilinear 보간) →
    
3. 기존 **분류+bbox**와 **병렬**로 **마스크(예: 28×28) 예측 분지** 추가(픽셀 단위 시그모이드) →
    
4. NMS(박스), 마스크 후처리.
    
    ※ **박스/마스크 분할**을 깔끔히 분리하여 학습 안정.
    
---

# **4) 설계 포인트의 “왜”**

- **전역 특징 공유(= SPPNet 이후)**: RoI마다 CNN 재계산을 제거하여 **연산 복잡도**와 **메모리 I/O**를 크게 절감.
    
- **SPP → RoI Pooling → RoIAlign**:
    
    - SPP: 다양한 스케일 빈으로 고정 길이 벡터화(정렬 오차는 상대적으로 덜 문제화).
        
    - RoI Pooling: 간단·효율적이나 **정수 좌표 양자화**로 정렬 오차 발생.
        
    - **RoIAlign**: **보간**으로 양자화 제거 → **정밀 위치**가 중요한 마스크/키포인트 등에서 성능↑.
        
    
- **RPN 도입**: 외부 알고리즘(Selective Search) 제거 → **GPU 일관 파이프라인** + **특징 공유**로 성능·속도 동시 개선.
    
- **멀티태스크 학습**: 분류와 회귀(그리고 마스크)를 **동시에** 학습하여 **표현 공유**와 **총합 손실 최적화**.
    

---

## 2.1 [[YOLOv1]]
## 2.2 [[YOLOv2]]
## 2.3 [[YOLOv3]]
---
## Difference with YOLO

### 1. 다중 스케일 특징 맵 (Multi-scale Feature Maps)

가장 핵심적인 차이점입니다. SSD는 객체 탐지를 위해 네트워크의 여러 단계에 있는 **다양한 크기의 특징 맵(feature map)을 모두 사용**합니다. 상대적으로 앞쪽 레이어의 고해상도 특징 맵은 작은 객체를 탐지하는 데 사용되고, 뒤쪽 레이어의 저해상도 특징 맵은 큰 객체를 탐지하는 데 사용됩니다.

반면, YOLO는 네트워크의 **가장 마지막에 있는 단일 특징 맵**에서만 모든 예측을 수행합니다. 이로 인해 SSD는 다양한 크기의 객체를 YOLO보다 더 정확하게 탐지할 수 있습니다. 논문의 그림 2는 이 구조적 차이를 명확하게 보여줍니다.

### 2. 다양한 종횡비의 디폴트 박스 (Default Boxes with Different Aspect Ratios)

SSD는 각 특징 맵의 각 위치마다 크기와 종횡비(aspect ratio)가 다른 여러 개의 '디폴트 박스'(Faster R-CNN의 앵커 박스와 유사)를 미리 정의해두고, 이 박스들을 기준으로 객체의 위치와 크기를 미세 조정합니다.

반면, 논문에서 비교된 YOLO 버전은 이러한 다양한 형태의 사전 정의된 박스를 여러 스케일에서 사용하지 않고, 최종 특징 맵에서 직접 바운딩 박스를 예측합니다. 이로 인해 SSD가 다양한 형태의 객체에 더 유연하게 대응할 수 있습니다.

### 3. 성능 (정확도 및 속도)

논문에 따르면 SSD는 YOLO에 비해 **정확도(mAP)와 속도(FPS) 모두에서 뛰어난 성능**을 보입니다. PASCAL VOC2007 데이터셋 기준으로 다음과 같이 비교합니다 (논문 2페이지).

- **SSD (300x300 입력):** 74.3% mAP, 59 FPS
- **YOLO:** 63.4% mAP, 45 FPS

이러한 성능 향상은 위에서 언급한 다중 스케일 특징 맵과 다양한 디폴트 박스를 사용한 덕분이라고 논문은 설명합니다.

### 4. 네트워크 구조

SSD는 VGG-16과 같은 표준 분류 네트워크를 기반으로 하고, 그 위에 예측을 위한 추가적인 컨볼루션 레이어(convolutional layer)들을 쌓는 구조입니다. 즉, 네트워크의 예측 부분이 완전히 컨볼루션으로 이루어져 있습니다.

반면, YOLO는 예측을 위해 마지막에 완전 연결 레이어(fully connected layer)를 사용합니다. 논문의 4페이지에서는 "YOLO는 이 단계(예측 단계)를 위해 컨볼루션 필터 대신 중간에 완전 연결 레이어를 사용한다"고 언급하며 구조적 차이를 설명합니다.

요약하자면, **SSD는 다양한 크기의 특징 맵과 다양한 형태의 디폴트 박스를 활용하여 단일 특징 맵만 사용하는 YOLO에 비해 더 높은 정확도와 속도를 달성한 것이 가장 큰 차이점**입니다.

## 3.1 [[SSD]]
## 3.2 [[DSSD]]

---

## 4.1 [[Feature Pyramid Network]]
## 4.2 M2Det
---

## 5.1 R-FCN
## 5.2 Cascade R-CNN
---

## 6.1 OHEM
## 6.2 DCN
## 6.3 RetinaNet
---

## 7.1 [[OverFeat]]
## 7.2 RefineDet
---
title: "Two-Stream Network"
tags: [architecture, video, action-recognition]
---
# Two-Stream Network

## 한줄 요약
RGB 기반 공간 스트림과 Optical Flow 기반 시간 스트림을 병렬로 학습하고 결합해 행동을 인식하는 비디오 인식 기본 구조.

## 핵심 아이디어
- 행동은 외형(appearance)과 움직임(motion)으로 구성됨
- 단일 2D-CNN만으로는 움직임 단서를 충분히 표현하기 어려움
- 공간 스트림과 시간 스트림을 분리 학습하고 후단에서 결합해 성능을 높임

## 아키텍처 구성
- Spatial stream: RGB 프레임 입력, 2D-CNN으로 정적 외형 특징 추출
- Temporal stream: 연속 프레임의 dense optical flow를 스택해 입력, 2D-CNN으로 움직임 특징 추출
	기존의 방식(예: RGB 프레임만 쌓아서 넣는 방식)은 신경망이 학습 과정에서 스스로 "픽셀이 어떻게 움직이는지"에 대한 모션 정보를 **암묵적으로(implicitly)** 찾아내야 했습니다. 이는 신경망 입장에서 매우 어려운 작업
	프레임 간의 변위(displacement) 정보를 미리 계산하여 신경망의 입력으로 직접 투입. 이렇게 하면 모션 정보가 **명시적으로(explicitly)** 주어지기 때문에, 신경망이 굳이 복잡한 연산을 통해 모션을 추정할 필요가 없어지고, 결과적으로 동작 인식(Action Recognition)이 훨씬 수월
- Fusion: late fusion(점수 평균/가중 합), mid-level(특징 결합), early fusion(입력 결합) 등

## 입력과 전처리
- 프레임 샘플링: 고정 길이 클립 또는 TSN처럼 구간별 샘플링
- Optical flow: 보통 TV-L1이나 Farneback 사용, x/y 성분을 L프레임 스택해 2L 채널 구성
- 정규화: 스트림별 평균/분산 정규화, 흐름 값 클리핑으로 안정화

![[스크린샷 2025-12-28 오후 5.50.53.png]]
![[스크린샷 2025-12-28 오후 5.51.27.png]]
## Bi-directional optical flow
시간적(temporal) 정보를 더 풍부하게 표현하는 방법을 설명하고 있습니다.

1.  **기존의 순방향(Forward) 방식:**
    *   기존 방식은 프레임 $t$의 픽셀이 다음 프레임 $t+1$의 어느 위치로 이동하는지를 나타내는 변위 필드(displacement field) $d_t$를 사용합니다.
    *   즉, 시간의 흐름에 따라 미래 방향으로의 움직임만 고려합니다.

2.  **양방향(Bi-directional) 확장:**
    *   저자들은 미래로 향하는 흐름뿐만 아니라, **반대 방향(역방향)**으로의 변위 필드를 추가로 계산하여 확장하는 것을 제안합니다.
    *   이는 특정 시점을 기준으로 **이전 프레임들(과거)**과 **이후 프레임들(미래)**의 움직임 정보를 모두 활용하겠다는 의미입니다.

3.  **입력 볼륨 $I_\tau$ 구성 방법:**
    *   총 $L$개의 프레임을 쌓는다고 가정할 때, 이를 반으로 나누어 구성합니다.
    *   **미래 방향:** 기준 프레임 $\tau$에서 $\tau + L/2$까지의 순방향 흐름 $L/2$개.
    *   **과거 방향:** 이전 프레임 $\tau - L/2$에서 $\tau$까지의 역방향 흐름 $L/2$개.
    *   이 두 그룹을 합쳐서 입력 볼륨을 만듭니다.

4.  **채널 수 유지:**
    *   이렇게 앞뒤로 나누어 구성하더라도, 전체 채널의 수는 여전히 $2L$개(x, y 성분 포함)로 유지됩니다. 따라서 네트워크 구조를 크게 변경하지 않고도 적용할 수 있습니다.

결론적으로, 이 방식은 단순히 시간 순서대로 $L$개의 프레임을 보는 대신, **특정 시점 $\tau$를 중심으로 과거와 미래의 움직임을 동시에 고려**하여 행동 인식의 정확도를 높이려는 시도



## **Mean flow subtraction**
'카메라 움직임(Global Motion)'이라는 잡음을 제거하여 '행동(Action)'을 더 뚜렷하게 만들기 위한 과정

### 1. 카메라 움직임의 문제점 (Dominated by Camera Motion)
비디오 촬영 시 카메라는 가만히 있지 않고 팬(pan), 틸트(tilt), 줌(zoom) 등을 합니다.
*   **상황:** 만약 카메라가 왼쪽으로 빠르게 이동하면, 화면 속의 모든 물체(배경 포함)는 오른쪽으로 이동하는 것처럼 Optical Flow가 잡힙니다.
*   **문제:** 이렇게 되면 사람의 팔다리가 움직이는 작은 동작(Local Motion)은 카메라가 만들어낸 거대한 전체 움직임(Global Motion)에 묻혀버립니다. 신경망 입장에서는 이것이 '달리기'인지 단순히 '카메라가 옆으로 가는 것'인지 구분하기 어려워집니다.

### 2. Mean Flow Subtraction의 역할
이 기법은 전체 Optical Flow 벡터들의 **평균(Mean)**을 구해서, 모든 픽셀의 움직임 값에서 그 평균을 빼주는 것입니다.

*   **원리:** 일반적으로 화면의 대부분은 배경입니다. 따라서 전체 움직임의 평균은 '카메라의 움직임'과 거의 유사합니다.
*   **효과:** 평균값을 빼주면,
    *   **배경(카메라 움직임):** 움직임 값이 거의 0에 가까워져서 정지한 것처럼 보정됩니다.
    *   **행동(Action):** 배경과 다르게 움직이는 사람의 동작은 평균값과 다르므로, 뺄셈 후에도 값이 살아남아 오히려 더 도드라지게 됩니다.

### 3. 요약
질문하신 "보상 작용으로 인해 움직임이 소멸되는 것"은 **우리가 원치 않는 카메라의 움직임(노이즈)** 이 소멸되는 것이며, 이는 **우리가 원하는 행동(신호)** 을 더 잘 감지하기 위해 **극복하는 방법(Solution)** 으로 사용

논문에서도 언급하듯이, 이는 복잡한 Global Motion estimation 알고리즘(등에서 사용하는 방식)에 비하면 훨씬 단순한 접근(simpler approach)이지만, 딥러닝 모델이 입력 데이터의 비선형성(Rectification non-linearities)을 더 잘 활용하게 돕고(Zero-centering), 결과적으로 성능 향상에 도움을 준다는 점을 강조
## Multi-task Learning
### 1. 핵심 요약 및 설명

**"데이터가 부족한 비디오 학습 환경에서 어떻게 효율적으로 모델을 학습시킬 것인가?"** 에 대한 해결책

#### 1) 문제점: 데이터 부족과 과적합
*   **Spatial Stream:** 이미지 기반이므로 ImageNet과 같은 거대 데이터셋(수백만 장)으로 사전 학습이 쉬워 성능 확보가 용이합니다.
*   **Temporal Stream:** 움직임(Optical Flow)을 학습해야 하므로 비디오 데이터가 필수적입니다. 하지만 논문 작성 시점의 데이터셋(UCF-101, HMDB-51)은 크기가 매우 작아(수천 개 수준), 모델이 데이터를 단순히 외워버리는 **과적합(Overfitting)** 이 발생하기 쉽습니다.

#### 2) 단순 결합의 한계
*   두 데이터셋을 단순히 섞어서 하나의 큰 데이터셋으로 만들면 좋겠지만, 두 데이터셋 간에 중복되는 행동(예: '양치하기'가 둘 다 존재)이 있거나 라벨링 기준이 달라 단순 병합이 어렵습니다.

#### 3) 해결책: 멀티태스크 러닝 (Multi-task Learning)
저자들은 두 데이터셋을 물리적으로 합치는 대신, **네트워크 구조를 변경**하여 두 데이터셋을 동시에 학습하는 방식을 택했습니다.

*   **공유되는 부분 (Shared Representation):** 네트워크의 대부분(Convolution 층 ~ 마지막 Fully-connected 층 직전까지)은 두 데이터셋이 공유합니다. 이를 통해 네트워크는 특정 데이터셋에 국한되지 않는 **보편적인 동작 특징(feature)** 을 배울 수 있습니다.
*   **분리되는 부분 (Task-specific Heads):** 네트워크의 맨 끝단만 두 갈래로 나눕니다.
    *   **Head A:** HMDB-51의 51개 클래스를 분류
    *   **Head B:** UCF-101의 101개 클래스를 분류
*   **학습 과정:**
    *   HMDB-51 데이터가 들어오면 Head A를 통해 오차를 계산하고 역전파합니다.
    *   UCF-101 데이터가 들어오면 Head B를 통해 오차를 계산하고 역전파합니다.
    *   이 과정에서 **공유되는 레이어(Shared Layers)** 들은 두 데이터셋의 정보를 모두 활용하여 업데이트되므로, 결과적으로 더 많은 데이터를 학습한 효과를 얻습니다.

#### 4) 효과
*   **정규화(Regularization):** 더 많은 데이터를 보게 되므로 특정 데이터셋의 노이즈에 과적합되는 것을 막아줍니다.
*   **성능 향상:** 결과적으로 더 강건한(robust) 특징을 학습하여 두 데이터셋 모두에서(특히 데이터가 더 적은 HMDB-51에서) 성능이 향상됩니다.
## 학습과 추론
- 스트림별로 독립 학습 후 결합하는 방식이 일반적
- Spatial stream은 ImageNet 사전학습 가중치 활용 가능
- Temporal stream은 처음부터 학습하거나 RGB 가중치를 흐름 채널에 복제해 초기화
- 추론 시 다중 크롭과 다중 클립 평균으로 안정적 성능 확보

## Implementation details
공간 및 시간 스트림 모두에 대해 CNN-M-2048 아키텍처를 기반으로 한 ConvNet 설정을 사용
모든 은닉층에는 ReLU 활성화 함수가 적용
$3 \times 3$ 공간 윈도우와 스트라이드 2를 사용하는 맥스 풀링(max-pooling)이 포함

### 공간 네트워크와 시간 네트워크의 주요 차이점
시간 네트워크의 경우 메모리 사용량을 줄이기 위해 두 번째 정규화(normalization) 레이어를 제거했다는 점입니다. 
학습 과정에서는 모멘텀이 0.9인 미니 배치 확률적 경사 하강법(SGD)을 사용
공간 네트워크는 전체 프레임에서 무작위로 $224 \times 224$ 영역을 자르거나 RGB 지터링(jittering)을 수행하는 방식의 데이터 증강을 적용
시간 네트워크는 $224 \times 224 \times 2L$ 크기의 입력 볼륨을 무작위로 자르는 방식을 사용
학습률은 초기 $10^{-2}$에서 시작하여 단계적으로 $10^{-3}$, $10^{-4}$로 감소시키는 스케줄
테스트 시에는 비디오당 25개의 프레임을 샘플링하고 각 프레임에서 10개의 입력(4개의 코너와 중앙, 그리고 각각의 좌우 반전)을 추출하여 얻은 점수를 평균 내어 최종 분류 점수를 계산
계산 효율성을 위해 4개의 GPU를 병렬로 사용
광학 흐름(optical flow)은 미리 계산하여 JPEG로 압축해 저장하는 방식

## Evaluation
UCF-101과 HMDB-51 데이터셋을 사용하여 모델의 성능을 검증
공간 ConvNet의 경우, ImageNet 데이터로 사전 학습(pre-training)을 수행한 후 미세 조정(fine-tuning)을 하는 것이 처음부터 학습하는 것보다 훨씬 높은 성능을 보임
시간 ConvNet 실험에서는 여러 프레임의 광학 흐름을 쌓아서(stacking) 입력으로 사용하는 것이 단일 프레임보다 훨씬 유리하다는 것을 확인(특히 10개 프레임)
입력 데이터의 평균을 뺀(mean subtraction) 처리가 글로벌 모션의 영향을 줄여 성능 향상에 도움
.데이터가 적은 HMDB-51의 경우, UCF-101과 함께 멀티태스크 학습(multi-task learning)을 적용했을 때 과적합을 방지하고 성능을 가장 크게 높임

최종적으로 공간 스트림과 시간 스트림을 결합했을 때 각각의 단일 스트림보다 성능이 크게 향상되어 두 정보가 상호 보완적임을 입증
융합 방식으로는 평균(averaging)보다 SVM을 사용하는 것이 더 나은 결과
Two-Stream 아키텍처는 당시의 다른 딥러닝 기반 방법론들을 큰 차이로 앞서고 최신 핸드크래프트(hand-crafted) 기능 기반 모델들과 대등한 성능(UCF-101 기준 88.0%)을 기록



## 대표 변형
- Two-Stream CNN (Simonyan and Zisserman, 2014)
- Temporal Segment Networks(TSN): 구간 샘플링 + 합의(consensus) 결합
- Two-Stream I3D: 3D-CNN을 두 스트림으로 확장해 성능 강화
- 압축 비디오 기반 스트림: optical flow 대신 motion vector로 비용 절감

## 장점
- 공간/시간 정보를 분리해 학습하므로 해석과 디버깅이 쉬움
- optical flow 기반 시간 스트림이 성능을 크게 끌어올리는 경우가 많음
- 모듈식 구조라 다른 백본으로 쉽게 교체 가능

## 한계
- optical flow 계산 비용이 매우 큼
- 스트림 간 정합 문제와 late fusion의 정보 손실 가능성
- 긴 시간 의존성을 충분히 모델링하기 어려움

## 실무 팁과 주의점
- flow는 미리 계산해 캐시하고, 스토리지/입출력 병목을 고려
- 긴 영상은 TSN처럼 구간 샘플링을 사용해 커버리지 확보
- 스트림 가중치(예: 1:1, 1:2)를 검증셋에서 튜닝
- 계산 제약이 크면 motion vector나 RGB 차분(RGB diff)으로 근사

## 관련 노트
- [[Histogram of Oriented Gradients]]
- [[VAR Based on DL|Video Action Recognition]]
- [[Temporal Segment Networks]]
- [[Optical Flow]]
- [[3D-CNN]]
- [[Transformer]]

---
title: "I3D (Inflated 3D ConvNet)"
tags: [paper, architecture, video, action-recognition, spatiotemporal]
year: 2017
venue: "CVPR"
---
# I3D (Inflated 3D ConvNet)

## 한줄 요약
2D CNN 가중치를 3D로 "인플레이트"해 비디오 시공간 특징을 학습하는 3D-CNN 계열의 대표 모델.

## 핵심 아이디어
- 2D Conv 커널을 시간 축으로 확장해 3D Conv로 변환
- ImageNet 사전학습 가중치를 활용해 안정적으로 비디오 학습
- RGB와 Optical Flow의 두-스트림 구조로 성능을 강화

## 아키텍처 개요
- 백본: Inception-v1 기반을 3D로 확장
- 입력: 짧은 클립(예: 64프레임) 단위
- 출력: 클립 단위 분류 후 평균/합의로 영상 단위 예측

제공해주신 텍스트는 I3D 모델이 시간(Time)과 공간(Space) 정보를 처리할 때, '수용 영역(Receptive Field)'의 크기를 어떻게 조절했는지에 대한 핵심적인 내용을 담고 있습니다.

먼저 해당 단락을 한국어로 번역해 드린 후, 내용을 풀어서 설명해 드리겠습니다.

### **공간, 시간 및 네트워크 깊이에 따른 수용 영역(Receptive Field) 확장 속도 조절**

이 단락은 2D 이미지 분류 모델을 3D 비디오 분류 모델로 확장할 때 **시간 차원(Temporal Dimension)을 어떻게 다룰 것인가**에 대한 고민과 해결책을 설명하고 있습니다.

핵심은 **수용 영역(Receptive Field)의 균형**입니다. 수용 영역이란 네트워크의 특정 뉴런이 보고 있는 입력 데이터의 범위를 말합니다. 일반적인 이미지(2D) 모델에서는 가로와 세로의 성질이 비슷하기 때문에 수용 영역을 가로세로 똑같이 키워도 문제가 없습니다. 하지만 비디오(3D)에서는 '공간(이미지)'과 '시간(프레임의 흐름)'이 물리적으로 전혀 다른 성질을 가집니다.
비디오 데이터는 보통 가로($H$) $\times$ 세로($W$) $\times$ 시간($T$) 형태의 3차원 데이터로 취급되지만, **공간축($H, W$)과 시간축($T$)은 본질적으로 다른 정보를 담고 있습니다.**

- **공간(Space)의 성질: "형태와 구조"**
    
    - **정적인 정보:** 공간 정보는 "무엇(What)"이 있는지를 정의합니다. 물체의 모양, 색상, 질감, 테두리(Edge) 등이 여기에 해당합니다.
    - **등방성(Isotropy):** 이미지 내에서 왼쪽으로 10픽셀 이동하는 것과 위로 10픽셀 이동하는 것은 물리적으로 비슷한 "위치 이동"입니다. 데이터의 밀도나 상관관계가 상하좌우로 비슷합니다.
    - **상관관계:** 바로 옆 픽셀은 나와 비슷한 색일 확률이 매우 높습니다.
- **시간(Time)의 성질: "변화와 인과"**
    
    - **동적인 정보:** 시간 정보는 "어떻게(How)" 변하는지를 정의합니다. 움직임, 속도, 변형, 그리고 사건의 순서(인과관계)가 여기에 해당합니다.
    - **비등방성(Anisotropy):** 시간은 한 방향으로 흐릅니다. 1초 전과 1초 후는 단순히 위치가 다른 게 아니라 "사건의 전후"가 다릅니다.
    - **프레임 레이트:** 공간 해상도(예: 1920x1080)에 비해 시간 해상도(예: 30fps)는 데이터의 밀도가 훨씬 낮고 성격이 다릅니다.

저자들은 시간축의 수용 영역이 커지는 속도가 중요하다고 강조합니다. 만약 시간축 수용 영역이 너무 빨리 커지면(즉, 시간 정보를 너무 빨리 압축해 버리면), 서로 다른 물체의 움직임 정보가 섞여버려서 초기 단계에서 물체의 윤곽이나 특징을 제대로 잡아내지 못하는 문제가 발생합니다. 

시간축 수용 영역이 빨리 커진다는 것은, 네트워크 초반부에서 시간 차원의 정보를 빠르게 압축(Pooling)해 버린다는 뜻입니다. 예를 들어, 연속된 5개의 프레임을 1개의 정보로 뭉뚱그려 버리는 것과 같습니다.

- **"장노출 사진" 효과 (Long-exposure Effect):** 카메라 셔터를 오래 열어두고 움직이는 사람을 찍으면 심령사진처럼 흐릿하게 나오는 것을 상상해 보세요.
    - 네트워크가 초반부터 시간 정보를 합쳐버리면, $t=1$초에 있던 팔의 위치와 $t=2$초에 있던 팔의 위치가 하나의 데이터 안에서 뒤섞여 버립니다.
- **엣지(Edge)의 붕괴:** 논문에서 언급한 "conflate edges(가장자리가 융합됨)"가 바로 이 문제입니다. 물체를 인식하려면 선명한 테두리가 필요합니다. 그런데 움직이는 물체의 시간을 너무 빨리 합쳐버리면, 물체의 윤곽선이 겹치고 뭉개져서 **"이것이 팔인지 다리인지" 혹은 "물체가 배경과 어디서 구분되는지"** 형체를 알아볼 수 없게 됩니다.

**결과:** 초기 특징 추출(Early Feature Detection) 단계에서 물체의 정확한 모양과 위치를 놓치게 됩니다.

반대로 너무 천천히 커지면, 비디오 전체의 움직임 흐름(Dynamic)을 파악하지 못하게 됩니다.

시간축 수용 영역이 천천히 커진다는 것은, 네트워크가 깊어졌는데도 여전히 아주 짧은 순간(예: 1~2 프레임)만 따로따로 보고 있다는 뜻입니다.

- **"터널 시야" 또는 "키홀(Keyhole)" 효과:** 영화를 볼 때 전체 화면을 보지 못하고, 바늘구멍만 한 구멍으로 매 순간의 장면만 본다고 상상해 보세요.
    - 지금 이 순간 손이 위에 있다는 건 알 수 있지만, 이 손이 "올라가는 중"인지 "내려가는 중"인지, 혹은 "공을 던지려고 하는지" 알 수 없습니다.
- **문맥(Context)의 부재:** '앉기(Sitting down)'라는 행동을 인식하려면 서 있음 $\to$ 엉덩이를 내림 $\to$ 의자에 착석이라는 **일련의 과정 전체**를 한 번에 조망해야 합니다.
    - 수용 영역이 작으면 네트워크는 "엉덩이를 내리는 찰나"만 봅니다. 이게 스쿼트 운동을 하는 건지, 의자에 앉는 건지 구별할 수 없습니다.

**결과:** 전체적인 장면의 역동성(Dynamics)이나 행동의 의도를 파악하지 못하고, 단편적인 이미지들의 나열로만 인식하게 됩니다.

이 논문의 저자들은 이 딜레마를 해결하기 위해 **"처음엔 천천히, 나중엔 확실하게"** 전략을 썼습니다.

1. **네트워크 앞단 (초기):** 시간축 풀링을 하지 않습니다(Stride 1). $\to$ **"일단 흐릿해지지 않게 각 프레임의 물체 모양부터 선명하게 따내자."** (장노출 방지)
2. **네트워크 뒷단 (후기):** 시간축 풀링을 수행합니다. $\to$ **"모양은 다 따냈으니, 이제 이 프레임들을 합쳐서 전체적인 움직임의 흐름을 보자."** (문맥 파악)

이렇게 함으로써 물체의 디테일(공간 정보)을 훼손하지 않으면서도 행동의 전체적인 흐름(시간 정보)을 효과적으로 학습할 수 있게 된 것입니다.

Inception-v1 모델을 3D로 확장(Inflate)할 때, 네트워크의 앞부분(초기 레이어)에서는 시간 정보를 압축하지 않도록 설계했습니다. 구체적으로는 처음 두 개의 맥스 풀링 레이어에서 커널 사이즈를 `3x3x3`이 아닌 `1x3x3`으로 설정하고, 시간축 스트라이드(stride)를 1로 설정했습니다. 이렇게 하면 공간 정보는 줄어들지만 시간 정보(프레임 수)는 유지됩니다.

결과적으로 이 설계는 네트워크 초반부에서는 시간적인 해상도를 유지하여 디테일한 움직임을 놓치지 않으면서, 네트워크 후반부로 갈수록 시간 정보를 통합하여 전체적인 행동(Action)을 인식할 수 있게 만드는 전략입니다. 이는 I3D 모델이 단순히 2D 모델을 3D로 부풀린 것이 아니라, 영상 데이터의 특성에 맞춰 세밀하게 튜닝되었음을 보여주는 부분입니다.

## 학습 전략
-  2D 가중치를 3D로 복제해 초기화
- 대규모 비디오 데이터셋(Kinetics 등)으로 사전학습
- RGB/Flow 스트림을 각각 학습 후 late fusion

### **2D 필터에서 3D 필터 부트스트래핑(Bootstrapping)하기**

비디오 데이터셋은 이미지 데이터셋(ImageNet)에 비해 규모가 작기 때문에, 3D 모델을 바닥부터(from scratch) 학습시키면 성능이 잘 나오지 않거나 과적합(Overfitting)되기 쉬움.
-> 따라서 저자들은 2D 모델의 가중치를 3D로 확장(Inflate)하여 초기화하는 방법을 제안

**'지루한 비디오(Boring Video)'의 개념**
저자들은 정지된 이미지 한 장을 시간 축으로 계속 복사하여 만든 비디오를 '지루한 비디오'라고 부릅니다. 논리적으로 생각했을 때, 2D 모델이 '고양이 사진'을 보고 "고양이"라고 판단했다면, 이 사진을 10초 동안 보여주는 3D 모델(비디오 모델)도 똑같이 "고양이"라고 판단해야 합니다. 이 논리적 일치성을 **'지루한 비디오 고정점(Boring-video fixed point)'** 이라고 정의합니다.

**가중치 확장(Inflation) 방법**
기존 2D 필터의 크기가 $N \times N$이라고 할 때, 이를 3D 필터인 $N \times N \times N$으로 확장해야 합니다. 저자들은 단순히 2D 필터의 무늬(가중치)를 시간 축으로 $N$번 복사하여 쌓습니다. 하지만 단순히 복사만 해서 더하면, 시간 축으로 값이 $N$번 더해지므로 출력값이 $N$배 커지게 됩니다. 따라서, 필터의 값을 $N$으로 나누어 줍니다(Rescaling).

**결과**
이렇게 초기화하면, 3D 모델은 학습을 시작하는 시점에 2D 모델(ImageNet으로 학습된 성능 좋은 모델)과 수학적으로 완전히 동일한 출력을 내게 됩니다. 즉, 이미지를 아주 잘 분류하는 상태에서 비디오의 '움직임(시간적 특성)'을 학습하기 시작할 수 있게 되는 것입니다. 이는 I3D 모델이 적은 비디오 데이터로도 높은 성능을 낼 수 있는 핵심적인 이유입니다.

## 장점
- 3D-CNN의 강한 시공간 표현력
- 2D 사전학습 가중치 활용으로 수렴 안정화
- Two-Stream 구조로 성능 상승

## 한계
- 높은 연산/메모리 비용
- 긴 시간 의존성은 여전히 제한적
- Optical Flow 스트림은 전처리 비용이 큼

## 활용 포인트
- 비디오 분류/행동 인식의 강력한 베이스라인
- I3D 특징을 다른 모듈(RNN/Transformer)과 결합 가능

## 관련 노트
- [[3D-CNN]]
- [[Two-Stream Network]]
- [[Optical Flow]]
- [[20_Tasks/VideoUnderstanding/Abstract/Video Action Recognition]]

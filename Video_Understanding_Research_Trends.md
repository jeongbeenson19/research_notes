# Video Understanding 분야 핵심 연구 동향 요약

이 문서는 제공된 `20_Tasks/VideoUnderstanding` 폴더 내의 자료들을 종합하여 비디오 이해(Video Understanding) 분야의 주요 연구 흐름을 시대별로 정리한 것입니다.

---

## 1. 초기 딥러닝 시대 (2014-2018): 시공간 특징 학습의 정립

이 시기 연구의 핵심은 이미지에서 성공을 거둔 딥러닝 모델을 어떻게 비디오의 '시간' 차원에 적용할 것인가에 있었습니다.

- **Two-Stream Networks의 등장**: 비디오를 정지 이미지(Spatial Stream)와 움직임 정보(Temporal Stream, Optical Flow)로 나누어 처리하는 **Two-Stream ConvNets** 방식이 표준으로 자리 잡았습니다. 이는 공간적 정보와 시간적 정보를 분리하여 학습하는 패러다임을 제시했습니다.
- **3D-CNN의 발전**: C3D, I3D와 같은 3D Convolutional Neural Networks가 등장하여, 2D 필터를 시간 축으로 확장함으로써 비디오의 시공간적 특징을 한 번에 학습하려는 시도가 주를 이루었습니다.
- **효율적 시공간 모델링**: **SlowFast** 네트워크는 이름처럼 공간적 변화가 적은 'Slow' 경로와 시간적 변화가 빠른 'Fast' 경로를 병렬로 두어, 연산 효율성과 성능을 동시에 잡는 혁신적인 아키텍처를 제안했습니다.

> **요약**: 정지 이미지 모델을 비디오에 적용하는 단계를 넘어, '움직임'과 '시간'을 명시적으로 다루는 딥러닝 아키텍처가 정립된 시기입니다.

---

## 2. 트랜스포머와 자기지도 학습의 부상 (2018-2022): 대규모 사전학습의 시대

이 시기에는 자연어 처리 분야에서 시작된 트랜스포머(Transformer)와 대규모 데이터셋을 활용한 자기지도 학습(Self-Supervised Learning)이 비디오 분야에도 큰 영향을 미쳤습니다.

- **트랜스포머의 도입**: **ActionFormer**, **TransReID**, **MOTR** 등 다양한 태스크에서 어텐션 메커니즘을 활용해 비디오 프레임 간의 장거리 의존성(Long-range Dependency)을 모델링하는 트랜스포머 기반 모델들이 SOTA(State-of-the-Art) 성능을 달성하기 시작했습니다.
- **Vision-Language Pre-training (VLP)의 혁신**: OpenAI의 **CLIP** 모델이 등장하며, 이미지-텍스트 쌍으로 학습된 모델이 비디오 분야로 확장되었습니다. **CLIP4Clip**과 같은 연구는 CLIP을 비디오 검색(Retrieval)의 표준 프레임워크로 만들었고, 이는 Zero-shot 학습의 가능성을 열었습니다.
- **자기지도 학습의 효율성 증명**: 라벨이 없는 대규모 데이터에서 스스로 특징을 학습하는 자기지도 방식이 주목받았습니다. 특히 **VideoMAE**는 비디오의 높은 중복성을 이용해 90% 이상의 패치를 마스킹하고 복원하는 방식으로 강력한 시공간 표현을 학습하여 사전 학습의 효율성을 획기적으로 높였습니다.

> **요약**: 모델이 비디오의 전역적인 문맥을 이해하기 시작했으며, 라벨링 비용 없이도 대규모 데이터를 활용해 범용적인 시각 표현을 학습하는 패러다임이 정착되었습니다.

---

## 3. 거대 멀티모달 모델(MLLM)과 파운데이션 모델 시대 (2023-현재): 추론과 통합의 시대

최근 2-3년은 LLM(거대 언어 모델)과 SAM(Segment Anything Model)이라는 두 개의 게임 체인저가 등장하며 비디오 이해 기술이 '인식(Perception)'을 넘어 '인지(Cognition)'와 '추론(Reasoning)'의 영역으로 확장된 시기입니다.

- **LLM의 통합**: **Video-ChatGPT**, **LLMTrack**, **VideoChat** 등 LLM을 '두뇌'로 활용하여 비디오의 내용을 분석하고, 복잡한 질의에 답하며, 시맨틱한 의미를 바탕으로 객체를 추적하는 연구가 폭발적으로 증가했습니다. 이는 비디오를 단순 픽셀 덩어리가 아닌, 논리적 추론이 가능한 정보의 원천으로 다루게 만들었습니다.
- **SAM의 혁명**: 메타(Meta)가 발표한 **SAM(Segment Anything Model)** 과 그 후속작 **SAM 2**는 비디오의 픽셀 단위 이해 방식에 혁명을 가져왔습니다. **SAM2MOT**, **ReferDINO**, **VoCap** 등의 연구는 SAM을 기반으로 비디오 객체 분할(VOS), 인스턴스 분할(VIS), 참조 분할(RVOS) 등의 태스크에서 이전과는 차원이 다른 정교함과 제로샷 성능을 보여주었습니다.
- **비디오 파운데이션 모델의 등장**: **InternVideo** 시리즈와 같은 대규모 비디오 전용 파운데이션 모델이 등장하여, 검색, 셔플, 인식, 대화 등 다양한 비디오 관련 태스크를 하나의 모델로 처리하려는 시도가 이루어지고 있습니다.
- **멀티모달리티의 심화**: 시각과 텍스트의 결합을 넘어, 오디오(**DAVE**), 모션 등 다양한 정보를 통합하여 비디오를 입체적으로 이해하는 연구가 필수가 되었습니다.

> **요약**: LLM과 SAM을 양대 축으로, 기존의 개별 태스크들을 하나의 거대 모델 안에서 통합적으로 해결하고, 인간 수준의 추론 능력을 부여하려는 시도가 현재 연구의 주류를 이루고 있습니다.

---

## 4. 현재의 핵심 도전 과제 및 미래 방향

현재 비디오 이해 연구는 다음과 같은 도전 과제를 해결하는 데 집중하고 있습니다.

- **장편 비디오 처리 및 효율성 (Long-Form Video & Efficiency)**: 수 분에서 수 시간에 이르는 긴 비디오를 처리하기 위해, 트랜스포머의 계산 복잡도를 해결할 수 있는 **Mamba**와 같은 State Space Model(SSM)이나, **스트리밍(Streaming)** 처리 방식, 그리고 과거 정보를 효율적으로 저장하는 **메모리 증강(Memory-Augmented)** 구조가 활발히 연구되고 있습니다.
- **에이전트적 추론 및 인과관계 이해 (Agentic Reasoning & Causality)**: 모델이 단순히 주어진 정보를 처리하는 것을 넘어, 부족한 정보를 **외부 웹에서 검색**하고(**Watching, Reasoning, and Searching**), 비디오 내 사건의 **인과관계를 파악**하며(**Harnessing Temporal Causality**), **논리적인 사고 과정(Chain-of-Thought)** 을 통해 결론을 도출하는 '에이전트(Agent)'로서의 역할이 강조되고 있습니다.
- **오픈 소스 및 데이터셋 생태계의 발전**: **Molmo2**, **STEP3-VL-10B** 등 상용 모델에 필적하는 강력한 오픈 소스 모델들이 공개되고 있으며, **Action100M**과 같은 초대규모 데이터셋이 구축되면서 연구 생태계가 더욱 가속화되고 있습니다.

결론적으로 비디오 이해 분야는 **단순 분류(Classification)에서 시작하여, 시공간적 맥락 이해(Contextualization)를 거쳐, 현재는 복합적인 멀티모달 정보를 바탕으로 논리적 추론을 수행하는 'AI 에이전트'로 진화**하고 있습니다.
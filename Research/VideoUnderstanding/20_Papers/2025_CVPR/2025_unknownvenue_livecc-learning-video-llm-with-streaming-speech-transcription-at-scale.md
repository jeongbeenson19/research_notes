---
type: paper
title: "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale"
venue: "CVPR"
year: 2025
authors: ["Joya Chen", "Ziyun Zeng", "Yiqi Lin", "Wei Li", "Zejun Ma", "Mike Zheng Shou"]
url: "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_LiveCC_Learning_Video_LLM_with_Streaming_Speech_Transcription_at_Scale_CVPR_2025_paper.pdf"
tasks: ["benchmark", "streaming"]
methods: []
datasets: []
metrics: []
trends: []
status: to-read
date_read: ""


---

# Main Contribution (3 lines)
1) **What**: 비디오 이해를 위한 새로운 모델/학습/프롬프트 방법을 제안.
2) **Why**: 비디오-LLM의 정렬/추론/적응을 개선하는 구성요소 또는 학습 전략을 제안.
3) **Impact**: 온라인/스트리밍 측면에서 비디오 이해/평가를 확장.

## Method (<=5 bullets)
- 온라인/스트리밍 입력을 고려한 설정.
- 스트리밍 음성 전사 신호를 결합.

## Evidence
- Benchmarks:
- Key numbers:
- Key ablations (2):
  -
  -

## Assumptions / Limitations (2 each)
- Assumptions:
  -
- Limitations:
  -

## Failure Modes (3)
-
-
-

## One-liner takeaway
- LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale은/는 비디오-LLM의 정렬/추론/적응을 개선하는 구성요소 또는 학습 전략을 목표로 한다.

## Next experiment idea (1)
-

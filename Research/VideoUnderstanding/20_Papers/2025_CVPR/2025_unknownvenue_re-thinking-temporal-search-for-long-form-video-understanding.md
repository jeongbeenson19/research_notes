---
type: paper
title: "Re-thinking Temporal Search for Long-Form Video Understanding"
venue: "CVPR"
year: 2025
authors: ["Jinhui Ye", "Zihan Wang", "Haosen Sun", "Keshigeyan Chandrasegaran", "Zane Durante", "Cristobal Eyzaguirre", "Yonatan Bisk", "Juan Carlos Niebles", "Ehsan Adeli", "Li Fei-Fei", "Jiajun Wu", "Manling Li"]
url: "https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_Re-thinking_Temporal_Search_for_Long-Form_Video_Understanding_CVPR_2025_paper.pdf"
tasks: ["benchmark", "long-video"]
methods: ["retrieval"]
datasets: []
metrics: []
trends: []
status: to-read
date_read: ""



---

# Main Contribution (3 lines)
1) **What**: "Re-thinking Temporal Search for Long-Form Video Understanding"에서 제안하는 비디오 이해 접근을 정리.
2) **Why**: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
3) **Impact**: 장시간, 효율, 시간적 근거 측면에서 비디오 이해/평가를 확장.

## Method (<=5 bullets)
- 시간 구간 검색/탐색을 재설계.
- 장시간 비디오 입력을 다루는 구조/전략을 설계.

## Evidence
- Benchmarks:
- Key numbers:
- Key ablations (2):
  -
  -

## Assumptions / Limitations (2 each)
- Assumptions:
  -
- Limitations:
  -

## Failure Modes (3)
-
-
-

## One-liner takeaway
- Re-thinking Temporal Search for Long-Form Video Understanding은/는 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 목표로 한다.

## Next experiment idea (1)
-

---
type: paper
title: "EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering"
venue: "CVPR"
year: 2025
authors: ["Sheng Zhou", "Junbin Xiao", "Qingyun Li", "Yicong Li", "Xun Yang", "Dan Guo", "Meng Wang", "Tat-Seng Chua", "Angela Yao"]
url: "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_EgoTextVQA_Towards_Egocentric_Scene-Text_Aware_Video_Question_Answering_CVPR_2025_paper.pdf"
tasks: ["benchmark", "temporal-grounding", "videoqa"]
methods: ["retrieval"]
datasets: []
metrics: []
trends: []
status: to-read
date_read: ""


---

# Main Contribution (3 lines)
1) **What**: "EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering"에서 제안하는 비디오 이해 접근을 정리.
2) **Why**: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
3) **Impact**: 비디오 이해 문제에 대한 새로운 접근을 제시.

## Method (<=5 bullets)
- 문제 설정에 맞춘 비디오 이해 파이프라인/모델 구성을 제안.

## Evidence
- Benchmarks:
- Key numbers:
- Key ablations (2):
  -
  -

## Assumptions / Limitations (2 each)
- Assumptions:
  -
- Limitations:
  -

## Failure Modes (3)
-
-
-

## One-liner takeaway
- EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering은/는 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.

## Next experiment idea (1)
-

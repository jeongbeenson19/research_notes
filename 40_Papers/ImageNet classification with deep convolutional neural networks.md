---
title: "ImageNet classification with deep convolutional neural networks"
authors: 
zoteroKey: krizhevskyImageNetClassificationDeep2017
zoteroURL: 
tags: [zotero, journalArticle]
created: 1752651163000
modified: 1752651163000
---

# ImageNet classification with deep convolutional neural networks

> We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.

## 📘 메타데이터
- 저자: 
- 연도: 1495551600000
- 저널명: Communications of the ACM
- 출판사: 
- DOI: 10.1145/3065386
- Zotero 링크: [열기]()

## **📌 Abstract (초록)**

우리는 ImageNet LSVRC-2010 대회의 1000개 클래스에 속하는 **120만 장의 고해상도 이미지**를 분류하기 위해 대규모 심층 합성곱 신경망(Deep Convolutional Neural Network)을 학습시켰습니다. 테스트 데이터에서 **Top-1 오류율 37.5%**, **Top-5 오류율 17.0%** 를 달성했으며, 이는 당시 기존 최고 성능보다 상당히 우수한 결과입니다.

이 신경망은 **6천만 개의 파라미터**와 **65만 개의 뉴런**으로 구성되어 있으며, **5개의 합성곱 계층(convolutional layers)** 과 일부 계층 뒤에 이어지는 **최대 풀링(max-pooling) 계층**, 그리고 마지막에 **1000-클래스 소프트맥스(softmax)** 를 가진 **3개의 완전 연결 계층(fully-connected layers)** 으로 이루어져 있습니다.

학습 속도를 높이기 위해, 우리는 **비포화형 활성화 함수(non-saturating neurons)** 와 **효율적인 GPU 기반 합성곱 연산 구현**을 사용했습니다. 또한 완전 연결 계층에서 과적합을 줄이기 위해, 최근 개발된 정규화 기법인 **“[[Dropout|드롭아웃(dropout)]]”** 을 적용했으며, 이 방법은 매우 효과적인 것으로 입증되었습니다.

이 모델의 변형 버전을 **ILSVRC-2012 대회**에 출전시킨 결과, **Top-5 테스트 오류율 15.3%** 를 달성했으며, 이는 2위 모델(오류율 26.2%)을 크게 앞서는 성능입니다.

---

## **📌 1. Introduction (서론)**

현재의 객체 인식(object recognition) 접근법은 기본적으로 **기계 학습 방법**에 의존하고 있습니다. 이러한 성능을 개선하기 위해서는 **더 큰 데이터셋을 수집**하고, **더 강력한 모델을 학습**하며, **과적합을 방지하는 더 나은 기법**을 사용하는 것이 필요합니다.

이전까지는 라벨이 지정된 이미지 데이터셋의 크기가 상대적으로 작았습니다. 예를 들어, **NORB [16]**, **Caltech-101/256 [8,9]**, **CIFAR-10/100 [12]** 등은 수만 장 수준의 이미지로 구성되어 있었습니다. 이러한 크기의 데이터셋은 단순한 인식 과제에는 충분하며, 라벨을 유지하는 변환(label-preserving transformations)을 활용하면 성능이 더욱 향상됩니다. 예를 들어, **MNIST 손글씨 숫자 인식**의 최신 오류율은 **0.3% 미만**으로 인간 성능에 근접해 있습니다 [4].

그러나 **현실 세계의 객체**는 상당한 변동성을 보이므로, 이를 제대로 인식하려면 훨씬 더 큰 학습 데이터셋이 필요합니다. 작은 이미지 데이터셋의 한계는 이미 널리 인식되어 왔지만 [21], 수백만 장의 라벨이 지정된 이미지를 수집할 수 있게 된 것은 최근의 일입니다. 예를 들어, **LabelMe [23]** 는 수십만 장의 완전 분할된 이미지로 구성되어 있고, **ImageNet [6]** 은 2만2천 개 이상의 카테고리에서 **1천5백만 장 이상의 고해상도 이미지**를 포함하고 있습니다.

수백만 장의 이미지로부터 수천 개의 객체를 학습하려면 **높은 학습 용량을 가진 모델**이 필요합니다. 하지만 객체 인식 과제의 복잡성이 매우 크기 때문에, **ImageNet과 같은 대규모 데이터셋조차도 문제를 완전히 기술하지 못합니다.** 따라서 모델은 **사전 지식(prior knowledge)** 을 충분히 가져야 합니다.

**합성곱 신경망(CNN, Convolutional Neural Networks)** 은 이러한 요구를 만족하는 대표적인 모델입니다 [16,11,13,18,15,22,26]. CNN의 용량은 깊이(depth)와 폭(breadth)을 조절해 제어할 수 있으며, 이미지의 통계적 특성과 국소성(locality of pixel dependencies) 등에 대한 강력하고 대부분 정확한 가정을 내포합니다. 따라서, 비슷한 규모의 계층을 가진 일반적인 피드포워드 신경망과 비교할 때, CNN은 연결 수와 파라미터 수가 훨씬 적어 학습이 더 용이하며, 이론적으로 최고의 성능도 큰 차이가 나지 않습니다.

---

### **GPU와 대규모 CNN의 가능성**

CNN이 매력적인 특성을 가지고 있음에도 불구하고, **고해상도 이미지에 대규모로 적용하기에는 비용이 너무 많이 들었습니다.** 그러나 최근에는 **2D 합성곱의 고도로 최적화된 구현**과 **현대 GPU의 연산 능력** 덕분에 대규모 CNN 학습이 가능해졌습니다. 또한 **ImageNet과 같은 대규모 라벨 데이터셋** 덕분에 과적합 없이 모델을 학습할 수 있습니다.

이 논문의 주요 기여는 다음과 같습니다:

1. **ILSVRC-2010과 ILSVRC-2012** 대회에서 사용된 ImageNet의 부분 데이터셋으로, 당시까지 보고된 것 중 가장 우수한 성능을 달성한 대규모 CNN을 학습시켰습니다.
    
2. CNN 학습에 필요한 **2D 합성곱 및 기타 연산의 고도로 최적화된 GPU 구현**을 작성하였으며, 이를 공개했습니다.
    
3. 네트워크 성능을 향상시키고 학습 시간을 단축하는 **새롭고 독창적인 특징**을 도입했습니다(3장에서 설명).
    
4. 네트워크가 매우 크기 때문에, 120만 장의 라벨 이미지에도 과적합 문제가 심각하여 이를 방지하기 위한 여러 효과적인 기법을 적용했습니다(4장에서 설명).
    

이 최종 네트워크는 **5개의 합성곱 계층과 3개의 완전 연결 계층**으로 이루어져 있으며, 이러한 깊이는 매우 중요합니다. 합성곱 계층 중 하나라도 제거하면 성능이 저하되었으며(각 합성곱 계층의 파라미터는 전체의 1% 미만임에도), 이는 깊이가 성능에 핵심적인 요소임을 의미합니다.

네트워크의 크기는 **GPU 메모리 용량**과 **허용 가능한 학습 시간**에 의해 주로 제한됩니다. 현재 이 네트워크는 **GTX 580 3GB GPU 두 개**로 학습하는 데 **5~6일**이 소요됩니다. 더 빠른 GPU와 더 큰 데이터셋이 등장하면 성능은 더욱 향상될 것으로 예상됩니다.

---

## **📌 2. The Dataset (데이터셋)**

**ImageNet**은 약 **22,000개 카테고리**에 속하는 **1천5백만 장 이상의 라벨이 지정된 고해상도 이미지**로 구성된 데이터셋입니다. 이미지는 웹에서 수집되었으며, **Amazon Mechanical Turk** 크라우드소싱 도구를 사용해 사람이 직접 라벨링했습니다.

2010년부터 **Pascal Visual Object Challenge**의 일환으로 **ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)** 가 매년 개최되고 있습니다. ILSVRC는 ImageNet의 부분 데이터셋을 사용하며, 각 **1,000개 카테고리당 약 1,000장의 이미지**로 구성됩니다. 총 데이터셋의 규모는 **120만 장의 학습 이미지**, **50,000장의 검증 이미지**, **150,000장의 테스트 이미지**입니다.

ILSVRC-2010은 **테스트 세트의 라벨이 공개된 유일한 버전**이므로, 우리는 대부분의 실험을 이 버전에서 수행했습니다. 또한 우리는 ILSVRC-2012 대회에도 모델을 출전시켰으며, **섹션 6**에서 테스트 세트 라벨이 공개되지 않은 이 버전에 대한 결과도 보고합니다.  
ImageNet에서는 일반적으로 두 가지 오류율을 보고합니다:

- **Top-1 오류율**: 모델이 가장 높은 확률로 예측한 라벨이 정답이 아닐 확률
    
- **Top-5 오류율**: 정답 라벨이 모델이 예측한 상위 5개의 후보 라벨 안에 없을 확률
    

---

### **이미지 전처리**

ImageNet의 이미지는 해상도가 다양하지만, 우리의 시스템은 **고정된 입력 크기**를 필요로 합니다. 따라서 우리는 이미지를 **256×256 고정 크기로 다운샘플링**했습니다.

전처리 과정은 다음과 같습니다:

1. **직사각형 이미지**의 경우, **짧은 변의 길이가 256이 되도록 비율을 유지하며 리사이즈**했습니다.
    
2. 그 후, 리사이즈된 이미지의 **중앙 256×256 패치**를 잘라냈습니다.
    
3. 추가적인 전처리는 수행하지 않았으며, 단지 **훈련 세트 전체의 평균 픽셀 값을 각 픽셀에서 빼주는 방식으로 정규화**했습니다.
    

즉, 우리는 **정규화된 원본 RGB 픽셀 값(raw RGB values)** 만을 사용하여 네트워크를 학습시켰습니다.

---

## **📌 3. The Architecture (네트워크 구조)**

우리 네트워크의 구조는 **그림 2**에 요약되어 있습니다. 이 네트워크는 **총 8개의 학습 가능한 계층**으로 구성되며,

- **5개의 합성곱 계층(convolutional layers)**
    
- **3개의 완전 연결 계층(fully-connected layers)**  
    을 포함합니다.
    

아래에서는 네트워크의 독창적이거나 새로운 구조적 특징을 설명하며, **중요도 순**(가장 중요한 것부터)으로 나열합니다.

---

### **3.1 ReLU 비선형성 (ReLU Nonlinearity)**

일반적으로 뉴런의 출력 $f(x)$는 입력 $x$에 대해

- $f(x) = \tanh(x)$ 또는
    
- $f(x) = (1 + e^{-x})^{-1}$
    

와 같은 **포화형(saturating) 비선형성**으로 모델링됩니다. 그러나 **경사 하강법(gradient descent)** 을 이용한 학습 시간은 이들 포화형 함수보다 **비포화형(non-saturating)** 함수가 훨씬 빠릅니다.

우리는 $f(x) = \max(0, x)$을 사용했으며, 이 함수를 사용하는 뉴런을 **ReLU(Rectified Linear Unit)** 라고 부릅니다 [20]. ReLU를 사용한 심층 CNN은 **tanh 뉴런을 사용하는 네트워크보다 학습 속도가 여러 배 빠릅니다**.  
예를 들어, **CIFAR-10** 데이터셋에서 **4계층 CNN**은 **ReLU 사용 시 25%의 학습 오류율에 도달하는 데 걸리는 반복 횟수가 tanh 뉴런 사용 대비 6배나 빠릅니다(그림 1 참조)**.

이처럼 빠른 학습 덕분에 우리는 매우 큰 신경망을 실험할 수 있었습니다. 만약 전통적인 포화형 뉴런을 사용했다면, 본 논문에서 다루는 정도의 대규모 신경망 실험은 불가능했을 것입니다.

---

### **3.2 다중 GPU 학습 (Training on Multiple GPUs)**

**단일 GTX 580 GPU**는 메모리가 3GB에 불과하여, 이론적으로 학습 가능한 네트워크의 크기가 제한됩니다. 그러나 **120만 장의 학습 이미지**는 단일 GPU의 용량을 초과하는 대형 네트워크 학습이 가능하도록 충분합니다.

따라서 우리는 네트워크를 **두 개의 GPU에 분산**시켰습니다.

- 현대 GPU는 **서로의 메모리에 직접 접근(read/write)** 할 수 있으므로 병렬화에 매우 적합합니다.
    
- 우리는 절반의 커널(또는 뉴런)을 각각의 GPU에 배치했으며, **특정 계층에서만 GPU 간 통신이 이루어지도록 설계**했습니다.
    

예를 들어:

- **3번째 합성곱 계층**의 커널은 **2번째 계층의 모든 커널 맵**에서 입력을 받지만,
    
- **4번째 계층**의 커널은 **같은 GPU에 있는 3번째 계층의 커널 맵**에서만 입력을 받습니다.
    

이러한 연결 패턴은 교차 검증으로 조정했으며, 통신량이 계산량에 비해 적절하도록 세밀히 조절했습니다.

결과적으로, **단일 GPU에서 학습한 네트워크에 비해 Top-1 오류율은 1.7%**, **Top-5 오류율은 1.2% 감소**했습니다. 또한 **두 개의 GPU로 학습한 네트워크가 단일 GPU 네트워크보다 약간 더 빠르게 학습**되었습니다.

---

### **3.3 국소 응답 정규화 (Local Response Normalization)**

ReLU는 입력 정규화를 하지 않아도 뉴런이 포화되지 않는 장점이 있습니다. 그러나 **일부 층에서 국소 정규화(local normalization)를 수행하면 일반화 성능이 향상**됨을 확인했습니다.

정규화 수식은 다음과 같습니다:

$$
b_{i}^{x,y} = \frac{a_{i}^{x,y}}{\left(k + \alpha \sum_{j=\max(0, i - n/2)}^{\min(N-1, i + n/2)} (a_{j}^{x,y})^2 \right)^{\beta}}
$$

여기서,

- $a_{i}^{x,y}$: 특정 위치 $(x, y)$에서 커널 $i$를 적용 후 ReLU를 통과한 활성값
    
- $N$: 해당 층의 총 커널 수
    
- $k, n, \alpha, \beta$: 하이퍼파라미터  
    ($k=2$, $n=5$, $\alpha=10^{-4}$, $\beta=0.75$)
    

이 방식은 **생물학적 뉴런의 측방 억제(lateral inhibition)** 에서 영감을 받은 것으로, 커널 간 경쟁을 유도합니다.

이 정규화는:

- **Top-1 오류율을 1.4% 감소**,
    
- **Top-5 오류율을 1.2% 감소**시켰습니다.
    

---

### **3.4 겹치는 풀링 (Overlapping Pooling)**

CNN의 풀링 계층은 인접한 뉴런 그룹의 출력을 요약합니다. 전통적인 풀링에서는 **인접 풀링 유닛의 요약 영역이 겹치지 않습니다**(즉, 간격 s = 크기 z).

우리는 **겹치는 풀링(overlapping pooling)**을 사용했으며, 구체적으로:

- $s = 2$
    
- $z = 3$
    

이 방식은 전통적인 $s = 2, z = 2$ 풀링보다 **Top-1 오류율을 0.4%**, **Top-5 오류율을 0.3% 줄였으며**, 과적합이 약간 덜 발생했습니다.

---

### **3.5 전체 네트워크 구조 (Overall Architecture)**

우리 CNN의 최종 구조는 다음과 같습니다:

- **8개의 학습 가능한 계층** (5개의 합성곱 + 3개의 완전 연결)
    
- 마지막 완전 연결 계층 출력은 **1000-클래스 소프트맥스**로 전달되어 클래스 확률을 출력합니다.
    
- **ReLU 비선형성**은 모든 합성곱 계층과 완전 연결 계층에 적용됩니다.
    
- **정규화 층**은 1번째와 2번째 합성곱 계층 뒤에 위치하며,
    
- **Max-pooling**은 1·2번째 정규화 층 및 5번째 합성곱 계층 뒤에 위치합니다.
    

각 계층의 주요 파라미터는 다음과 같습니다:

1. **Conv1**: 224×224×3 입력 이미지를 **크기 11×11×3, 스트라이드 4**의 **96개 커널**로 필터링
    
2. **Conv2**: 응답 정규화 및 풀링된 Conv1 출력을 **크기 5×5×48**의 **256개 커널**로 필터링
    
3. **Conv3**: **크기 3×3×256**의 **384개 커널**
    
4. **Conv4**: **크기 3×3×192**의 **384개 커널**
    
5. **Conv5**: **크기 3×3×192**의 **256개 커널**
    
6. **FC6 & FC7**: 각각 **4096개의 뉴런**
    
7. **FC8**: **1000개의 뉴런** → 소프트맥스 출력
    

---

## **📌 4. Reducing Overfitting (과적합 감소)**

우리 신경망 구조는 **총 6천만 개의 파라미터**를 가지고 있습니다. ILSVRC의 1000개 클래스가 각 학습 샘플마다 약 10비트의 제약을 제공하긴 하지만, 여전히 과적합(overfitting)을 피하기에는 부족합니다.  
우리는 과적합을 줄이기 위해 두 가지 주요 방법을 사용했습니다.

---

### **4.1 데이터 증강 (Data Augmentation)**

이미지 데이터의 과적합을 줄이는 가장 쉽고 일반적인 방법은 **라벨을 유지하는 변환(label-preserving transformations)** 을 통해 **데이터셋을 인위적으로 확대**하는 것입니다 [25,4,5].  
우리는 두 가지 데이터 증강 방법을 사용했으며, 이 방법들은 원본 이미지에서 변환된 이미지를 매우 적은 연산으로 생성할 수 있어 **디스크에 저장할 필요 없이 실시간으로 생성**됩니다.  
구체적으로, **CPU(Python 코드)** 에서 증강 이미지를 생성하는 동안 **GPU는 이전 배치를 학습**하기 때문에 사실상 연산 비용이 거의 들지 않습니다.

#### ✅ **(1) 이미지 이동 및 좌우 반전**

- 256×256 이미지에서 **임의의 224×224 패치**를 잘라내고, 그 **좌우 반전 이미지**를 함께 학습에 사용했습니다.
    
- 이 방법으로 학습 세트의 크기가 **2048배** 증가하지만, 생성된 훈련 샘플은 상호 강하게 의존적입니다.
    
- 이 과정을 사용하지 않으면 네트워크는 심각한 과적합을 겪게 되어, 훨씬 더 작은 네트워크를 사용해야 했을 것입니다.
    

**테스트 시 예측 방법**:

- 256×256 이미지에서 **4개의 모서리 패치와 중앙 패치**(224×224), 그리고 이들의 **좌우 반전 이미지**를 추출(총 10개의 패치).
    
- 네트워크는 이 10개의 패치 각각에 대해 softmax 출력을 계산하고, **평균을 내어 최종 예측**을 수행했습니다.
    

#### ✅ **(2) RGB 채널 강도 변환**

- ImageNet 전체 학습 세트의 **RGB 픽셀 값에 대해 PCA**를 수행했습니다.
    
- 각 학습 이미지에 대해 **PCA의 주성분 벡터(p1, p2, p3)** 와 **고유값(λ1, λ2, λ3)** 을 기반으로 RGB 값을 변경했습니다.
    

변환식은 다음과 같습니다:

$$
I_{xy} = I_{xy} + [p_1, p_2, p_3] \cdot [\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T
$$

여기서,

- $I_{xy}$: $(x, y)$ 픽셀의 RGB 값
    
- $\alpha_i$: 평균 0, 표준편차 0.1의 정규분포에서 샘플링한 난수
    
- 각 이미지마다 학습에 사용될 때마다 새로 $α$를 샘플링
    

이 방법은 **자연 이미지의 중요한 특성**, 즉 **조명 강도와 색상이 달라져도 물체의 정체성은 유지**된다는 점을 근사적으로 반영합니다. 이 방법으로 **Top-1 오류율이 1% 이상 감소**했습니다.

---

### **4.2 드롭아웃 (Dropout)**

여러 모델의 예측을 결합하는 것은 테스트 오류를 줄이는 매우 효과적인 방법입니다 [1,3]. 그러나 이미 학습에 며칠이 걸리는 대규모 신경망에서는 현실적으로 어렵습니다.  
이를 대체할 수 있는 효율적인 방법이 바로 **드롭아웃(dropout)** 입니다 [10].

- 학습 중, **각 은닉 뉴런의 출력을 0으로 만들 확률을 0.5로 설정**합니다.
    
- 드롭된 뉴런은 **순전파(forward pass)** 와 **역전파(backpropagation)** 모두에 참여하지 않습니다.
    
- 즉, 매 입력마다 네트워크가 **랜덤한 아키텍처 샘플**을 사용하지만, 모든 샘플은 **가중치를 공유**합니다.
    
- 이로 인해 뉴런은 특정 다른 뉴런에 의존하지 않고, **더 일반적이고 강건한 특징**을 학습하도록 강제됩니다.
    

**테스트 시**, 모든 뉴런을 사용하지만 출력을 **0.5로 스케일링**하여, 지수적으로 많은 드롭아웃 네트워크의 예측 분포의 기하평균(geometric mean)을 근사합니다.

우리는 **첫 두 개의 완전 연결 계층**에 드롭아웃을 적용했습니다.  
드롭아웃 없이는 네트워크가 상당한 과적합을 보였으며, 드롭아웃은 수렴까지 필요한 반복 횟수를 약 2배로 늘리지만 과적합을 크게 줄였습니다.

---

## **📌 5. Details of Learning (학습 세부 사항)**

우리는 모델을 **확률적 경사 하강법(SGD, Stochastic Gradient Descent)** 으로 학습했습니다. 주요 설정은 다음과 같습니다:

- **배치 크기(batch size):** 128
    
- **모멘텀(momentum):** 0.9
    
- **가중치 감쇠(weight decay):** 0.0005
    

이 작은 값의 weight decay는 모델 학습에서 매우 중요했습니다. 즉, 단순한 정규화 목적뿐 아니라 **훈련 오류 감소**에도 기여했습니다.

---

### **5.1 가중치 업데이트 규칙**

가중치 ww의 업데이트 식은 다음과 같습니다:

$$
\begin{align}
v_{i+1} := 0.9 \cdot v_i - 0.0005 \cdot \varepsilon \cdot w_i - \varepsilon \cdot \left\langle \frac{\partial L}{\partial w} \bigg|_{w_i} \right\rangle_{D_i} 
w_{i+1} := w_i + v_{i+1}
\end{align}
$$

여기서,

- $i$: 반복(iteration) 인덱스
    
- $v$: 모멘텀 변수
    
- $\varepsilon$: 학습률(learning rate)
    
- $\left\langle \frac{\partial L}{\partial w} \big|_{w_i} \right\rangle_{D_i}$: **$i$번째 배치 $D_i$**에 대한 목적 함수 $L$의 평균 기울기
    

---

### **5.2 초기화 (Initialization)**

- **가중치 초기화:** 평균이 0이고 표준편차가 0.01인 **정규분포(가우시안)** 에서 샘플링
    
- **바이어스 초기화:**
    
    - **2·4·5번째 합성곱 계층**과 **완전 연결 계층**의 은닉 뉴런 바이어스는 **1**로 초기화 → 이는 학습 초기에 ReLU가 **양의 입력**을 받을 가능성을 높여 학습 속도를 빠르게 함
        
    - 나머지 계층의 바이어스는 **0**으로 초기화
        

---

### **5.3 학습률 조정 (Learning Rate Schedule)**

- 모든 계층에서 동일한 학습률을 사용했습니다.
    
- 학습률은 **검증 오류가 개선되지 않을 때마다 10분의 1로 감소**시켰습니다.
    
- 초기 학습률은 **0.01**, 이후 총 **3회 감소**시켰습니다.
    

---

### **5.4 학습 시간**

- **1.2M 이미지 전체를 약 90회(epoch) 순회**하며 학습했습니다.
    
- **2개의 NVIDIA GTX 580 3GB GPU**로 학습했으며, 총 **5~6일**이 소요되었습니다.
    

---

## **📌 6. Results (결과)**

---

### **6.1 ILSVRC-2010 결과**

우리 네트워크의 ILSVRC-2010 결과는 **표 1**에 요약되어 있습니다.

- **Top-1 오류율:** 37.5%
    
- **Top-5 오류율:** 17.0%
    

이는 당시 최고 성능이었던 다음 기법들을 크게 능가한 결과입니다:

1. **Sparse Coding [2]:** Top-1 47.1%, Top-5 28.2%
    
2. **SIFT + Fisher Vectors (FVs) [24]:** Top-1 45.7%, Top-5 25.7%
    

📌 **표 1: ILSVRC-2010 테스트 세트 결과 비교**

|모델|Top-1|Top-5|
|---|---|---|
|Sparse coding [2]|47.1%|28.2%|
|SIFT + FVs [24]|45.7%|25.7%|
|**우리 CNN**|**37.5%**|**17.0%**|

---

### **6.2 ILSVRC-2012 결과**

우리는 모델을 **ILSVRC-2012** 대회에도 출전시켰으며, **표 2**에 결과를 요약했습니다.  
ILSVRC-2012 테스트 세트의 라벨은 공개되지 않았으므로 검증 세트 결과를 사용했습니다(검증과 테스트 오류율 차이는 0.1% 이하).

1. **단일 CNN:** Top-5 오류율 18.2%
    
2. **5개의 CNN 예측 평균:** 16.4%
    
3. **ImageNet 2011 Fall (15M 이미지, 22K 클래스) 전체를 사전 학습한 CNN:** 16.6%
    
4. **위 사전 학습된 2개 CNN + 5개 CNN 앙상블:** 15.3% (**대회 1위**)
    
5. **2위 참가팀:** 26.2% (FVs 기반)
    

📌 **표 2: ILSVRC-2012 검증/테스트 세트 결과**

|모델|Top-1 (val)|Top-5 (val)|Top-5 (test)|
|---|---|---|---|
|SIFT + FVs [7]|—|—|26.2%|
|1 CNN|40.7%|18.2%|—|
|5 CNNs|38.1%|16.4%|16.4%|
|1 CNN* (사전 학습)|39.0%|16.6%|—|
|__7 CNNs_ (사전 학습 + 앙상블)_*|**36.7%**|**15.4%**|**15.3%**|

(* 사전 학습: ImageNet 2011 Fall 전체를 분류하도록 먼저 학습 후, ILSVRC-2012에 파인튜닝)

---

### **6.3 ImageNet Fall 2009 결과**

우리는 **10,184 클래스, 8.9M 이미지**로 구성된 ImageNet 2009 Fall 버전에서도 실험했습니다. (이 데이터셋은 표준 테스트 세트가 없기 때문에, 이미지의 절반을 훈련·나머지를 테스트에 사용했습니다.)

- **Top-1 오류율:** 67.4%
    
- **Top-5 오류율:** 40.9%
    
- 당시 최고 기록: 78.1%(Top-1), 60.9%(Top-5) [19]
    

---

### **6.4 정성적 평가 (Qualitative Evaluations)**

**그림 3**:

- 첫 번째 합성곱 계층에서 학습된 **96개의 11×11×3 필터**를 시각화했습니다.
    
- GPU 1에서 학습된 상위 48개 필터는 **색상과 무관한(색상 비특이적)** 경향이 있고, GPU 2에서 학습된 하위 48개 필터는 **색상에 특화**되었습니다.
    
- 이러한 **전문화(specialization)** 는 매 학습마다 일관되게 발생하며 초기화(random seed)와 무관합니다.
    

**그림 4 (왼쪽):**

- 8장의 테스트 이미지에 대한 Top-5 예측을 보여줍니다.
    
- 예를 들어, 표범 이미지의 경우 모델은 다른 고양이 종만을 Top-5 후보로 고려했으며, 사진 초점에 애매함이 있는 이미지(예: 자동차 그릴, 체리)에서는 합리적인 후보를 제시했습니다.
    

**그림 4 (오른쪽):**

- 테스트 이미지 5장을 기준으로, 마지막 은닉 계층(4096차원)에서 **유클리드 거리**가 가장 가까운 6장의 학습 이미지를 검색했습니다.
    
- 픽셀 수준의 유사도와 달리, 네트워크는 **다양한 자세의 개나 코끼리 이미지**를 올바르게 유사하다고 판단했습니다.
    

---

## **📌 7. Discussion (논의)**

우리의 결과는 **대규모 심층 합성곱 신경망(Deep Convolutional Neural Network)** 이 순수한 **지도 학습(supervised learning)** 만으로도 **매우 도전적인 데이터셋에서 기존 최고 기록을 경신할 수 있음**을 보여줍니다.

---

### **7.1 네트워크 깊이의 중요성**

네트워크의 성능은 **합성곱 계층(convolutional layers)** 이 하나라도 제거되면 크게 저하되었습니다.

- 예를 들어, **중간 계층 중 하나를 제거하면 Top-1 성능이 약 2% 하락**했습니다.
    
- 이는 **깊이(depth)** 가 성능 향상에 핵심적인 역할을 한다는 것을 의미합니다.
    

---

### **7.2 비지도 사전 학습(Unsupervised Pre-training)에 대한 언급**

이번 연구에서는 실험의 단순화를 위해 **비지도 사전 학습(unsupervised pre-training)** 을 사용하지 않았습니다. 그러나 충분한 컴퓨팅 자원이 확보된다면, 특히 네트워크 크기를 크게 늘릴 수 있지만 라벨 데이터 양이 비례해 늘어나지 않는 경우, 비지도 학습이 도움이 될 것으로 예상됩니다.

---

### **7.3 향후 방향**

현재까지 우리의 결과는 **네트워크의 크기를 키우고 학습 시간을 늘릴수록 성능이 향상**됨을 보여줬습니다. 그러나 인간의 시각 시스템, 특히 **infero-temporal pathway**에 비견하려면 여전히 수많은 연구가 필요합니다.

궁극적으로 우리는 **매우 크고 깊은 CNN을 비디오 시퀀스에 적용**하고자 합니다. 비디오는 정적 이미지에는 명확하지 않은 **시간적 구조(temporal structure)** 를 포함하고 있어, 객체 인식 성능을 한층 더 높이는 데 유용할 것입니다.

---

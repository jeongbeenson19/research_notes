---
title: Deep Residual Learning for Image Recognition
authors: 
zoteroKey: heDeepResidualLearning2015
zoteroURL: 
tags:
  - zotero
  - preprint
created: 1752195964003
modified: 1752195964000
---

# Deep Residual Learning for Image Recognition

> Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.

## 📘 메타데이터
- 저자: 
- 연도: 1449673200000
- 저널명: 
- 출판사: 
- DOI: 10.48550/arXiv.1512.03385
- Zotero 링크: [열기]()

## **초록 (Abstract)**

더 깊은 신경망은 학습이 어렵다.  
우리는 기존보다 훨씬 더 깊은 네트워크의 학습을 쉽게 하기 위한 **잔차 학습(residual learning) 프레임워크**를 제안한다. 우리는 각 층을 입력에 대한 **잔차 함수(residual function)** 를 학습하는 형태로 명시적으로 재정의하며, 입력과 무관한 함수를 학습하는 대신 이러한 방식을 채택한다.

우리는 이러한 잔차 네트워크가 최적화가 더 쉽고, 깊이가 상당히 증가했음에도 정확도가 향상됨을 보여주는 실험적 증거를 제시한다.  
ImageNet 데이터셋에서 우리는 최대 **152층 깊이의 Residual Net**을 평가하였다. 이는 VGG 네트워크보다 **8배 더 깊지만, 연산 복잡도는 더 낮다**. 이 ResNet 앙상블은 ImageNet 테스트 셋에서 **3.57% 오차율**을 기록하며, **ILSVRC 2015 분류 과제에서 1위를 차지**하였다.

또한 CIFAR-10 데이터셋에서 100층 및 1000층 네트워크에 대한 분석도 제시한다.

깊은 표현(표현의 깊이)은 시각 인식 작업에서 매우 중요하다. 오직 깊은 표현만으로 COCO 객체 검출 데이터셋에서 **상대적으로 28% 향상된 성능**을 얻었다. Deep Residual Net은 ILSVRC와 COCO 2015 대회에 제출된 우리의 기본 모델이며, ImageNet 검출, ImageNet 위치추정, COCO 검출, COCO 분할 과제에서 모두 **1위를 달성**하였다.

---

## **1. 서론 (Introduction)**

딥 컨볼루션 신경망은 이미지 분류에서 일련의 **획기적인 성과**를 이끌었다.  
딥 네트워크는 저/중/고 수준의 특징들과 분류기를 자연스럽게 통합하며, **층을 쌓을수록 더 복잡한 표현**을 할 수 있게 된다.

최근 연구는 네트워크의 **깊이가 결정적으로 중요**하다는 점을 밝혔고, ImageNet과 같은 까다로운 데이터셋에서 우수한 성능을 기록한 모델들은 모두 **16층~30층 이상의 매우 깊은 구조**를 사용하였다. 이 외에도 다양한 시각 인식 작업에서 **"매우 깊은 모델"** 은 유의미한 성능 향상을 가져왔다.

하지만 단순히 층을 많이 쌓는 것이 좋은 네트워크를 학습하는 **간단한 해법**일까?

이 질문에는 **소실/폭발하는 그래디언트 문제**로 인해 답하기 어려웠지만, 최근에는 정규화 초기화 및 Batch Normalization과 같은 기법을 통해 **수십 층짜리 네트워크의 학습이 가능**해졌다.

그러나 이보다 더 깊은 네트워크에서는 **학습 성능의 저하(degradation)** 문제가 발생한다.  
즉, 네트워크 깊이가 증가하면 정확도가 포화되고, 이후에는 오히려 악화된다. 이는 단순한 과적합의 문제가 아니라, 더 깊은 모델이 더 높은 **학습 오차를 유발**한다는 실험 결과가 보고되었다.

이 논문에서는 이러한 학습 성능 저하 문제를 **"잔차 학습(Residual Learning)"** 을 통해 해결하고자 한다.  
우리는 각 층이 직접 복잡한 함수를 학습하기보다는, 입력을 기준으로 한 **잔차 함수**를 학습하도록 유도한다.  
예를 들어, 원하는 함수가 H(x)일 때, 실제로는 H(x) - x인 F(x)를 학습시키고, 결과적으로는 F(x) + x 형태로 표현한다.  
이렇게 하면 학습이 쉬워질 수 있으며, 최적 함수가 항등함수(identity mapping)에 가까울 경우 특히 효과적이다.

학습 정확도의 저하는, 모든 네트워크 구조가 똑같이 최적화되기 쉬운 것은 아님을 시사한다.  
더 얕은 구조와 그에 기반한 더 깊은 구조(즉, 몇 개의 층을 추가한 구조)를 생각해보자.  
깊은 모델에는 '구성적으로 존재하는 솔루션'이 존재한다. 이는 추가된 층들이 항등 함수(identity function)로 동작하고, 나머지 층들은 얕은 모델에서 학습된 것을 그대로 복사한 경우이다.  
이러한 솔루션이 존재한다는 것은 **더 깊은 모델이 더 얕은 모델보다 학습 오차가 높아지지 않아야 함을 의미**한다.  
하지만 실험 결과는 현재 우리가 사용하는 학습 알고리즘이 그러한 솔루션을 **찾지 못하거나, 현실적인 시간 내에 찾지 못한다**는 것을 보여준다.

이 논문에서는 이러한 **성능 저하 문제(degradation problem)** 를 해결하기 위해 **깊은 잔차 학습 프레임워크(deep residual learning framework)** 를 도입한다.  
단순히 층들을 쌓아 원하는 함수를 직접 학습하도록 하는 대신, 우리는 이 층들이 **잔차 함수(residual mapping)** 를 학습하도록 한다.  
정확히 말하면, 원하는 함수가 H(x)일 때, 우리는 F(x) := H(x) − x 라는 새로운 함수를 학습시키며, 전체 함수는 F(x) + x 형태가 된다.  
우리는 **잔차 함수**를 학습하는 것이 기존의 원래 함수를 학습하는 것보다 **최적화가 더 쉽다**고 가정한다.  
예를 들어, 만약 항등 함수(identity mapping)가 최적인 경우라면, 비선형 층을 통해 항등 함수를 억지로 학습하는 것보다, 잔차를 0으로 만드는 것이 훨씬 더 쉽다.

이러한 F(x) + x 형태의 수식은 **"단축 연결(shortcut connection)"** 을 가진 순방향 신경망을 통해 구현할 수 있다 (그림 2 참고).  
단축 연결은 하나 또는 그 이상의 층을 건너뛰는 연결을 말한다.  
이 경우 단축 연결은 단순히 **항등 함수**를 수행하고, 그 출력은 층들을 통해 나온 출력과 더해진다.  
항등 단축 연결은 **추가적인 파라미터나 연산 복잡도를 증가시키지 않으며**, 전체 네트워크는 기존의 방법대로 SGD와 역전파를 사용해 **엔드 투 엔드 학습이 가능**하다.  
또한, Caffe [19] 같은 일반적인 딥러닝 프레임워크를 사용하여 손쉽게 구현 가능하다.

우리는 ImageNet 데이터셋 [36]에 대해 **학습 성능 저하 문제를 보여주고**, 우리의 방법을 평가한다.  
다음과 같은 결과를 제시한다:

1. 매우 깊은 **Residual Net은 최적화가 쉽다.** 반면 단순히 층을 쌓은 일반 네트워크는 깊어질수록 학습 오차가 높아진다.
    
2. 깊은 Residual Net은 **깊이가 증가함에 따라 성능이 향상되며**, 기존 네트워크보다 훨씬 더 좋은 결과를 낼 수 있다.
    

CIFAR-10 [20] 데이터셋에 대해서도 유사한 결과를 보이며, 이러한 최적화 문제와 Residual Learning의 효과가 특정 데이터셋에만 한정되지 않음을 보여준다.  
우리는 **100층 이상의 모델**을 안정적으로 학습하였고, **1000층 이상의 모델**도 실험적으로 탐구하였다.

ImageNet 분류 과제에서 우리는 **152층 Residual Net**을 사용해 우수한 결과를 얻었다. 이는 지금까지 발표된 ImageNet용 네트워크 중 가장 깊은 구조이며, VGG보다 **복잡도는 낮다**.  
우리의 앙상블은 ImageNet 테스트 셋에서 top-5 에러율 3.57%를 기록해 ILSVRC 2015에서 1위를 차지하였다.  
이처럼 매우 깊은 표현은 다른 인식 과제에서도 뛰어난 일반화 성능 을 보여주었고, ImageNet 검출, ImageNet 위치 추정, COCO 검출, COCO 분할에서도 1위를 차지하게 되었다.

이러한 강력한 결과는 잔차 학습 원리가 **범용적이며**, 컴퓨터 비전 이외의 문제에도 적용 가능할 것임을 시사한다.

## **2. 관련 연구 (Related Work)**

#### **Residual Representations (잔차 표현)**

이미지 인식 분야에서 **[[../10_Concepts/Computer_Vision/FeatureEngineering/FeatureEncoding/VLAD & Fisher Vector#^1a6a27|VLAD]]** 는 사전 정의된 딕셔너리에 대해 잔차 벡터(residual vector) 를 인코딩하는 표현 방식이며, **[[../10_Concepts/Computer_Vision/FeatureEngineering/FeatureEncoding/VLAD & Fisher Vector#^0450ff|Fisher Vector]]** 는 VLAD의 확률적 버전으로 볼 수 있다.  
이들 두 방법은 **이미지 검색과 분류**에서 강력한 성능을 보이는 **얕은 표현(shallow representation)** 들이다 [4, 48].

벡터 양자화(vector quantization)에서도, **원본 벡터 자체보다 잔차 벡터를 인코딩하는 방식이 더 효과적**이라는 것이 밝혀졌다 [17].

저수준 비전과 컴퓨터 그래픽스 분야에서, **편미분방정식(PDE)** 을 푸는 데 널리 사용되는 **[[Multi-Grid Method|Multigrid 방법]] [3]** 은 문제를 여러 스케일의 하위 문제로 재구성하여, 각 하위 문제는 더 조밀한(finer) 스케일과 더 성긴(coarser) 스케일 사이의 **잔차 해**를 담당하게 한다.

Multigrid의 대안인 [[Hierarchical Basis Preconditioning|계층적 기저 사전조건화(Hierarchical Basis Preconditioning)]]  역시 두 스케일 사이의 잔차 벡터를 나타내는 변수를 활용한다.  
이러한 방법들은 **잔차 구조를 인식한 해법이 표준 해법보다 훨씬 더 빠르게 수렴**함을 보여주었으며,  
이처럼 **좋은 재구성 혹은 전처리(preconditioning)** 는 **최적화를 단순화할 수 있음을** 시사한다.

---
#### ✅ 최종 정리

| 관점          | Fisher Vector           | Hierarchical Basis Preconditioning |
| ----------- | ----------------------- | ---------------------------------- |
| ResNet과의 관계 | **개념적 유사성 O**, 직접적 차용 X | **이론적으로 유사성 O**, 직접적 차용 X          |
| 공통점         | 잔차(Residual) 또는 변화량 학습  | 저주파-고주파 분리(잔차 학습)                  |
| 실제 설계 의도    | 관련 없음 (CNN 맥락에서 독립적 발전) | 관련 없음 (후속 연구 해석에서 유사성 언급)          |
👉 **즉, ResNet은 이 둘의 “컨셉과 비슷한 효과를 자연스럽게 낸다”고 볼 수 있지만, 설계 철학이 Fisher Vector나 HBP에서 출발한 것은 아닙니다.**

---
#### **Shortcut Connections (단축 연결)**

단축 연결에 대한 실용적 접근과 이론적 배경은 오랜 기간 동안 연구되어 왔다 [2, 34, 49].

초기에는 다층 퍼셉트론(MLP)을 학습시킬 때 **입력에서 출력으로 직접 연결되는 선형층을 추가**하는 방식이 사용되었다 [34, 49].

또한 [44, 24]에서는 **중간층을 보조 분류기(auxiliary classifier)** 에 직접 연결하여 **그래디언트 소실/폭주 문제**를 해결하고자 하였다.  
[39, 38, 31, 47]에서는 **층 응답(layer responses), 그래디언트, 역전파 오차** 등을 중심화(centering)하는 방법이 제안되었으며,  
이는 단축 연결을 통해 구현될 수 있다.

[44]의 “Inception” 구조는 **얕은(branch) 연결과 깊은 연결을 병렬적으로 구성**하여 구현된다.

---

우리의 연구와 **동시대에 발표된 "Highway Networks" [42, 43]** 는 **게이트 함수(gating functions)** 를 사용하는 **단축 연결**을 제안했다 [15].  
이 게이트들은 **데이터 기반(data-dependent)** 이며 **파라미터를 가진다**는 점에서, **우리가 제안한 항등 단축 연결(identity shortcut)** 과는 다르다.

게이트가 “닫히면” (0에 가까우면), Highway Network는 **잔차 함수가 아닌 일반적인 비잔차 함수**를 나타낸다.  
반면에, 우리의 구조에서는 항상 **잔차 함수**를 학습하며, **단축 연결은 항상 열려있고**, **모든 정보는 항상 전달된다**.  
학습해야 할 것은 단지 **추가적인 잔차 함수**뿐이다.

또한, Highway Network는 **100층 이상의 매우 깊은 네트워크**에서의 **정확도 향상을 보여주지 못했다**는 점도 다르다.

---
## **3. 깊은 잔차 학습 (Deep Residual Learning)**

### **3.1. 잔차 학습 (Residual Learning)**

H(x)를 몇 개의 층이 근사해야 하는 대상 함수라고 가정하고, x는 이 층들의 입력이라고 하자.  
여러 비선형 층이 복잡한 함수를 점차 근사할 수 있다고 가정한다면 (물론 이는 아직 완전히 증명된 가설은 아님), 이들은 **잔차 함수 H(x) - x 또한 근사 가능**하다고 볼 수 있다. (입출력이 같은 차원일 경우)

따라서 H(x)를 직접 근사하게 하지 않고, 명시적으로 **잔차 함수 F(x) := H(x) - x** 를 근사하도록 한다.  
그러면 전체 함수는 **F(x) + x** 형태가 된다.  
두 형태 모두 결국 H(x)를 근사할 수 있으나, **학습의 용이성**은 다를 수 있다.

이러한 재정의는 앞서 논의한 **성능 저하(degradation) 문제**에서 동기를 얻은 것이다 (그림 1 참조).  
앞서 언급했듯이, 추가된 층들이 항등 함수로 구성 가능하다면, 더 깊은 모델은 더 얕은 모델보다 더 나쁜 결과를 낼 이유가 없다.  
그러나 실제로는 항등 함수를 여러 비선형 층으로 근사하는 것이 어려워 보인다.

잔차 학습으로 재정의하면, **만약 항등 함수가 최적이라면**, 솔버(학습기)는 **가중치를 0에 가깝게 만들면서 항등 함수에 접근**할 수 있다.  
현실에서는 항등 함수가 최적일 가능성은 낮지만, 이러한 재정의는 **문제의 조건을 더 잘 정규화(preconditioning)** 할 수 있다.

즉, 최적 함수가 **0 함수보다는 항등 함수에 더 가까운 경우**, 항등 함수에서 **작은 변화량만 학습**하면 되므로, 완전히 새로운 함수를 학습하는 것보다 더 쉽다.  
실험 결과 (그림 7)는 잔차 함수의 출력이 일반적으로 작다는 것을 보여주며, 이는 **항등 함수가 좋은 시작점이 될 수 있음**을 시사한다.

---

### **3.2. 단축 연결에 의한 항등 매핑 (Identity Mapping by Shortcuts)**

우리는 **몇 개의 층 단위로 잔차 학습**을 적용한다.  
그림 2는 그 기본 블록을 보여준다.

수식으로 표현하면 다음과 같다:

$
  y = F(x, {Wi}) + x   (1)
$
여기서 x와 y는 해당 블록의 입력과 출력 벡터이다.  
$F(x, {Wi})$는 학습 대상인 **잔차 함수**이다.  
예를 들어 2개의 층을 사용하는 경우, **$F = W₂σ(W₁x)$** 로 표현할 수 있으며, $σ$는 ReLU이다. (편의를 위해 bias 항은 생략함)

  $F(x) + x$ 연산은 **단축 연결**과 **요소별 덧셈(element-wise addition)** 으로 구현된다.  
  ReLU는 합산 이후에 적용한다 (즉, $σ(y)$).

이러한 단축 연결은 **파라미터를 추가하지 않으며, 계산량도 증가시키지 않는다**.  
이는 실용적으로 매력적이며, **plain 네트워크와의 공정한 비교**를 가능하게 해준다.  
즉, 잔차 네트워크와 일반 네트워크는 **같은 층 수, 파라미터 수, 연산량**을 가진 상태에서 비교할 수 있다. (요소별 덧셈은 무시할 정도로 작음)

단, $x$와 $F$의 차원이 다르면 다음과 같이 선형 변환 $Wₛ$를 추가할 수 있다:
$

  y = F(x, {Wi}) + Wₛx   (2)
$
이때 $Wₛ$는 보통 1×1 컨볼루션을 사용하며, 입력/출력 채널 수를 맞춘다.  
하지만 대부분의 경우 단순한 항등 연결로도 충분하며 효율적 이므로, $Wₛ$는 필요할 때만 사용한다.
$F$ 함수의 구조는 유연하다.  
논문에서는 F를 **2층 또는 3층** 구조로 사용하였으며 (그림 5), 더 깊게 구성할 수도 있다.  
하지만 단일층일 경우에는 단지 **선형층 + skip connection**이 되기 때문에 큰 장점은 없었다.

또한 위 수식은 전결합층(fully-connected layer) 기반으로 설명했지만, 실제로는 **컨볼루션 층에 모두 적용 가능**하다.  
$F(x, {Wi})$는 여러 개의 Conv 층으로 구성되며, 요소별 덧셈은 채널별로 수행된다.

---

### **3.3. 네트워크 아키텍처 (Network Architectures)**

우리는 다양한 plain/residual 네트워크를 실험해보았으며, **일관된 결과**를 확인했다.  
여기서는 논의의 예로, ImageNet을 위한 두 가지 모델을 설명한다.

#### **Plain Network**

기본 구조는 **[[VGGNet|VGG 네트워크]] [41]의 철학**을 따르고 있으며 (그림 3 왼쪽), 주요 구조는 다음과 같다:

- 대부분의 Conv 층은 3×3 필터 사용
    
- 출력 feature map 크기가 같으면 filter 수 유지
    
- 크기를 절반으로 줄일 경우, filter 수는 2배로 늘려 연산량 유지
    
- 다운샘플링은 stride=2의 Conv 층으로 수행
    
- 마지막은 global average pooling → 1000-way FC → softmax
    

이 plain 네트워크는 **총 34층이며**, **VGG-19보다 훨씬 적은 계산량 (3.6 GFLOPs, VGG는 19.6 GFLOPs)** 을 가진다.

---

#### **Residual Network**

위의 plain 네트워크에 **단축 연결**을 추가하면, 잔차 네트워크가 된다 (그림 3 오른쪽).

- 입력/출력 차원이 같을 때는 **항등 단축 연결** 사용 (실선)
    
- 차원이 달라질 경우 (예: 채널 수 증가 시), 두 가지 옵션:
    
    - **(A)** 항등 연결에 **zero-padding** 으로 차원 맞춤 (파라미터 없음)
        
    - **(B)** 1×1 Conv 기반의 **투영 연결(projection shortcut)** 사용 (파라미터 있음)
        

특히 잔차 블록이 여러 층일 때도 효율적이며, **연산량을 늘리지 않고 깊이를 확장할 수 있는 구조**를 제공한다.

---

### **3.4 구현 (Implementation)**

ImageNet을 위한 구현은 [21, 41]의 기존 방법을 따른다.

- 이미지의 짧은 변(shorter side)은 **256~480 픽셀 사이**에서 무작위로 선택하여 **스케일 증강(scale augmentation)** 을 수행한다 [41].
    
- 그런 다음 **224×224 크롭**을 이미지(또는 수평 반전 이미지)에서 무작위로 잘라낸다.
    
- 이후 **픽셀 단위 평균값을 빼서 정규화**한다 [21].
    
- [21]의 **표준 컬러 증강(color augmentation)** 도 적용한다.
    

각 Conv 층 뒤에는 **[[Batch Normalization]] (BN) [16]** 을 적용하고, 그 다음에 활성화 함수(ReLU)를 적용한다.  
가중치 초기화는 [13]의 방식을 따르며, **모든 plain 및 residual 네트워크는 처음부터 학습**시킨다.

- **미니배치 크기**는 256
    
- 초기 **학습률**은 0.1이며, 에러율이 plateau에 도달할 때마다 **10배 감소**
    
- 총 **최대 60만 iteration (60×10⁴)** 동안 학습
    
- **Weight decay**: 0.0001
    
- **Momentum**: 0.9
    
- **Dropout**은 사용하지 않음 ([16]을 따름)
    

---

**테스트 시**, 비교 실험에서는 [21]의 **10-crop 평가 방식**을 따른다.  
최고 성능을 위한 경우, [41, 13]과 동일하게 **Fully-Convolutional 형태로 테스트**를 수행하며,  
**여러 스케일**에 대해 score를 평균낸다.  
(짧은 변이 {224, 256, 384, 480, 640}인 여러 버전의 이미지를 사용)

---

## **4. 실험 (Experiments)**

### **4.1 ImageNet 분류**

우리는 ImageNet 2012 분류 데이터셋 [36]에서 제안한 방법을 평가한다.  
이 데이터셋은 **1000개의 클래스**로 구성되며, **128만 장의 학습 이미지**와 **5만 장의 검증 이미지**,  
그리고 **최종 평가를 위한 10만 장의 테스트 이미지**로 이루어져 있다.  
결과는 **top-1 및 top-5 오류율**로 평가한다.

---

#### **Plain 네트워크 실험**

먼저 **18층 및 34층의 일반(plain) 네트워크**를 실험하였다.  
34층 구조는 그림 3(중앙)의 형태이며, 18층도 유사한 방식이다.  
구체적인 아키텍처는 표 1 참조.

표 2 결과에 따르면, **34층 plain 네트워크가 18층보다 검증 오류가 더 높았다.**  
이를 이해하기 위해 그림 4(왼쪽)에서는 두 모델의 **학습 및 검증 오류 변화**를 비교했다.

그 결과, **성능 저하(degradation) 문제**가 발생함을 확인하였다.  
34층 네트워크가 전체 학습 동안 **더 높은 학습 오류**를 기록했다.  
이는 18층 모델의 해 공간이 34층의 부분 공간(subspace)에 포함됨에도 불구하고, 학습이 제대로 되지 않았음을 의미한다.

---

이러한 최적화 문제는 **vanishing gradient** 현상 때문은 아닌 것으로 보인다.  
BN을 적용하였기 때문에 **순전파 신호는 비영이 되고**, **역전파 신호의 gradient norm도 건강한 수준**임을 확인했다.

즉, 학습은 어느 정도 작동하지만, **plain 구조는 깊어질수록 수렴 속도가 매우 느려져서** 성능 저하가 발생한다고 본다.  
이는 단순히 학습 iteration을 더 늘린다고 해결되지 않는다.

---

#### **Residual 네트워크 실험**

다음으로 18층과 34층의 **Residual Net (ResNet)** 을 실험하였다.  
기본 구조는 앞선 plain 네트워크와 동일하며, **각 3×3 conv 쌍마다 단축 연결을 추가**하였다 (그림 3 오른쪽 참조).

초기 실험에서는 **option A (항등 연결 + zero padding)** 을 사용하여,  
plain 네트워크와 **추가 파라미터 없이 동일한 조건**으로 비교하였다.

---

표 2와 그림 4(오른쪽)를 통해 다음과 같은 주요 결과를 확인할 수 있었다:

1. **잔차 학습을 적용하면 성능 저하 문제가 해결되었고**,  
    34층 ResNet이 18층보다 **2.8% 더 낮은 오류율**을 보였다.
    
2. 34층 ResNet은 plain보다 **훨씬 더 낮은 학습 오류**를 기록하였고, 검증 데이터에서도 잘 일반화되었다.  
    ⇒ 즉, **깊이에 따른 정확도 향상**이 가능해졌다.
    
3. 18층 모델의 경우에는 plain과 ResNet이 비슷한 정확도를 보였지만,  
    ResNet이 **초기 수렴 속도가 훨씬 빠름**을 확인할 수 있었다 (그림 4 오른쪽 vs 왼쪽).
    

---

#### **항등 연결 vs 투영 연결 (Identity vs Projection Shortcuts)**

우리는 앞에서 **항등 단축 연결(identity shortcut)** 만으로도 학습이 가능함을 보였다.  
다음으로 **투영 연결(projection shortcut)** 도 실험하였다.

표 3에서는 세 가지 옵션을 비교하였다:

- **(A)**: zero-padding으로 차원을 맞춤. 모든 shortcut은 파라미터 없음.
    
- **(B)**: 차원이 증가할 때는 projection 사용. 나머지는 identity.
    
- **(C)**: 모든 shortcut을 projection으로 처리.
    

결과:

- 세 가지 방식 모두 plain 네트워크보다 훨씬 더 나은 성능을 보였다.
    
- (B)는 (A)보다 약간 더 좋았다. (A)의 zero-padding은 사실상 해당 차원에서 잔차 학습이 수행되지 않기 때문.
    
- (C)는 가장 좋지만, **projection 연결 수가 많아져 파라미터와 연산량이 증가**함.
    

⇒ 결론: 투영 연결은 성능 향상에 도움이 되지만, **항등 연결만으로도 성능 저하 문제 해결에는 충분**하다.  
특히 **병목 구조(bottleneck)** 에서는 항등 연결이 더 효율적이다.

---

#### **더 깊은 Bottleneck 구조**

우리는 ImageNet에서 더 깊은 구조를 실험하기 위해 **bottleneck 디자인** 을 사용하였다.

- 각 residual 함수 F는 이제 **3개의 Conv 층**으로 구성된다 (1×1 → 3×3 → 1×1).
    
- 처음 1×1은 차원 축소, 마지막 1×1은 차원 복원
    
- 가운데 3×3이 병목층 역할을 한다
    

**그림 5**: 왼쪽은 34층용 블록, 오른쪽은 50/101/152층용 bottleneck 블록

특히, bottleneck에서는 **항등 shortcut이 projection shortcut보다 2배 더 효율적**이다.  
따라서 ResNet-50 이상에서는 **항등 shortcut을 가능한 한 많이 사용**한다.

---

#### **50 / 101 / 152층 ResNet**

- **50층 ResNet**: 34층의 각 2층 블록을 3층 bottleneck 블록으로 교체 (옵션 B 사용)
    
- **101층 / 152층 ResNet**: 더 많은 3층 블록 사용
    

152층 ResNet조차도 **VGG-16/19보다 복잡도는 낮다**.  
(11.3 GFLOPs vs VGG-19의 19.6 GFLOPs)

이러한 매우 깊은 모델들은 **34층보다 훨씬 더 나은 정확도**를 보였으며,  
**성능 저하 문제도 나타나지 않음**을 확인하였다.  
⇒ 깊이가 증가할수록 성능 향상이 분명히 나타남.

---

### **4.2 CIFAR-10 및 분석**

우리는 추가적인 연구로 **CIFAR-10** 데이터셋 [20]을 사용하여 실험하였다.  
이 데이터셋은 **50,000개의 학습 이미지**와 **10,000개의 테스트 이미지**로 구성되어 있으며, 총 **10개의 클래스**를 포함한다.

우리는 이 실험에서 **최고 성능을 달성하는 것이 목표가 아니라**,  
**매우 깊은 네트워크의 동작 특성**을 분석하는 데 중점을 두었다.  
따라서 구조는 단순하게 설계하였다:

---

#### **네트워크 아키텍처**

Plain 및 Residual 구조는 **그림 3(중앙 및 오른쪽)** 구조를 기반으로 한다.  
입력은 **32×32 크기의 이미지**이며, 픽셀 평균값을 뺀 뒤 입력한다.

- 첫 번째 층은 **3×3 conv**
    
- 그 다음은 {32, 16, 8} 해상도의 feature map에서 각각 **2n개씩**, 총 **6n개의 3×3 conv 층** 사용
    
- 필터 수는 각각 {16, 32, 64}
    
- 다운샘플링은 stride=2의 conv로 수행
    
- 마지막은 **global average pooling → 10-way FC → softmax**
    

총 **6n + 2개의 학습 층(weighted layers)** 을 가진다.

표로 요약하면:

|Feature map 크기|층 수|필터 수|
|---|---|---|
|32×32|1 + 2n|16|
|16×16|2n|32|
|8×8|2n|64|

**단축 연결**은 3×3 conv 두 개마다 적용하며, 총 **3n개의 shortcut**이 생긴다.  
CIFAR-10에서는 모두 **항등 shortcut (옵션 A)** 만 사용하였기 때문에,  
plain과 residual 모델은 **깊이, 폭, 파라미터 수가 완전히 동일**하다.

---

#### **학습 설정**

- Weight decay: 0.0001
    
- Momentum: 0.9
    
- 가중치 초기화: [13]
    
- Batch Normalization 적용
    
- **Dropout은 사용하지 않음**
    
- 미니배치 크기: 128 (GPU 2개 사용)
    
- 초기 학습률: 0.1
    
    - 32k iteration 이후 ×0.1
        
    - 48k iteration 이후 ×0.1
        
    - 총 64k iteration 학습  
        (45k/5k 학습/검증 분할에서 결정)
        
- **데이터 증강**: [24]의 간단한 방법 사용
    
    - 각 변에 4픽셀 패딩 후
        
    - 32×32 크롭 또는 수평 반전 이미지 사용
        
- **테스트**: 원본 32×32 이미지를 그대로 평가 (single view)
    

---

#### **결과 및 분석**

우리는 n={3, 5, 7, 9}에 대해 실험하여 각각 **20, 32, 44, 56층 네트워크**를 만들었다.

- **그림 6(왼쪽)**: plain 네트워크는 깊어질수록 **학습 오류가 증가**
    
    - CIFAR-10에서도 ImageNet에서 보았던 **최적화 문제** 발생
        
    - MNIST에서도 유사한 현상이 보고됨 ([42])
        
- **그림 6(중앙)**: ResNet은 **이 최적화 문제를 해결**하며,
    
    - 깊이가 증가할수록 **정확도도 향상됨**
        

---

#### **110층 ResNet**

우리는 n=18인 구조를 추가로 실험하여, **총 110층의 ResNet**을 구성하였다.

이 경우, 초기 학습률 0.1로는 수렴이 다소 느려

- 처음 400 iteration은 0.01로 “워밍업”
    
- 이후 0.1로 복귀하여 나머지를 학습
    

그 결과, 110층 네트워크는 잘 수렴하였고 (그림 6 중앙),  
**파라미터 수는 FitNet [35], Highway [42]보다 작지만, 정확도는 최고 수준(6.43%)** 이었다.  
(표 6 참조)

---

#### **잔차 응답 분석**

그림 7은 **각 층의 출력(응답)의 표준편차(std)** 를 보여준다.

- 응답은 **BN 이후, ReLU 이전의 feature map 출력**
    
- ResNet은 일반 네트워크보다 **출력의 규모가 작음**
    
- 이는 잔차 함수 F(x)가 대체로 0에 가까움을 시사하며,  
    우리가 기대한 **항등함수 기반의 근사 구조**가 잘 작동함을 보여줌
    

또한 깊이가 깊어질수록 (ResNet-20, 56, 110 비교),  
**각 개별 층의 변화량(출력 규모)은 작아지는 경향**이 있었다.

---

#### **1000층 이상 탐색: ResNet-1202**

우리는 n=200을 설정하여, **총 1202층의 ResNet**을 학습하였다.

- **최적화 문제는 전혀 없음**
    
- 학습 오류는 **0.1% 이하**까지 낮아짐 (그림 6 오른쪽)
    

그러나 테스트 오류는 **7.93%** 로, 오히려 **110층보다 떨어졌다**

→ 원인: **과적합(overfitting)** 으로 추정됨  
→ CIFAR-10은 작은 데이터셋이므로, **19.4M 파라미터**는 과도할 수 있음

우리는 maxout [10]이나 dropout [14] 같은 **강한 regularization 없이**,  
단순히 **얇고 깊은 구조로만 학습**하였다.  
향후에는 **정규화 기법과 병행하여 더 높은 성능도 기대**할 수 있다.

---

### **4.3 PASCAL 및 MS COCO에서의 객체 검출**

우리의 방법은 **다른 인식 과제**에서도 **우수한 일반화 성능**을 보여준다.  
표 7과 표 8은 **PASCAL VOC 2007/2012**와 **MS COCO**에서의 객체 검출 성능을 나타낸다.

우리는 기본 객체 검출 프레임워크로 **[[Faster R-CNN]] [32]** 을 사용하였다.

---

#### **PASCAL VOC**

- VOC 2007 테스트 셋:  
    학습은 VOC 2007 trainval (5,000장) + VOC 2012 trainval (16,000장),  
    즉 “**07+12**” 설정 사용.
    
- VOC 2012 테스트 셋:  
    학습은 VOC 2007 trainval+test (10,000장) + VOC 2012 trainval (16,000장),  
    즉 “**07++12**” 설정 사용.
    
- 학습 하이퍼파라미터는 [32]의 Faster R-CNN 기본 설정을 따름.
    

**표 7** 결과:  
기존의 VGG-16 기반 Faster R-CNN과 비교했을 때,  
**ResNet-101은 mAP를 3% 이상 향상**시켰다.  
이 향상은 네트워크의 더 나은 표현 학습 능력 덕분이며, **다른 요소는 동일**하다.

---

#### **MS COCO**

- COCO는 총 **80개의 객체 클래스**를 포함한다.
    
- 평가 지표는 다음 두 가지:
    
    - **mAP@0.5** (PASCAL VOC 스타일)
        
    - **mAP@[0.5:0.95]** (COCO 공식 지표, 평균 IoU 0.5~0.95)
        
- 학습은 COCO train (80k 이미지),  
    평가는 COCO val (40k 이미지)에서 수행.
    
- Faster R-CNN 구조는 PASCAL VOC 실험과 동일하며,  
    단지 네트워크 백본만 VGG-16 → ResNet-101로 변경.
    

**표 8** 결과:

- ResNet-101은 **mAP@[0.5:0.95]를 6% 절대 향상**,  
    즉 **상대적으로 28% 향상**시켰다.
    
- mAP@0.5 또한 6.9% 증가.
    

⇒ 이는 **더 깊은 네트워크가 단지 인식 정확도만이 아니라, 위치 정확도(localization)에도 도움이 됨**을 시사한다.

---

#### **ILSVRC & COCO 2015 대회 수상**

우리는 **Deep Residual Networks (ResNets)** 을 기반으로 다음 분야에서 **1위를 달성**하였다:

- ImageNet Detection (객체 검출)
    
- ImageNet Localization (위치 추정)
    
- COCO Detection (객체 검출)
    
- COCO Segmentation (객체 분할)
    

→ 이러한 결과는 잔차 학습(Residual Learning)의 **범용성, 확장성, 실용성**을 강력히 뒷받침한다.

---

## **A. 객체 검출 베이스라인 (Object Detection Baselines)**

이 섹션에서는 우리가 사용한 객체 검출 구조를 설명한다.  
기반은 **Faster R-CNN [32]** 시스템이며,  
분류용으로 사전 학습된 ImageNet 모델을 초기화한 뒤, **객체 검출용 데이터로 미세 조정(fine-tuning)** 을 수행한다.  
ResNet-50과 ResNet-101이 사용되었다.

---

### **fc 층이 없는 ResNet을 위한 대응 방식**

VGG-16과 달리, ResNet에는 **은닉 전결합층(fc layers)** 이 없다.  
이를 해결하기 위해 우리는 [33]의 **[[Networks on Convolutional Feature Maps]] (NoC)** 개념을 사용하였다.

#### 구체적 구조:

- 이미지 전체에 대해 conv1, conv2_x, conv3_x, conv4_x까지의 **총 91개 conv 층**을 적용하여  
    **공유된 feature map**을 계산한다.  
    이들의 stride 합은 **16픽셀**이며, 이는 VGG-16의 conv 층과 동일하다.
    
- 이 feature map은 두 네트워크에서 공유됨:
    
    1. **[[Faster R-CNN#^e27b20|Region Proposal Network]] (RPN)**: 약 300개의 후보 영역 생성
        
    2. **Fast R-CNN 검출기**: 후보 영역의 분류 및 위치 추정
        
- RPN과 Fast R-CNN의 중간에는 **RoI Pooling** 이 적용되며,  
    conv5_x 층 이후는 각 RoI 별로 독립적으로 적용된다.  
    conv5_x 이후부터는 VGG의 fc 층 역할을 대체한다.
    
- 마지막은 두 개의 sibling 출력층:
    
    1. 분류 (classification)
        
    2. 경계 상자 회귀 (box regression) [7]
        

---

### **Batch Normalization(BN) 처리 방식**

분류용 사전학습 후, **BN 통계(mean/variance)** 는 ImageNet 학습 데이터에서 **고정된 값으로 사전 계산**한다.

- 객체 검출 미세 조정 시 BN 레이어는 **고정 상태**이며,  
    학습 중 업데이트되지 않는다.
    
- 이로 인해 BN 레이어는 **상수 계수와 오프셋이 포함된 선형 연산**으로 간주된다.
    
- BN을 고정함으로써 Faster R-CNN 학습 시 **메모리 사용량을 줄일 수 있다.**
    

---

### **PASCAL VOC**

[7, 32]을 따르며:

- **VOC 2007 테스트 세트**:
    
    - 학습 데이터: VOC 2007 trainval (5k) + VOC 2012 trainval (16k)  
        → “**07+12**” 설정
        
- **VOC 2012 테스트 세트**:
    
    - 학습 데이터: VOC 2007 trainval+test (10k) + VOC 2012 trainval (16k)  
        → “**07++12**” 설정
        
- Faster R-CNN의 하이퍼파라미터는 [32]의 설정을 그대로 사용
    

**표 7**은 **ResNet-101이 VGG-16보다 mAP를 3% 이상 향상**시킴을 보여준다.  
이 향상은 오직 **ResNet 자체의 특성 덕분**이다.

---

### **MS COCO**

COCO는 **80개의 객체 클래스**를 포함한다.  
우리는 다음 두 가지 평가 지표를 사용한다:

- **mAP@0.5** (PASCAL 스타일)
    
- **mAP@[0.5:0.95]** (COCO 공식 지표, 다양한 IoU에서 평균값)
    

#### 학습 설정:

- 학습 데이터: COCO train (80k 이미지)
    
- 평가 데이터: COCO val (40k 이미지)
    
- 학습은 8개의 GPU를 사용하여 수행
    

#### 배치 크기:

- RPN 단계: 이미지 8장 (GPU 당 1장)
    
- Fast R-CNN 단계: 이미지 16장
    

#### 학습 스케줄:

- 0.001 학습률로 **240k iteration**,  
    이후 0.0001로 **80k iteration 추가**
    

**표 8**에서 확인할 수 있듯, **ResNet-101은 VGG-16 대비 mAP@[0.5:0.95]를 6%p 향상**시켰다.  
이는 **상대적으로 28% 개선**이며, 오직 네트워크 구조 차이로 인한 성능 향상이다.

---

## **B. 객체 검출 성능 향상 (Object Detection Improvements)**

완전한 정보를 위해, 우리는 **대회 참가를 위해 수행한 성능 향상 기법들**을 이 섹션에서 보고한다.  
이들은 **잔차 학습 기반의 깊은 표현**을 기반으로 하기 때문에, ResNet의 장점을 더욱 활용할 수 있다.

---

### **MS COCO**

#### **1. 박스 보정 (Box refinement)**

- 박스 보정은 [6]의 **반복적 위치 조정(iterative localization)** 방식을 부분적으로 따름.
    
- Faster R-CNN의 최종 출력은 RPN 제안 영역이 아닌, **회귀된(regressed) 박스**이다.
    
- 따라서 테스트 시, **회귀된 박스에서 다시 feature를 추출하여 새로운 분류와 박스 회귀를 수행**한다.
    
- 이렇게 얻어진 **300개의 새로운 예측과 기존의 300개 예측을 합쳐**,  
    **Non-Maximum Suppression(NMS)** 을 IoU 0.3 기준으로 수행하고,  
    **Box Voting [6]** 을 적용하여 최종 예측을 보정한다.
    

➡️ **mAP를 약 2점 향상**시킨다. (표 9 참조)

---

#### **2. 글로벌 컨텍스트 (Global context)**

- 우리는 Fast R-CNN의 **RoI 단위 분류**에서 **글로벌 컨텍스트 정보를 함께 활용**하였다.
    
- 전체 이미지의 conv feature map에서, **이미지 전체를 하나의 RoI로 간주**하여  
    **Global Spatial Pyramid Pooling [12]** 을 수행함. (single-level pyramid)
    
- 이 글로벌 특징은 기존 RoI의 특징과 **concatenate** 되고,  
    이후 **분류 및 박스 회귀 층으로 전달**된다.
    

➡️ **mAP@0.5를 약 1점 향상**시킨다. (표 9 참조)

---

#### **3. 멀티 스케일 테스트 (Multi-scale testing)**

- 기본 모델은 [32]처럼 **단일 스케일 (짧은 변 s = 600)** 로 학습/테스트
    
- 멀티 스케일 테스트는 [12, 7]에서 제안된 방식:
    
    - [33]에서는 maxout 레이어로 병합
        
- 본 논문에서는:
    
    - **멀티 스케일 학습은 수행하지 않고**,  
        **테스트 단계에서만 멀티 스케일을 적용**
        
    - **RPN에는 적용하지 않고, Fast R-CNN 단계에서만 적용**
        

##### 멀티 스케일 처리 방식:

- 이미지의 짧은 변 s ∈ {200, 400, 600, 800, 1000} 으로 resize
    
- 두 개의 인접 스케일 feature map을 선택하여,
    
- RoI pooling 및 후속 layer를 각각 적용하고,  
    **maxout으로 병합**한다 ([33] 방식)
    

➡️ **mAP를 2점 이상 향상**시킨다. (표 9 참조)

---

#### **4. 전체 결과 요약**

**표 9 (COCO 검증 세트 결과 요약):**

|설정|mAP@.5|mAP@[.5:.95]|
|---|---|---|
|Faster R-CNN (VGG-16)|41.5|21.2|
|Faster R-CNN (ResNet-101)|48.4|27.2|
|+ 박스 보정|49.9|29.9|
|+ 글로벌 컨텍스트|51.1|30.0|
|+ 멀티스케일 테스트|53.8|32.5|
|앙상블|59.0|37.4|

---

#### **5. Test-dev 결과 (최종 평가 세트)**

- 학습: train + val (총 120k 이미지)
    
- 평가: test-dev (20k 이미지)
    
- 결과는 평가 서버에서 제공
    

**최종 성능:**

|설정|mAP@.5|mAP@[.5:.95]|
|---|---|---|
|단일 모델|55.7|34.9|
|앙상블(3개)|59.0|37.4|

➡️ 이 결과는 **COCO 2015 검출 대회에서 1위**를 차지하였다.

---

## **C. 이미지넷 위치추정 (ImageNet Localization)**

ImageNet Localization (LOC) 과제는 이미지를 분류하는 동시에, **해당 객체의 위치(바운딩 박스)** 를 예측하는 문제이다.  
[40, 41]을 따르며, 우리는 **이미지 분류기(classifier)** 를 먼저 사용하여 이미지의 클래스를 예측하고,  
이후 **바운딩 박스 회귀기(localizer)** 가 예측된 클래스에 대해 위치를 추정하는 방식을 채택하였다.

---

### **Per-Class Regression 전략**

우리는 [40, 41]에서 사용된 **클래스별 회귀 (Per-Class Regression, PCR)** 전략을 채택하였다.  
즉, 각 클래스마다 독립적인 바운딩 박스 회귀기를 학습시킨다.

ImageNet 분류용으로 사전 학습된 모델을 바탕으로 **Localization 용으로 다시 학습(fine-tune)** 한다.  
학습은 ImageNet의 **1000개 클래스 전체 학습 데이터**를 사용하였다.

---

### **우리의 Localization 네트워크 구조**

우리는 [32]의 **Region Proposal Network (RPN)** 프레임워크를 기반으로,  
다음과 같은 **수정된 구조**를 적용하였다:

- [32]는 **클래스 무관 객체 탐지(category-agnostic RPN)** 인 반면,  
    우리는 **클래스별 객체 탐지(class-specific RPN)** 를 수행함.
    

#### 구체적 변경 사항:

- RPN의 출력은 두 개의 1×1 conv로 구성:
    
    - **cls**: 1000개의 이진 분류 출력 (각 클래스가 해당되는지 여부)
        
    - **reg**: 1000×4개의 회귀 출력 (각 클래스마다 바운딩 박스 좌표 4개)
        
- 바운딩 박스 회귀는 여전히 **anchor box 기반 회귀**로 이루어짐.
    

---

### **학습 과정**

- 데이터 증강: 분류와 동일하게 **224×224 크롭을 무작위로 선택**
    
- 미니배치 크기: 256 이미지
    
- 각 이미지당 **8개의 anchor** 를 무작위로 샘플링 (positive/negative 비율 1:1)
    

→ 이렇게 하면, **학습 시 데이터 균형 유지 및 다양성 확보**가 가능

- 테스트 시, 전체 이미지에 대해 **완전 합성곱 방식(fully convolutional)** 으로 네트워크 적용
    

---

### **Localization 결과 비교**

표 13은 localization 성능을 보여준다.  
[41]에서는 분류가 이미 알려진 상태에서 테스트(oracle 테스트)를 수행하였고,  
**VGG-16이 center crop 기준 33.1% localization 오류율**을 기록하였다.

우리의 ResNet-101 기반 RPN은 **center crop 기준 오류율 13.3%로 크게 개선**되었다.

- **멀티스케일 fully-convolutional 테스트** 적용 시:  
    오류율 **11.7%** (여전히 GT 클래스 사용)
    
- 예측된 클래스 기반 테스트에서는:
    
    - **ResNet-101 분류기 (top-5 분류 오류율 4.6%)** 사용
        
    - localization 오류율: **14.4%**
        

---

### **R-CNN 기반 추가 향상**

기본 RPN 외에도, 우리는 추가적으로 **Fast R-CNN 대신 R-CNN 구조를 적용**하였다.  
그 이유는 다음과 같다:

- ImageNet의 LOC 데이터는 일반적으로 **한 장의 이미지에 한 개의 객체**만 있음.
    
- RoI들이 거의 **중첩(overlap)** 되어 있어, RoI Pooling된 feature들이 **서로 매우 유사**
    
- 이런 경우, **RoI 중심의 샘플링이 아닌, 이미지 중심의 Fast R-CNN 학습은 학습 다양성이 부족**
    

➡️ 그래서 우리는 RoI 중심 학습이 가능한 **기존 R-CNN 구조 [8]** 를 사용하였다.

#### 구현 방식:

- 먼저, 사전 학습된 RPN을 이용하여 **GT 클래스에 대해 바운딩 박스를 예측**
    
- 이 중 가장 높은 점수를 가진 **상위 200개 proposal**을 추출하여 R-CNN 학습용 데이터로 사용
    
- 각 박스는 잘라낸 뒤 224×224로 resize, 이후 ResNet-101에 입력
    
- 출력은 1000개 클래스 각각에 대해 분류/회귀를 수행하는 **per-class classifier & regressor**
    

➡️ 테스트 시, RPN이 예측한 **클래스별 200개 박스를 R-CNN으로 다시 정제**

---

### **최종 성능**

- 단일 모델로 **top-5 localization 오류율 10.6%** 달성 (검증 세트 기준)
    
- 앙상블 적용 시, **테스트 세트에서 top-5 오류율 9.0%** 기록
    

➡️ 이는 ILSVRC 2014 기록 대비 **오류율을 64% 상대적으로 감소**시킨 성과이며,  
  **ImageNet Localization 과제에서 2015년 1위**를 차지하였다.

**표 14**는 이전 대표 모델들과의 성능 비교를 요약한 표이다.

---

이로써 본 논문의 본문과 부록에 수록된 모든 주요 내용이 한글로 번역되었습니다.  
전체 내용을 다시 요약하거나, 특정 부분에 대해 시각적 정리 혹은 코드 구현 예시 등을 원하신다면 말씀해 주세요.
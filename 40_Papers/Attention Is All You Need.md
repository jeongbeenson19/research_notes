---
title: "Attention Is All You Need"
authors: 
zoteroKey: vaswaniAttentionAllYou2023
zoteroURL: 
tags: [zotero, preprint]
created: 1752740640000
modified: 1752740640000
---

# Attention Is All You Need

> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

## 📘 메타데이터
- 저자: 
- 연도: 1690902000000
- 저널명: 
- 출판사: 
- DOI: 10.48550/arXiv.1706.03762
- Zotero 링크: [열기]()

### 초록(Abstract)

기존의 시퀀스 변환(sequence transduction) 모델은 복잡한 순환 신경망(Recurrent Neural Network, RNN) 또는 합성곱 신경망(Convolutional Neural Network, CNN)을 기반으로 하며, 인코더와 디코더를 포함한다. 최고 성능의 모델은 또한 주의(attention) 메커니즘을 통해 인코더와 디코더를 연결한다. 본 연구에서는 전적으로 주의 메커니즘에만 의존하며, 순환과 합성곱을 완전히 배제하는 새로운 간단한 네트워크 아키텍처인 **Transformer**를 제안한다. 두 개의 기계 번역 과제에 대한 실험 결과, 이 모델은 품질 면에서 우수하며 병렬화가 가능하고 학습 시간이 크게 단축됨을 보여준다. 우리의 모델은 WMT 2014 영어-독일어 번역 과제에서 BLEU 점수 28.4를 기록하였으며, 기존 최고 성능(앙상블 포함) 대비 2 BLEU 이상 향상되었다. 또한 WMT 2014 영어-프랑스어 번역 과제에서는 8개의 GPU로 3.5일 학습 후 단일 모델 기준 BLEU 점수 41.8을 달성하여 기존 최고 성능을 갱신했다. 우리의 모델은 영어 구문 분석(English constituency parsing)에도 성공적으로 적용되어, 대규모 및 제한된 학습 데이터 모두에서 좋은 일반화 성능을 보였다.

### 1. 서론(Introduction)

순환 신경망(Recurrent Neural Network, RNN), 특히 장단기 메모리(Long Short-Term Memory, LSTM) [13]와 게이트드 순환 신경망(Gated Recurrent Neural Network, GRU) [7]은 언어 모델링과 기계 번역 [35, 2, 5]과 같은 시퀀스 모델링 및 변환 문제에서 확고히 최첨단(state-of-the-art) 접근법으로 자리 잡았다. 이후 수많은 연구가 순환 언어 모델과 인코더-디코더 아키텍처의 성능 향상 [38, 24, 15]을 위해 진행되었다.

순환 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치에 따라 연산을 분해한다. 위치를 연산 시간 단계에 정렬하여, 이전 은닉 상태 $h_{t-1}$와 해당 위치 $t$의 입력에 따라 은닉 상태 $h_t$의 시퀀스를 생성한다. 이러한 **본질적으로 순차적인 특성**은 학습 예제 내 병렬화를 어렵게 만들며, 이는 특히 긴 시퀀스 길이에서 메모리 제약으로 인해 예제 간 배칭(batch processing)을 제한하는 중요한 문제가 된다. 최근 연구들은 분해 기법 [21]과 조건부 연산 [32]을 통해 계산 효율성을 크게 향상시키고(특히 후자의 경우 성능도 개선), 이러한 문제를 완화하고자 하였다. 그러나 **순차 연산의 근본적인 제약**은 여전히 남아 있다.

주의(attention) 메커니즘은 입력과 출력 시퀀스에서의 거리와 관계없이 의존성을 모델링할 수 있도록 하여, 다양한 작업의 시퀀스 모델링 및 변환 모델에서 핵심적인 요소가 되었다 [2, 19]. 그러나 대부분의 경우 [27]를 제외하면, 이러한 주의 메커니즘은 순환 네트워크와 함께 사용되었다.

본 연구에서는 순환 구조를 배제하고 오직 주의 메커니즘만으로 입력과 출력 간의 전역 의존성을 학습하는 **Transformer** 모델 아키텍처를 제안한다. Transformer는 **병렬화를 크게 향상**시키며, 단 8개의 P100 GPU로 약 12시간 학습만으로 번역 품질에서 새로운 최첨단 성능을 달성할 수 있다.

### 2. 배경(Background)

순차 연산을 줄이려는 목표는 **Extended Neural GPU** [16], **ByteNet** [18], **ConvS2S** [9]와 같은 모델의 기초가 되었다. 이들은 기본 구성 요소로 합성곱 신경망(CNN)을 사용하여 입력 및 출력의 모든 위치에 대해 병렬로 은닉 표현을 계산한다. 그러나 이러한 모델에서 임의의 두 입력 또는 출력 위치 간의 신호를 연결하는 데 필요한 연산 수는 위치 간 거리에 따라 증가한다. 예를 들어, ConvS2S에서는 선형적으로, ByteNet에서는 로그 비율로 증가한다. 이는 **멀리 떨어진 위치 간의 의존성 학습을 어렵게** 만든다 [12]. 반면, Transformer에서는 이 연산 수가 **상수**로 줄어들지만, 주의 가중치가 적용된 위치의 평균화로 인해 **효과적인 해상도가 낮아지는** 문제가 발생할 수 있다. 우리는 이를 **다중 헤드 주의(Multi-Head Attention)** 를 통해 해결한다(3.2절 참조).

**자기-주의(Self-Attention)**, 혹은 **내부 주의(Intra-Attention)** 는 단일 시퀀스의 서로 다른 위치를 연결하여 해당 시퀀스의 표현을 계산하는 주의 메커니즘이다. 자기-주의는 독해, 추상적 요약, 텍스트 함의, 작업 독립적인 문장 표현 학습 등 다양한 작업에서 성공적으로 사용되어 왔다 [4, 27, 28, 22].

**엔드-투-엔드 메모리 네트워크(End-to-End Memory Networks)** 는 시퀀스 정렬된 순환 대신 **순환형 주의 메커니즘**을 기반으로 하며, 단순한 언어 질의응답 및 언어 모델링 작업에서 좋은 성능을 보였다 [34].

그러나 우리가 아는 한, Transformer는 입력과 출력의 표현을 계산하는 데 시퀀스 정렬된 RNN이나 합성곱을 사용하지 않고 전적으로 자기-주의에만 의존하는 최초의 변환 모델이다. 다음 섹션에서는 Transformer를 설명하고, 자기-주의의 동기를 부여하며, 이를 [17, 18, 9]와 같은 기존 모델과 비교한 장점을 논의한다.

### 3. 모델 아키텍처(Model Architecture)

대부분의 경쟁력 있는 신경망 기반 시퀀스 변환 모델은 **인코더-디코더 구조**를 갖는다 [5, 2, 35]. 인코더는 입력 기호 시퀀스 $(x_1, ..., x_n)$를 연속적인 표현 시퀀스 $z = (z_1, ..., z_n)$로 변환한다. 이때 디코더는 $z$를 기반으로 출력 기호 시퀀스 $(y_1, ..., y_m)$를 한 번에 한 요소씩 생성한다. 각 단계에서 모델은 **자기회귀(auto-regressive)** 방식으로, 이전에 생성된 기호를 추가 입력으로 사용하여 다음 기호를 생성한다 [10].

Transformer는 이 기본 구조를 따르지만, 인코더와 디코더 모두에서 **스택된 자기-주의(Self-Attention)** 와 **위치별 완전연결(Point-wise Fully Connected) 계층**을 사용한다(그림 1 참조).
![[스크린샷 2025-07-18 오전 11.21.22.png]]

### 3.1 인코더 및 디코더 스택(Encoder and Decoder Stacks)

#### **인코더(Encoder)**

인코더는 $N = 6$개의 동일한 층으로 구성된다. 각 층은 두 개의 하위 계층(sub-layer)을 갖는다:

1. **다중 헤드 자기-주의(Multi-Head Self-Attention)** 메커니즘
    
2. **위치별 완전연결(feed-forward) 네트워크**
    

각 하위 계층 주위에는 **잔차 연결(Residual Connection)** [11]이 적용되고, 이후 **층 정규화(Layer Normalization)** [1]가 적용된다. 즉, 각 하위 계층의 출력은 다음과 같이 계산된다.

$$\text{LayerNorm}(x + \text{Sublayer}(x))$$

여기서 $\text{Sublayer}(x)$는 해당 하위 계층의 함수이다. 이러한 **잔차 연결을 가능하게 하기 위해**, 모델의 모든 하위 계층과 임베딩 계층은 **동일한 차원 $d_{\text{model}} = 512$을 출력**한다.

---

#### **디코더(Decoder)**

디코더 역시 $N = 6$개의 동일한 층으로 구성된다. 각 층은 인코더 층과 같은 두 개의 하위 계층 외에 세 번째 하위 계층이 추가되며, 이는 **인코더 출력에 대한 다중 헤드 주의**를 수행한다. 인코더와 마찬가지로, 각 하위 계층 주위에는 잔차 연결과 층 정규화가 적용된다.

또한 디코더의 자기-주의 하위 계층은 **미래 위치를 참조하지 못하도록 마스킹(masking)** 된다. 이는 출력 임베딩이 한 위치만큼 오프셋(offset)되는 것과 결합되어, $i$번째 위치의 예측이 $i$보다 작은 위치의 출력에만 의존하도록 보장한다.

---

### 3.2 주의(Attention)

주의 함수는 **쿼리(Query)** 와 **키-값(Key-Value) 쌍의 집합**을 출력으로 매핑하는 것으로 정의된다. 이때 쿼리, 키, 값, 출력은 모두 벡터이다. 출력은 값들의 가중합으로 계산되며, 각 값에 할당된 가중치는 해당 키와 쿼리 간의 **호환성 함수(compatibility function)** 를 통해 계산된다.

---
![[스크린샷 2025-07-18 오전 11.18.45.png]]
#### **3.2.1 스케일드 닷-프로덕트 어텐션([[Scaled Dot-Product Attention]])**

본 연구에서는 그림 2(좌)와 같은 **스케일드 닷-프로덕트 어텐션**을 사용한다. 입력으로는 쿼리와 키가 $d_k$ 차원, 값이 $d_v$ 차원으로 주어진다.  
쿼리와 모든 키의 내적(dot product)을 계산한 후, 이를 $\sqrt{d_k}$로 나누고, softmax 함수를 적용하여 값의 가중치를 얻는다.

실제 구현에서는 여러 쿼리를 동시에 처리하기 위해, 쿼리 $Q$, 키 $K$, 값 $V$를 각각 행렬로 묶어 연산한다. 출력은 다음과 같이 계산된다.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V \tag{1}
$$

가장 널리 사용되는 주의 함수는 **가산 어텐션(Additive Attention)** [2]과 **닷-프로덕트 어텐션(Dot-Product Attention)** 이다. 닷-프로덕트 어텐션은 위의 식과 동일하나, 스케일링이 없다. 가산 어텐션은 단일 은닉층을 가진 전방향(feed-forward) 신경망을 사용하여 호환성을 계산한다. 두 방식은 이론적 복잡도는 유사하나, 닷-프로덕트 어텐션은 고도로 최적화된 행렬 곱셈 코드를 사용할 수 있어 **훨씬 빠르고 메모리 효율적**이다.

그러나 $d_k$가 큰 경우, 스케일링이 없는 닷-프로덕트 어텐션은 softmax의 기울기가 매우 작은 영역으로 들어가 학습이 어려워진다 [3]. 이를 완화하기 위해 우리는 내적을 $\sqrt{d_k}$로 나누어 스케일링한다.

---

#### **3.2.2 다중 헤드 어텐션(Multi-Head Attention)**

단일 주의 함수 대신, 쿼리, 키, 값을 각각 $h$개의 다른 학습된 선형 투영으로 $d_k, d_k, d_v$ 차원으로 사영한다. 그런 다음, 이 사영된 쿼리, 키, 값에 대해 **주의 함수를 병렬로 수행**하며, 각 주의 출력은 $d_v$-차원의 값으로 나타난다. 이 출력들은 연결(concatenate)된 후 다시 사영되어 최종 출력이 된다(그림 2 우측 참조).

다중 헤드 어텐션은 모델이 **서로 다른 표현 하위공간(subspace)** 의 정보를 서로 다른 위치에서 동시에 주의할 수 있게 한다. 단일 헤드 어텐션에서는 이러한 정보가 평균화되어 억제된다.

$$
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
\\
\text{where } \text{head}_i &= \text{Attention}(Q W^Q_i, K W^K_i, V W^V_i)
\end{align}
$$

투영 행렬은
$$
W^Q_i \in \mathbb{R}^{d_{\text{model}}\times d_k}, W^K_i \in \mathbb{R}^{d_{\text{model}}\times d_k}, W^V_i \in \mathbb{R}^{d_{\text{model}}\times d_v}, W^O \in \mathbb{R}^{hd_v\times d_{\text{model}}}이다.
$$
본 연구에서는 $h = 8$개의 병렬 주의 층(헤드)을 사용하며, 각 $d_k = d_v = d_{\text{model}} / h = 64$이다. 각 헤드의 차원이 작기 때문에 전체 계산 비용은 단일 헤드 어텐션과 유사하다.

---

#### **3.2.3 본 모델에서의 주의 활용(Applications of Attention in our Model)**

Transformer는 다음 세 가지 방식으로 다중 헤드 어텐션을 사용한다.

1. **인코더-디코더 어텐션**  
    쿼리는 디코더의 이전 층에서 오며, 키와 값은 인코더의 출력에서 온다. 이를 통해 디코더의 각 위치가 입력 시퀀스의 모든 위치를 참고할 수 있다. 이는 기존 시퀀스-투-시퀀스 모델 [38, 2, 9]에서의 인코더-디코더 주의와 유사하다.
    
2. **인코더의 자기-주의**  
    쿼리, 키, 값이 모두 인코더의 이전 층 출력에서 오며, 인코더의 각 위치가 이전 층의 모든 위치에 주의를 기울일 수 있다.
    
3. **디코더의 자기-주의**  
    디코더의 각 위치는 해당 위치까지의 모든 위치에 주의를 기울일 수 있다. **왼쪽으로의 정보 흐름을 차단**하기 위해, softmax 입력에서 불법적인 연결에 해당하는 값은 $-\infty$로 마스킹된다(그림 2 참조).
    

---

### 3.3 위치별 완전연결 네트워크(Position-wise Feed-Forward Networks)

인코더와 디코더의 각 층에는 주의 하위 계층 외에도 **완전연결(feed-forward) 네트워크**가 포함된다. 이 네트워크는 각 위치에 대해 **독립적이고 동일하게** 적용된다. 이는 두 개의 선형 변환과 그 사이의 ReLU 활성화 함수로 구성된다.

$$
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2 \tag{2}
$$
여기서 선형 변환은 위치마다 동일하지만, 층마다 다른 파라미터를 사용한다. 다른 방식으로 표현하면, 이는 커널 크기가 1인 두 개의 합성곱(convolution)과 같다.  
입출력의 차원은 $d_{\text{model}} = 512$, 내부 계층의 차원은 $d_{ff} = 2048$이다.

---

### 3.4 임베딩과 소프트맥스(Embeddings and Softmax)

다른 시퀀스 변환 모델과 마찬가지로, 입력 토큰과 출력 토큰을 $d_{\text{model}}$ 차원의 벡터로 변환하기 위해 **학습된 임베딩**을 사용한다. 또한 디코더 출력은 학습된 선형 변환과 softmax 함수를 통해 **다음 토큰의 확률**로 변환된다.

우리 모델에서는 **입력 임베딩과 출력 임베딩**, 그리고 softmax 이전의 선형 변환에서 **동일한 가중치 행렬**을 공유한다 [30]. 임베딩 층에서는 이 가중치에 $\sqrt{d_{\text{model}}}$를 곱한다.

---

### 3.5 위치 인코딩(Positional Encoding)

본 모델은 순환과 합성곱을 포함하지 않기 때문에, 시퀀스의 **순서 정보**를 반영하기 위해 토큰의 상대적 또는 절대적 위치 정보를 추가해야 한다. 이를 위해 인코더와 디코더 스택의 입력 임베딩에 **위치 인코딩(Positional Encoding)** 을 더한다. 위치 인코딩은 임베딩과 동일한 차원 $d_{\text{model}}$을 갖기 때문에 두 벡터를 더할 수 있다.

위치 인코딩은 학습되거나 고정될 수 있다 [9]. 본 연구에서는 서로 다른 주파수의 **사인(sine)과 코사인(cosine)** 함수를 사용한다.

$$
\begin{align}
PE(pos, 2i) &= \sin\left( \frac{pos}{10000^{2i/d_{\text{model}}}} \right) 
\\
PE(pos, 2i+1) &= \cos\left( \frac{pos}{10000^{2i/d_{\text{model}}}} \right)
\end{align}
$$
여기서 $pos$는 위치, $i$는 차원이다. 각 차원은 다른 파장의 사인파를 나타내며, 파장은 $2\pi$에서 $10000 \cdot 2\pi$까지 기하급수적으로 증가한다.  
이 방식은 고정된 오프셋 $k$에 대해 $PE_{pos+k}$가 $PE_{pos}$의 선형 함수로 표현될 수 있어 **상대적 위치 기반 학습**을 용이하게 한다는 가설에 기반한다.

우리는 학습된 위치 임베딩도 실험했으며 [9], 두 방식 모두 거의 동일한 결과를 보였다. 그러나 사인/코사인 방식은 **훈련에서 보지 못한 더 긴 시퀀스**로의 일반화가 가능할 것으로 예상되어 이를 선택했다.

---

### 4. 왜 자기-주의인가(Why Self-Attention)

이 섹션에서는 자기-주의 층과, 전통적으로 가변 길이의 기호 시퀀스를 다른 시퀀스로 매핑하는 데 사용되는 **순환(Recurrent)** 및 **합성곱(Convolutional)** 층을 비교한다. 즉, 인코더 또는 디코더의 은닉층에서 $(x_1, ..., x_n)$을 $(z_1, ..., z_n)$으로 변환하는 작업에서 자기-주의를 사용할 동기를 설명한다.

우리가 고려한 세 가지 주요 요건은 다음과 같다.

1. **층당 총 계산 복잡도(Total Computational Complexity per Layer)**
    
2. **병렬화 가능한 연산량(Parallelizable Computation)** – 즉, 필요한 최소 순차 연산 수
    
3. **장기 의존성 학습을 위한 경로 길이(Path Length for Long-Range Dependencies)**
    

장기 의존성 학습은 많은 시퀀스 변환 작업에서 핵심 도전 과제이다. 입력과 출력 시퀀스의 임의의 위치 쌍 간에 **순전파 및 역전파 신호가 통과해야 하는 경로가 짧을수록**, 장기 의존성 학습이 용이하다 [12].

---

표 1에 나타난 바와 같이, **자기-주의(Self-Attention)** 층은 **모든 위치를 상수 개수의 순차 연산으로 연결**하는 반면, 순환 층은 $O(n)$의 순차 연산이 필요하다. 계산 복잡도 측면에서, 시퀀스 길이 $n$이 표현 차원 $d$보다 작은 경우(대부분의 최신 번역 모델에서 흔함), 자기-주의 층이 순환 층보다 더 빠르다.  
(예: 단어 조각(word-piece) [38] 및 바이트 쌍(Byte-Pair) [31] 표현)

매우 긴 시퀀스의 경우 계산 성능 향상을 위해 자기-주의를 입력 시퀀스의 **크기 $r$의 국소 영역(neighborhood)** 으로 제한할 수 있다. 이 경우 최대 경로 길이는 $O(n/r)$로 증가한다. 우리는 이를 향후 연구에서 탐구할 계획이다.

---

**합성곱 층**의 경우, 커널 너비 $k < n$인 단일 층은 모든 입력 및 출력 위치 쌍을 연결하지 못한다. 연속 커널의 경우 $O(n/k)$, 팽창 합성곱(dilated convolution) [18]의 경우 $O(\log_k(n))$개의 층이 필요하다. 또한 합성곱 층은 일반적으로 순환 층보다 계산 비용이 높으며, 복잡도는 $O(k \cdot n \cdot d^2)$이다. 다만, **분리 가능 합성곱(Separable Convolutions)** [6]은 이를 크게 줄여 $O(k \cdot n \cdot d + n \cdot d^2)$이 된다. 하지만 $k = n$인 경우에도 분리 가능 합성곱의 복잡도는 **자기-주의 층과 위치별 완전연결 층의 조합**과 동일하다.

---

부가적인 이점으로, 자기-주의는 **더 해석 가능한 모델**을 제공할 수 있다. 우리는 모델의 주의 분포를 검사하였으며, 부록에서 여러 예시를 제시한다. 주의 헤드마다 서로 다른 역할을 학습했으며, 많은 경우 문장의 구문 및 의미 구조와 관련된 행동을 보였다.

---

### 5. 학습(Training)

이 섹션에서는 Transformer 모델의 학습 절차를 설명한다.

---

### 5.1 학습 데이터 및 배치(Training Data and Batching)

우리는 약 **450만 문장 쌍**으로 구성된 표준 **WMT 2014 영어-독일어** 데이터셋으로 학습하였다. 문장은 **바이트 쌍 인코딩(Byte-Pair Encoding, BPE)** [3]으로 인코딩되었으며, 약 **37,000개의 소스-타겟 공유 어휘**를 사용하였다.

영어-프랑스어의 경우, 약 **3,600만 문장**으로 구성된 더 큰 **WMT 2014 영어-프랑스어** 데이터셋을 사용하였으며, 토큰은 **32,000 단어 조각(word-piece)** 어휘 [38]로 분할하였다.

문장 쌍은 대략적인 시퀀스 길이에 따라 배치되었다. 각 학습 배치는 약 **25,000개의 소스 토큰**과 **25,000개의 타겟 토큰**을 포함하였다.

---

### 5.2 하드웨어 및 스케줄(Hardware and Schedule)

모델 학습은 **8개의 NVIDIA P100 GPU**가 장착된 단일 머신에서 수행되었다.

- **기본(Base) 모델**: 논문 전반에서 언급된 하이퍼파라미터를 사용했으며, 학습 단계당 약 **0.4초**가 소요되었다. 총 **100,000단계**(약 **12시간**) 학습하였다.
    
- **대형(Big) 모델**: 표 3 하단에 명시된 하이퍼파라미터를 사용했으며, 단계당 **1.0초**, 총 **300,000단계**(약 **3.5일**) 학습하였다.
    

---

### 5.3 옵티마이저(Optimizer)

우리는 **Adam 옵티마이저** [20]를 사용했으며, 파라미터는 다음과 같다.

$\beta_1 = 0.9,\ \beta_2 = 0.98,\ \epsilon = 10^{-9}$

학습률은 단계에 따라 아래의 공식을 사용해 조절하였다.

$$
\text{lrate} = d_{\text{model}}^{-0.5} \cdot \min(\text{step\_num}^{-0.5},\ \text{step\_num} \cdot \text{warmup\_steps}^{-1.5}) \tag{3}
$$
즉, 처음 **warmup_steps** 동안은 선형적으로 증가시키고, 이후에는 단계 수의 역제곱근에 비례해 감소시킨다. 우리는 **warmup_steps = 4000**을 사용하였다.

---

### 5.4 정규화(Regularization)

학습 시 다음의 세 가지 정규화 기법을 사용하였다.

1. **잔차 드롭아웃(Residual Dropout)**  
    각 하위 계층 출력에 드롭아웃 [33]을 적용한 후, 이를 입력과 더해 정규화하였다. 또한 인코더와 디코더 스택의 **임베딩과 위치 인코딩의 합**에도 드롭아웃을 적용하였다.  
    기본 모델에서는 $P_{\text{drop}} = 0.1$을 사용하였다.
    
2. **레이블 스무딩(Label Smoothing)**  
    레이블 스무딩 [36] 값 $\epsilon_{ls} = 0.1$을 적용하였다. 이는 모델이 덜 확신하도록 학습하게 하여 퍼플렉시티는 악화되지만, 정확도와 BLEU 점수는 향상된다.
    

---
### 6. 결과(Results)

---

### 6.1 기계 번역(Machine Translation)

**WMT 2014 영어-독일어 번역 과제**에서, 대형(Transformer (big)) 모델은 기존에 보고된 최고 성능(앙상블 포함)보다 **2.0 BLEU 이상 향상**된 **28.4 BLEU** 점수를 기록하며 새로운 최첨단 성능을 달성했다(표 2 참조). 해당 모델의 설정은 표 3의 마지막 행에 제시되어 있으며, **8개의 P100 GPU**로 **3.5일** 동안 학습되었다. 기본(Base) 모델 또한, 학습 비용이 경쟁 모델 대비 훨씬 낮음에도 불구하고 기존에 발표된 모든 단일 및 앙상블 모델을 능가했다.

**WMT 2014 영어-프랑스어 번역 과제**에서는 대형 모델이 **41.0 BLEU**를 달성하였으며, 이는 기존 최고 성능의 단일 모델을 능가한다. 또한 이전 최고 성능 모델의 학습 비용의 **1/4 이하**로 성능을 달성하였다. 영어-프랑스어 학습 시 Transformer (big) 모델은 $P_{drop} = 0.1$을 사용했으며(영어-독일어에서는 0.3 사용), 이로 인해 학습 비용이 더 낮았다.

기본 모델의 경우, **마지막 5개의 체크포인트**(10분 간격으로 저장)를 평균한 단일 모델을 사용했다. 대형 모델은 **마지막 20개의 체크포인트**를 평균했다.  
추론 시 **빔 서치(beam search)** 를 사용했으며, 빔 크기는 4, 길이 패널티 $\alpha = 0.6$ [38]을 적용하였다. 최대 출력 길이는**입력 길이 + 50** 으로 설정했으며, 조기 종료를 허용하였다.

표 2는 번역 품질과 학습 비용을 기존 모델과 비교한 것이다. 학습 비용은 학습 시간, 사용된 GPU 수, GPU의 부동소수점 연산 용량을 곱하여 추정하였다.

---

### 6.2 모델 변형(Model Variations)

Transformer의 다양한 구성 요소의 중요성을 평가하기 위해, 기본 모델을 여러 방식으로 변형하며 **영어-독일어 번역(newstest2013 개발 세트)** 에서 성능 변화를 측정하였다. 이때 빔 서치를 사용하였으나, 체크포인트 평균은 사용하지 않았다(표 3 참조).

- **표 3 (A)**: **어텐션 헤드 수**와 키/값 차원을 변경하였다. 단일 헤드 어텐션은 최적 설정 대비 **0.9 BLEU 낮았으며**, 헤드 수가 너무 많아도 성능이 저하되었다.
    
- **표 3 (B)**: 어텐션 키 크기 dkd_k를 줄이면 성능이 떨어졌다. 이는 쿼리-키 호환성 판단이 어렵기 때문이며, 단순한 내적(dot product)보다 더 정교한 호환성 함수가 도움이 될 수 있음을 시사한다.
    
- **표 3 (C), (D)**: 예상대로 **모델이 클수록 성능이 향상**되며, 드롭아웃은 과적합 방지에 유효했다.
    
- **표 3 (E)**: 사인/코사인 위치 인코딩 대신 **학습된 위치 임베딩** [9]을 사용했을 때 기본 모델과 거의 동일한 결과를 보였다.
    

---

### 6.3 영어 구문 분석(English Constituency Parsing)

Transformer가 다른 작업으로 일반화할 수 있는지 평가하기 위해 **영어 구문 분석**을 실험했다. 이 작업은 출력이 강한 구조적 제약을 받으며 입력보다 훨씬 길다는 점에서 도전적이다. 또한 RNN 기반 시퀀스-투-시퀀스 모델은 **소규모 데이터 환경**에서 최첨단 성능을 내지 못한 바 있다 [37].

우리는 $d_{\text{model}} = 1024$의 **4층 Transformer**를 사용하여, **Penn Treebank의 WSJ(40K 문장)** 으로 학습하였다. 또한 **17M 문장**으로 구성된 **high-confidence** 및 **BerkeleyParser 코퍼스** [37]를 사용한 **준지도(semi-supervised)** 학습도 수행하였다. WSJ 전용에서는 **16K 토큰**, 준지도 학습에서는 **32K 토큰** 어휘를 사용하였다.

하이퍼파라미터 튜닝은 드롭아웃(주의 및 잔차), 학습률, 빔 크기만 소규모로 진행했으며, 나머지는 영어-독일어 기본 모델과 동일했다. 추론 시 **최대 출력 길이 = 입력 길이 + 300**, 빔 크기 21, α=0.3\alpha = 0.3을 사용하였다.

표 4 결과에 따르면, **작업 특화 튜닝 없이도 Transformer는 매우 우수한 성능**을 보였으며, 특히 WSJ 데이터셋만 사용했을 때도 **Berkeley Parser** [29]를 능가했다. 준지도 학습에서는 기존 모든 모델을 초과하여 **92.7 F1**을 기록했다.

---

### 7. 결론(Conclusion)

본 연구에서는 **Transformer**를 제안하였다. 이는 전적으로 **주의(attention)** 에 기반한 최초의 시퀀스 변환(sequence transduction) 모델로, 인코더-디코더 아키텍처에서 일반적으로 사용되던 순환 계층을 **다중 헤드 자기-주의(Multi-Head Self-Attention)** 로 대체하였다.

번역 작업에서 Transformer는 **순환 또는 합성곱 계층 기반 아키텍처보다 훨씬 빠르게 학습**될 수 있다. WMT 2014 영어-독일어 및 영어-프랑스어 번역 과제 모두에서 Transformer는 **새로운 최첨단 성능(state of the art)** 을 달성했으며, 특히 영어-독일어에서는 기존에 보고된 모든 앙상블 모델을 능가하였다.

우리는 향후 **주의 기반 모델의 잠재력**에 대해 매우 기대하고 있다. 앞으로 Transformer를 텍스트 외의 **다양한 입출력 모달리티(예: 이미지, 오디오, 비디오)** 에도 적용할 계획이며, **국소적·제한된 주의 메커니즘(local, restricted attention)** 을 연구하여 큰 입력과 출력(예: 이미지, 음성, 비디오)을 효율적으로 처리하고자 한다. 또한 **생성 과정의 비순차성(decreasing sequential generation)** 을 향상시키는 것도 우리의 연구 목표이다.

이 연구에서 모델 학습과 평가에 사용된 코드는 [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)에서 확인할 수 있다.

---

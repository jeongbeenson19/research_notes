
**사람 자세 추정 및 추적을 위한 간단한 기준선 모델**

---

## **초록(Abstract)**  
최근 자세 추정에서 큰 발전이 있었고, 자세 추적에 대한 관심도 증가하고 있다. 하지만 전체 알고리즘과 시스템의 복잡성 또한 증가하여 분석 및 비교가 어려워졌다. 본 논문은 간단하고 효과적인 기준선 방법을 제시하며, 이는 새로운 아이디어를 고안하거나 평가하는 데 유용하다. 도전적인 벤치마크에서 최고 성능을 달성하였고, 코드는 공개될 예정이다.

---

## **1. 서론**  
다른 많은 비전 과제처럼, 인간 자세 추정 문제도 딥러닝에 의해 크게 발전했다. [31,30]의 선구적인 작업 이후, MPII 벤치마크에서의 성능은 약 3년 사이 80%에서 90% 이상으로 상승했다. COCO 벤치마크에서는 1년 사이 mAP가 60.5에서 72.1로 증가했다. 자세 추정이 성숙해짐에 따라, 실세계에서의 동시 자세 탐지 및 추적이라는 더욱 도전적인 과제가 등장했다.

하지만 네트워크 구조와 실험 관행은 점점 복잡해지고 있으며, 알고리즘의 분석 및 비교가 어렵다. 예를 들어, MPII 벤치마크 상위 방법들 [22,8,7,33]은 많은 세부 차이를 가지지만 성능 차이는 작다. COCO에서도 주요 방법들이 복잡하고 서로 다르기 때문에 비교가 어렵다. 자세 추적은 연구가 많지 않지만 문제 차원이 커지면서 시스템 복잡도는 더 커질 수 있다.

본 연구는 반대로 "간단한 방법이 얼마나 성능이 좋을 수 있는가?"라는 질문을 던진다. 이를 위해 자세 추정 및 추적을 위한 간단한 기준선을 제시하며, 효과적이고 간단하다. 연구 커뮤니티를 위해 코드와 사전 학습 모델도 공개할 예정이다.

자세 추정은 ResNet 백본 위에 몇 개의 디컨볼루션 계층을 추가하는 구조로 되어 있으며, 이는 저해상도 피처맵에서 히트맵을 추정하는 가장 간단한 방법 중 하나다. COCO test-dev에서 mAP 73.7로 최고 성능을 달성했다.

자세 추적은 ICCV’17 PoseTrack 챌린지 우승자의 파이프라인을 따르되, 자체 추정기법과 광류 기반 자세 전파 및 유사도 측정 방법을 도입하였다. 결과적으로 mAP 74.6, MOTA 57.8을 기록하여 기존 최고 성능을 초과하였다.

이 논문은 이론적 근거보다는 간단한 기법과 포괄적인 실험을 통해 유효성을 입증하며, 기존 기법에 대한 우위를 주장하지는 않는다. 이 논문의 기여는 간단하고 강력한 기준선 제공이다.

## **2. 디컨볼루션 헤드 네트워크를 이용한 자세 추정**

[[ResNet|ResNet]] [13]은 이미지 피처 추출을 위한 가장 일반적인 백본 네트워크이며, [24,6]에서도 자세 추정에 사용된다. 본 방법은 ResNet의 마지막 컨볼루션 스테이지(C5)에 디컨볼루션 계층 몇 개를 단순히 추가한다. 전체 네트워크 구조는 그림 1(c)에 나와 있다. 이 구조는 깊고 저해상도인 피처로부터 히트맵을 생성하는 가장 간단한 방식 중 하나이며, Mask R-CNN [12]에서도 채택된 바 있다.
![[스크린샷 2025-08-04 오후 2.30.30.png]]
기본적으로 [[Batch Normalization|배치 정규화]] [15]와 ReLU 활성화 함수 [19]가 포함된 세 개의 디컨볼루션 계층을 사용한다. 각 계층은 커널 크기 4×4, 스트라이드 2, 채널 수 256을 가진다. 마지막에는 1×1 컨볼루션 계층을 추가하여 k개의 키포인트에 대한 히트맵 {H₁, ..., Hₖ}을 생성한다.

[30,22]와 동일하게, 예측 히트맵과 타깃 히트맵 간의 손실로 평균 제곱 오차(MSE)를 사용한다. 각 관절 k에 대한 타깃 히트맵 Ĥₖ은 해당 관절의 GT 위치를 중심으로 한 2D 가우시안 분포로 생성된다.

**논의**  
기준선의 단순성과 합리성을 이해하기 위해, 대표적인 두 구조 Hourglass [22]와 CPN [6]과 비교한다. 그림 1에 나와 있다.

- **[[Hourglass]]** 는 MPII 벤치마크에서 지배적인 접근으로, 상위 방법들 [8,7,33]의 기반이다. 반복적인 bottom-up / top-down 처리와 skip layer 피처 결합을 특징으로 한다.
    
- **[[Cascaded Pyramid Network|CPN (Cascaded Pyramid Network)]]** 은 COCO 2017 keypoint challenge [9]에서 최고 성능을 달성한 방법이며, skip layer 결합과 온라인 하드 키포인트 마이닝(OHKM)을 포함한다.
    

그림 1에서 세 구조를 비교해보면, 고해상도 피처맵을 생성하는 방식에서 차이를 보인다. Hourglass와 CPN은 업샘플링을 통해 해상도를 높이고, 다른 블록에 컨볼루션 연산을 분산한다. 반면 본 방법은 디컨볼루션 계층에 업샘플링과 컨볼루션을 모두 통합하여 skip 연결 없이 훨씬 단순한 방식으로 처리한다.

공통점으로는 세 방법 모두 세 번의 업샘플링과 세 단계의 비선형성이 사용된다는 점이다. 기준선이 좋은 성능을 내는 것을 보면, 고해상도 피처맵의 확보가 중요한 것으로 보인다. 다만 이 논의는 경험적이며, 어떤 구조가 더 우수한지 결론내리는 것이 본 연구의 목적은 아니다.

## **3. 광류 기반 자세 추적**

비디오에서 다중 인물의 자세를 추적하는 것은 각 프레임에서 자세를 추정한 뒤, 프레임 간 고유한 식별 번호(id)를 부여하여 추적하는 방식이다. 사람 인스턴스 P는 id가 포함된 형태로 $P = (J, \text{id})$로 표현되며, 여기서 $J = \{j_i\}_{1:N_J}$는 $N_J$개의 관절 좌표 집합이다. 프레임 $I_k$를 처리할 때, 이전 프레임 $I_{k-1}$에서의 인스턴스 집합 $P_{k-1}$과 현재 프레임에서 식별해야 할 인스턴스 집합 $P_k$가 주어진다. 만약 $P_k^j$가 $P_{k-1}^i$와 연결된다면, id는 전달되고 그렇지 않으면 새로운 id가 할당된다.

ICCV'17 PoseTrack Challenge [2]의 우승자 [11]는 이 문제를 다음과 같이 해결했다: Mask R-CNN [12]을 사용하여 각 프레임에서 사람의 자세를 추정한 후, 탐욕적인 이분 매칭 알고리즘으로 온라인 추적을 수행한다.

**기존 탐욕 매칭 알고리즘:**  
프레임 $I_{k-1}$의 인스턴스 $P_{k-1}^i$와 프레임 $I_k$의 인스턴스 $P_k^j$ 사이의 유사도가 가장 높을 경우 id를 부여하고, 이 두 인스턴스를 제거한 뒤 같은 과정을 반복한다. 만약 연결되지 않은 인스턴스가 있다면 새로운 id가 할당된다.

**본 논문의 차별점은 두 가지다:**

1. 탐지기로부터 얻은 박스와 광류로 이전 프레임에서 전파된 박스 두 종류를 사용한다.
    
2. 유사도 측정에서 새로운 **광류 기반 자세 유사도**를 제안한다.
    

이를 통해 개선된 광류 기반 자세 추적 알고리즘을 제시하며, 전체 파이프라인은 그림 2에 나와 있다.
![[스크린샷 2025-08-04 오후 2.34.08.png]]

---

### **3.1 광류 기반 관절 전파 (Joint Propagation using Optical Flow)**  
단일 이미지에 최적화된 탐지기(Faster-RCNN, R-FCN 등)를 비디오에 적용할 경우, 모션 블러 및 가림(occlusion)으로 인해 탐지 누락이 발생할 수 있다. 그림 2(c)에서 왼쪽의 검은색 인물은 빠른 움직임으로 탐지기가 놓쳤지만, 광류 기반 전파를 통해 이를 복원할 수 있다.

프레임 $I_{k-1}$의 인스턴스 $J_{k-1}^i$와 optical flow 필드 $F_{k-1 \rightarrow k}$가 주어지면, $J_k^i$는 다음과 같이 전파된다:

- 각 관절 (x, y)에 대해, 광류 벡터 (δx, δy)를 더해 새로운 위치 (x+δx, y+δy)를 얻는다.
    
- 전파된 관절 집합 $\hat{J}_k^i$의 바운딩 박스를 계산하고, 15% 확장하여 새로운 후보 박스로 활용한다.
    

이 방법은 탐지기가 실패하는 프레임에서 이전 프레임으로부터 사람을 복원할 수 있는 효과적인 방법이다.

---

### **3.2 광류 기반 자세 유사도 (Flow-based Pose Similarity)**  
기존의 바운딩 박스 [[IoU]] (SBbox) 기반 유사도는 고속 이동이나 군중 상황에서 오류가 발생할 수 있다. 자세 기반 유사도(SPose)는 [[Object Keypoint Similarity|OKS(Object Keypoint Similarity)]]를 사용하지만, 프레임 간 포즈 차이로 인해 여전히 문제가 있다.

이에 따라 다음과 같은 **광류 기반 유사도(SFlow)** 를 정의한다:

$$S_{\text{Flow}}(J_k^i, J_l^j) = \text{OKS}(\hat{J}_l^i, J_l^j)$$

여기서 $\hat{J}_l^i$는 optical flow $F_{k \rightarrow l}$를 통해 $J_k^i$를 l번째 프레임으로 전파한 결과이다.

또한 단일 프레임만 고려할 경우 재등장한 객체를 놓칠 수 있으므로, **다중 프레임 기반 전파(SMulti-Flow)** 를 통해 이를 보완한다.

---

### **3.3 전체 알고리즘**  
알고리즘 1에 기술된 전체 흐름은 다음과 같다:

1. 탐지기와 광류 전파로부터 얻은 박스들을 [[Greedy Non-maximum Suppression|NMS]]로 통합한다.
    
2. 각 박스를 기준으로 포즈 추정을 수행한다.
    
3. 이전 프레임들의 트래킹 결과를 큐(Q)에 저장한다.
    
4. 새 프레임의 관절 집합과 Q의 인스턴스들 사이 유사도 행렬 $M_{\text{sim}}$을 계산한다.
    
5. 탐욕 매칭을 통해 id를 부여한다.
    

이러한 방식으로 단순하지만 강력한 자세 추적이 가능해진다.

## **4. 실험**

---

### **4.1 COCO 데이터셋에서의 자세 추정**

COCO Keypoint Challenge [20]는 복잡한 조건에서 다중 인물의 키포인트를 정확히 위치 추정하는 과제이다. COCO train/val/test 세트는 20만 장 이상의 이미지와 25만 개 이상의 인물 인스턴스를 포함하며, 그 중 15만 개는 학습과 검증에 공개된다. 본 논문에서는 COCO train2017 (57K 이미지, 150K 인스턴스)만을 사용하였으며, 검증은 val2017, 최종 비교는 test-dev2017에서 수행된다.

COCO 평가지표는 OKS(Object Keypoint Similarity)를 정의하고, 10개의 OKS 임계값에 대해 평균 정밀도(AP)를 계산한다.

---

**학습 세부사항**

- 사람 박스는 4:3 비율로 리사이징.
    
- 입력 해상도는 기본 256×192 (CPN [6]과 동일).
    
- 데이터 증강: 스케일(±30%), 회전(±40도), 플립.
    
- 백본: ImageNet으로 사전학습된 ResNet-50, 101, 152.
    
- 옵티마이저: Adam [18].
    
- 학습률: 1e-3 (초기), 90 epoch에서 1e-4, 120 epoch에서 1e-5.
    
- 총 epoch 수: 140.
    
- 배치 사이즈: 128, GPU 4개 사용.
    

---

**테스트 세부사항**

- Top-down 방식 사용 (탐지 후 포즈 추정).
    
- 사람 탐지: Faster-RCNN, COCO val2017 기준 AP 56.4.
    
- 히트맵은 원본 + 좌우 반전 후 평균.
    
- 응답이 가장 높은 위치와 두 번째 높은 위치 사이 방향으로 1/4 보정 오프셋 적용.
    

---

**Ablation Study 요약 (표 2)**

1. **히트맵 해상도 (a vs. b):**
    
    - 디컨볼루션 3층 (64×48) > 2층 (32×24) → AP +2.5
        
2. **커널 사이즈 (a, c, d):**
    
    - 4 > 3 > 2 순 → 작을수록 성능 약간 감소 (최대 0.3)
        
3. **백본 깊이 (a, e, f):**
    
    - ResNet-50 < 101 < 152 → 깊을수록 성능 향상 (최대 +1.6)
        
4. **입력 해상도 (a, g, h):**
    
    - 128×96 → AP 60.6 (감소)
        
    - 384×288 → AP 72.2 (증가)
        
    - 연산량 증가/감소와 성능 간 trade-off 존재
        

---

**기존 방법과의 비교 (표 3)**

- **Hourglass [22]:** AP 66.9 (256×192)
    
- **CPN [6]:** AP 68.6 (256×192), AP 70.6 (384×288)
    
- **Ours:** AP 70.4 (256×192), AP 72.2 (384×288)
    

→ 동일 백본(ResNet-50) 기준, OHKM 미사용 시 CPN 대비 본 방법이 +1.8~1.6 AP 우위  
→ OHKM 사용 시에도 본 방법이 +0.6 AP 우위  
→ 구조는 더 단순하며 성능은 우수함

---

**COCO test-dev 비교 (표 4)**

- **CPN+ (ensemble):** AP 73.0
    
- **Ours (ResNet-152, 단일 모델):** AP 73.7
    

→ 백본이 약한 상태에서도, 기존보다 높은 성능 확보  
→ CPN은 ResNet-Inception, 본 연구는 ResNet-152 사용. 성능 차이 있음에도 불구하고 경쟁력 있음.

### **4.2 PoseTrack 데이터셋에서의 자세 추정 및 추적**

PoseTrack [2]는 비디오 상에서 다중 인물의 자세 추정 및 추적을 위한 대규모 벤치마크이다. 이 데이터셋은 총 514개의 비디오와 66,374개의 프레임으로 구성되며, 학습/검증/테스트 세트는 각각 300/50/208개의 비디오를 포함한다. 학습 세트는 중심의 30프레임, 검증 및 테스트 세트는 중심의 30프레임과 추가로 매 4프레임마다 주석이 포함된다. 각 인물 인스턴스는 15개의 관절 좌표, 고유 ID, 머리 위치 박스가 포함된다.

세 가지 과제가 정의되어 있다:

1. 단일 프레임 자세 추정 (mAP 측정)
    
2. 프레임 간 시간 정보 활용한 자세 추정
    
3. 다중 객체 추적 (MOTA 등 MOT 지표 사용)
    

본 논문에서는 시간 정보를 활용하는 Task 2, 3의 결과를 보고하며, Task 1에서도 최고 성능이나 생략하였다.

---

**학습 세부사항**

- COCO에서 사전학습된 모델을 미세 조정.
    
- GT 박스는 키포인트의 바운딩 박스를 15% 확장해 생성.
    
- 증강 방식 및 하이퍼파라미터는 COCO와 동일.
    
- 학습률: 초기 1e-4 → 10 epoch 후 1e-5 → 15 epoch 후 1e-6 (총 20 epoch)
    

---

**테스트 세부사항**

- 추적 성능은 탐지기의 성능에 민감하다. 따라서 두 종류 탐지기를 실험:
    
    - **R-FCN** (빠르지만 정확도 낮음)
        
    - **FPN-DCN** (느리지만 정확도 높음)
        
- 둘 다 ResNet-101을 백본으로 하며 공개 구현체 사용. 추가 파인튜닝 없음.
    
- [11]과 동일하게 낮은 confidence 점수의 박스 및 관절은 삭제하여 FP를 줄인다.
    
    - 박스/관절 제거 임계값은 검증 세트 기반으로 설정 (0.5 / 0.4)
        
- 광류: [[FlowNet2s|FlowNet2S]] [14] 사용.
    

---

**Ablation Study (표 5)**

- ResNet-50 / 152 + R-FCN / FPN-DCN 조합별로 mAP 및 MOTA 측정
    
- 핵심 결과 요약:
    
    - **Joint Propagation 효과**:
        
        - (a1→a3) 4.3% mAP, 3.8% MOTA 향상
            
        - (b1→b3) 3.1% mAP, 2.3% MOTA 향상
            
        - (c1→c3) 3.8% mAP, 2.8% MOTA 향상
            
    - 단순히 박스 수를 늘리는 것과는 달리, 실제로 탐지기에서 놓친 인물을 보완
        
- **광류 기반 유사도 효과**:
    
    - (b3→b5→b6)
        
        - SFlow 사용시 MOTA +0.3~0.8
            
        - SMulti-Flow 사용시 다중 프레임 전파 효과로 +0.5 추가 향상
            
    - 특히 빠르게 움직이거나 가려졌다 재등장하는 인물의 추적에서 효과적
        

---

**State-of-the-Art와 비교 (표 6, 7)**

- Pose Estimation (mAP, Task 2):
    
    - 기존 최고 성능인 [11] 대비 ResNet-152 기준 +16.1% mAP 향상
        
    - ResNet-50도 다른 방법보다 높은 성능 달성
        
- Pose Tracking (MOTA, Task 3):
    
    - Girdhar et al. [11] 대비 +10.2%(val), +5.8%(test) 향상
        
    - Xiu et al. [32] 대비 +7.1%(val), +6.6%(test) 향상
        

---

**PoseTrack 리더보드 비교 (표 8)**

- 모든 기존 방법과 비교해 가장 높은 mAP(74.6), MOTA(57.8)
    
- FAIRness를 위해 단일 모델 기준, COCO 데이터만 사용
    
- 시스템 구성과 구현 차이가 있으므로 구조적 비교보다는 시스템 레벨에서의 비교로 간주함
    

---

**5. 결론**

본 논문에서는 사람 자세 추정 및 추적을 위한 **단순하지만 강력한 기준선(baseline)** 을 제안하였다.

- 이 기준선은 복잡한 구조 없이도 **최신 벤치마크에서 최고 성능(state-of-the-art)** 을 달성하였으며,
    
- 다양한 **ablation 실험**을 통해 그 효과와 설계 선택을 정량적으로 분석하였다.
    

특히 본 기준선은 다음과 같은 기여점을 가진다:

- **단순성**: 기존 방법들과 달리 skip connection 없이, deconvolution 계층만으로 고해상도 히트맵을 효과적으로 생성
    
- **확장성**: 자세 추적에도 Optical Flow를 활용한 joint propagation과 flow-based pose similarity로 간단하게 확장 가능
    
- **재현 용이성**: 구조와 구현이 단순하여 다른 연구자들이 아이디어를 개발하고 검증하기 용이
    

따라서 본 연구는, 복잡한 설계 없이도 강력한 성능을 발휘할 수 있다는 것을 보여주며, **추후 연구 개발 및 평가의 출발점으로서의 기준선 역할**을 할 수 있을 것이다.
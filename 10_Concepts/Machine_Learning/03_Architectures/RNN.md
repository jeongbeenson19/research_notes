# 순환 신경망 (Recurrent Neural Network, RNN)
#NLP #SequentialModel

---

## 1. 개요

**순환 신경망(Recurrent Neural Network, RNN)** 은 **순서가 있는 데이터(Sequential Data)**, 예를 들어 시계열, 자연어, 음성 등을 처리하기 위해 특별히 설계된 인공 신경망의 한 종류입니다.

기존의 피드포워드 신경망(Feed-forward Neural Network)과 달리, RNN은 내부에 **순환하는 구조(recurrent loop)** 를 가지고 있어 이전 단계의 정보를 기억하고, 이를 현재 단계의 입력과 함께 사용하여 결과를 출력합니다. 이러한 특징 덕분에 RNN은 "메모리"를 가진 네트워크라고도 불립니다.

---

## 2. 핵심 구조와 원리

RNN의 핵심 아이디어는 은닉층(hidden layer)의 출력이 다시 다음 타임스텝(time step)의 은닉층 입력으로 들어간다는 점입니다.

- **$x_t$**: 타임스텝 $t$에서의 입력 벡터
- **$h_t$**: 타임스텝 $t$에서의 은닉 상태(hidden state)
- **$h_{t-1}$**: 이전 타임스텝 $t-1$에서의 은닉 상태
- **$W_{xh}$**: 입력에서 은닉층으로의 가중치 행렬
- **$W_{hh}$**: 이전 은닉 상태에서 현재 은닉 상태로의 가중치 행렬 (순환 가중치)
- **$W_{hy}$**: 은닉층에서 출력층으로의 가중치 행렬

수식으로 표현하면 다음과 같습니다.
- **은닉 상태:** $h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$
- **출력:** $y_t = f(W_{hy}h_t + b_y)$ (여기서 $f$는 활성화 함수)

$h_t$는 타임스텝 $t$까지의 입력 시퀀스 정보를 요약한 **문맥 벡터(context vector)**라고 볼 수 있습니다. 이 은닉 상태가 계속해서 다음 타임스텝으로 전달되면서 정보가 누적됩니다.

---

## 3. 장점 및 단점

### 장점
- **순차적 데이터 처리:** 시퀀스의 순서 정보를 모델링할 수 있어 자연어 처리, 음성 인식 등에서 뛰어난 성능을 보입니다.
- **가변 길이 입력:** 입력 시퀀스의 길이에 유연하게 대처할 수 있습니다.
- **파라미터 공유:** 모든 타임스텝에서 동일한 가중치 행렬($W_{xh}, W_{hh}$)을 사용하므로, 시퀀스 길이에 관계없이 모델의 파라미터 수가 일정하게 유지됩니다.

### 단점
- **장기 의존성 문제 (Long-Term Dependency Problem):** 시퀀스가 길어질수록, 앞쪽의 정보가 뒤쪽까지 전달되기 어렵습니다. 역전파 과정에서 기울기가 점차 사라지거나(Vanishing Gradient) 폭발하는(Exploding Gradient) 문제가 발생하기 때문입니다.
- **기울기 소실/폭발:** 특히 Vanishing Gradient 문제로 인해, RNN은 긴 시퀀스의 의존성을 학습하는 데 치명적인 약점을 가집니다.
- **병렬 처리의 어려움:** 각 타임스텝의 계산이 이전 타임스텝의 계산 결과($h_{t-1}$)에 의존하므로, 전체 시퀀스를 병렬적으로 처리하기 어렵습니다.

---

## 4. RNN의 종류

- **One-to-One:** 일반적인 피드포워드 신경망 (시퀀스 처리 X)
- **One-to-Many:** 이미지 캡셔닝 (하나의 이미지 입력, 문장 출력)
- **Many-to-One:** 감성 분석 (문장 입력, 긍정/부정 출력)
- **Many-to-Many (Seq2Seq):** 기계 번역 (한 언어 문장 입력, 다른 언어 문장 출력)
- **Many-to-Many:** 비디오 프레임별 분류

---

## 5. 한계 극복

RNN의 고질적인 장기 의존성 문제를 해결하기 위해 다음과 같은 개선된 모델들이 등장했습니다.

- **[[LSTM]]**
- **[[GRU]]**

이 모델들은 내부에 '게이트(gate)'라는 정교한 메커니즘을 도입하여 정보의 흐름을 제어함으로써 장기 의존성 문제를 크게 완화했습니다. 이후 [[Transformer]]의 등장으로 RNN의 패러다임이 바뀌기 전까지 NLP 분야의 핵심적인 역할을 수행했습니다.

---
tags:
  - ML
  - NLP
  - RNN
  - architecture
aliases:
  - LSTM
  - Long Short-Term Memory
  - 장단기 메모리
---

# Long Short-Term Memory (LSTM)

**LSTM(Long Short-Term Memory)**, 즉 장단기 메모리는 전통적인 [[RNN]]의 **장기 의존성 문제(Long-Term Dependency Problem)** 를 해결하기 위해 1997년 Sepp Hochreiter와 Jürgen Schmidhuber에 의해 제안된 순환 신경망의 한 종류입니다.

기본 RNN은 시퀀스가 길어질수록 앞쪽의 정보가 뒤로 효과적으로 전달되기 어려운, 즉 기울기가 소실(Vanishing Gradient)되거나 폭주(Exploding Gradient)하는 문제가 있었습니다. LSTM은 **셀 상태(Cell State)** 와 여러 개의 **게이트(Gate)** 를 도입하여, 중요한 정보는 오래 기억하고 불필요한 정보는 잊는 정교한 메커니즘을 구현했습니다.

## 🧱 핵심 구조: 셀 상태와 3개의 게이트

LSTM의 핵심은 컨베이어 벨트처럼 네트워크 전체를 관통하며 흐르는 **셀 상태($C_t$)** 입니다. 셀 상태는 장기적인 기억을 저장하는 역할을 합니다. LSTM은 이 셀 상태에 정보를 추가하거나 제거할 수 있는 능력을 가지고 있으며, 이러한 조절은 '게이트'라고 불리는 세 개의 구조에 의해 제어됩니다.

### 1. 망각 게이트 (Forget Gate, $f_t$)

-   **역할**: 과거의 정보(이전 셀 상태 $C_{t-1}$)를 얼마나 '잊을지' 결정합니다.
-   **계산**: `f_t = σ(W_f · [h_{t-1}, x_t] + b_f)`
-   **출력**: 0과 1 사이의 값을 가집니다. (1이면 정보를 온전히 보존, 0이면 완전히 잊음)

### 2. 입력 게이트 (Input Gate, $i_t$)

-   **역할**: 현재 입력($x_t$)과 이전 은닉 상태($h_{t-1}$)로부터 어떤 새로운 정보를 셀 상태에 저장할지를 결정합니다.
-   **계산**: 
    1.  어떤 값을 업데이트할지 결정: `i_t = σ(W_i · [h_{t-1}, x_t] + b_i)`
    2.  셀 상태에 추가할 후보 벡터 생성: `C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)`
-   **셀 상태 업데이트**: `C_t = f_t * C_{t-1} + i_t * C̃_t` (과거 정보는 잊고, 새로운 정보를 추가)

### 3. 출력 게이트 (Output Gate, $o_t$)

-   **역할**: 업데이트된 셀 상태($C_t$)를 바탕으로, 현재 타임스텝의 은닉 상태($h_t$, 즉 출력)를 무엇으로 할지를 결정합니다.
-   **계산**: 
    1.  셀 상태 중 어느 부분을 출력할지 결정: `o_t = σ(W_o · [h_{t-1}, x_t] + b_o)`
    2.  최종 은닉 상태($h_t$) 결정: `h_t = o_t * tanh(C_t)`

## ✨ RNN과의 차이점

| 구분          | 기본 RNN                               | LSTM                                         |
| :------------ | :------------------------------------- | :------------------------------------------- |
| **상태 벡터**   | 단일 은닉 상태 ($h_t$)                  | 셀 상태 ($C_t$)와 은닉 상태 ($h_t$) 두 개      |
| **정보 흐름**   | 단순한 반복 구조                       | 망각, 입력, 출력 게이트를 통한 정교한 제어   |
| **장기 의존성** | 약함 (기울기 소실/폭주 문제)           | 강함 (셀 상태를 통해 장기 기억 보존)         |

## 🔗 관련 개념

-   [[RNN]]
-   [[GRU]]: LSTM의 구조를 단순화한 모델입니다.
-   [[Vanishing Gradient Problem]]
-   [[Transformer]]: 어텐션 메커니즘을 기반으로 RNN/LSTM의 순차적 처리 한계를 극복한 모델로, 현대 NLP의 표준이 되었습니다.
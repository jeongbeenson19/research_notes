
--- 
tags:
  - ML
  - NLP
  - RNN
  - architecture
alias:
  - GRU
  - Gated Recurrent Unit
---

# Gated Recurrent Unit (GRU)

**GRU(Gated Recurrent Unit)**, 즉 게이트 순환 유닛은 2014년 조경현 교수팀이 제안한 모델로, [[LSTM]]과 마찬가지로 표준 [[RNN]]의 장기 의존성 문제(Long-term Dependency Problem)를 해결하기 위한 게이트 메커니즘을 갖춘 순환 신경망입니다.

GRU는 LSTM의 복잡한 구조를 간소화하면서도 그와 유사한 성능을 내는 것을 목표로 합니다. LSTM의 특징인 '셀 상태(Cell State)'와 '은닉 상태(Hidden State)'를 하나의 '은닉 상태'로 통합하고, 3개의 게이트를 2개로 줄인 것이 가장 큰 특징입니다.

## 🧱 핵심 구조: 2개의 게이트

GRU는 두 가지 주요 게이트를 통해 정보의 흐름을 제어합니다.

### 1. 리셋 게이트 (Reset Gate, $r_t$)

-   **역할**: 과거의 정보(이전 은닉 상태 $h_{t-1}$)를 얼마나 '잊을지' 또는 '무시할지' 결정합니다. 이 게이트의 값이 0에 가까울수록 과거 정보를 더 많이 잊고 현재 입력에 집중하게 됩니다.
-   **계산**: $r_t = σ(W_r · [h_{t-1}, x_t])$

### 2. 업데이트 게이트 (Update Gate, $z_t$)

-   **역할**: 과거 정보와 현재 정보의 조합 비율을 결정합니다. 즉, 이전 은닉 상태($h_{t-1}$)의 정보를 얼마나 유지하고, 현재 타임스텝에서 생성된 새로운 정보($	ilde{h}_t$)를 얼마나 반영할지를 결정합니다.
-   **계산**: $z_t = σ(W_z · [h_{t-1}, x_t])$

### 최종 은닉 상태 계산

1.  리셋 게이트를 사용하여 후보 은닉 상태($ilde{h}_t$)를 계산합니다. (과거 정보를 얼마나 반영할지 결정)
    $h̃_t = tanh(W_h · [r_t * h_{t-1}, x_t])$
2.  업데이트 게이트를 사용하여 최종 은닉 상태($h_t$)를 계산합니다.
    $h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t$

    -   $z_t$가 1에 가까우면 새로운 정보($ilde{h}_t$)를 많이 반영하고, 0에 가까우면 과거 정보($h_{t-1}$)를 많이 유지합니다. 이는 LSTM의 망각 게이트와 입력 게이트의 역할을 동시에 수행하는 것과 유사합니다.

## ✨ LSTM과의 비교

| 구분        | GRU                                       | LSTM                                         |
| :--- | :--- | :--- |
| **상태 벡터**   | 단일 은닉 상태 ($h_t$)                     | 셀 상태 ($C_t$) + 은닉 상태 ($h_t$)            |
| **게이트 수**   | 2개 (리셋, 업데이트)                      | 3개 (망각, 입력, 출력)                       |
| **파라미터 수** | 더 적음                                   | 더 많음                                      |
| **계산량**      | 더 적고, 학습 속도가 빠를 수 있음         | 더 많고, 학습이 상대적으로 느릴 수 있음       |
| **성능**        | 일반적으로 LSTM과 유사한 성능을 보임      | 데이터가 매우 크고 복잡할 때 약간 더 나은 성능을 보인다는 보고도 있음 |

## 🔗 관련 개념

-   [[RNN]]
-   [[LSTM]]
-   [[Vanishing Gradient Problem]]
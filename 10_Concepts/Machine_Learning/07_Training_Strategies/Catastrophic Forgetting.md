# Catastrophic Forgetting

## 한줄 요약
새로운 작업을 학습할 때 이전 작업 성능이 급격히 저하되는 현상.

## 핵심 개념
- 파라미터가 새 작업에 맞춰 덮어써지며 발생
- 멀티태스크/연속학습에서 큰 문제
- 규제, 리플레이, 지식 증류 등의 완화 기법 사용

## 주요 완화 기법
- 규제 기반: EWC, LwF 등
- 리플레이 기반: 경험 재생, 생성 리플레이
- 구조 기반: 네트워크 분리/동적 확장

## 예시
- 이미지 분류 모델이 새 클래스 추가 후 기존 클래스 정확도 급감

## 실무 팁/주의점
- 데이터 저장 가능 여부에 따라 방법 선택
- 평가 시 과거 태스크 성능을 반드시 포함

## 관련 노트
- [[Deep Learning]]
- [[Regularization]]
- [[Knowledge Distillation]]

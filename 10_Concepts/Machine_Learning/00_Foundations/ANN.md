---
tags:
  - ML
  - architecture
  - neural_network
alias:
  - ANN
  - 인공 신경망
---

# Artificial Neural Network (인공 신경망)

**Artificial Neural Network(ANN)**, 즉 인공 신경망은 인간의 뇌를 구성하는 생물학적 뉴런의 동작 원리에서 영감을 얻어 만들어진 머신러닝 모델입니다. ANN은 **뉴런(Neuron)** 또는 **유닛(Unit)**으로 이루어진 **층(Layer)**들로 구성되며, 데이터 속의 복잡한 패턴을 학습하는 데 사용됩니다.

## 📖 기본 구조

-   **입력층 (Input Layer)**: 외부 데이터를 받아들이는 층입니다.
-   **은닉층 (Hidden Layers)**: 입력을 처리하고 특징을 추출하는 층으로, 여러 개 존재할 수 있습니다. 은닉층이 깊어지면 [[Deep Learning]] 모델이라고 부릅니다.
-   **출력층 (Output Layer)**: 처리된 데이터로부터 최종 결과를 출력하는 층입니다.

각 뉴런은 입력 신호에 **가중치(weight)**를 곱하고, **편향(bias)**을 더한 뒤, **활성화 함수(activation function)**를 거쳐 다음 층으로 신호를 전달합니다.

## ⚙️ 작동 원리 및 학습

### 순전파 (Forward Propagation)

1.  입력층에서 데이터($x$)를 받습니다.
2.  각 연결에 정의된 가중치($w$)를 곱하고 편향($b$)을 더합니다: `z = w * x + b`
3.  계산된 값 `z`를 활성화 함수에 통과시켜 출력 `a = f(z)`를 계산합니다.
4.  이 출력이 다음 층의 입력으로 전달되며, 출력층에 도달할 때까지 반복됩니다.

### 역전파 (Backpropagation)

모델의 예측과 실제 정답 사이의 **오차(Loss)**를 계산한 뒤, 이 오차를 최소화하는 방향으로 모델의 가중치와 편향을 업데이트하는 과정입니다.
-   **손실 함수 (Loss Function)**: 예측과 정답의 차이를 측정합니다. (예: MSE, Cross-Entropy)
-   **경사 하강법 (Gradient Descent)**: 손실 함수의 기울기(gradient)를 계산하여, 손실이 감소하는 방향으로 가중치를 조정합니다.

## ✨ 주요 특징 및 비교

|특징|설명|
|---|---|
|비선형 모델|비선형 활성화 함수 사용으로 복잡한 문제 해결 가능|
|병렬 처리 가능|GPU를 활용한 대규모 병렬 연산|
|다층 구조|층을 깊게 쌓으면 더 복잡한 표현 가능 (→ 딥러닝)|

### 자주 쓰이는 활성화 함수

| 함수      | 수식                     | 특징                   |
| ------- | ---------------------- | -------------------- |
| Sigmoid | $rac{1}{1 + e^{-x}}$ | 출력이 0~1, 이진 분류에 적합   |
| ReLU    | $\max(0, x)$           | 연산 간단, 빠르고 성능 우수     |
| Tanh    | $\tanh(x)$             | 출력이 -1~1, 평균이 0에 가까움 |

### 다른 모델과의 비교

|항목|ANN|전통적 ML (예: SVM, 결정 트리)|
|---|---|---|
|데이터 필요량|많음|적어도 괜찮음|
|해석력|낮음 (블랙박스)|높음|
|성능|복잡한 문제에 강함|단순한 문제에 강함|

## 🔗 관련 개념 및 규칙

-   **[[ART(Adaptive resonance theory)]]**
    -   입력 패턴을 기존 범주와 비교해 공명 여부를 판단하며, 새 범주를 생성하거나 기존 범주를 업데이트하는 방식으로 **안정성과 유연성을 동시에 추구하는 신경망 학습 이론**입니다.

-   **[[Hebbian Rule]]**
    -   "함께 활성화되는 뉴런끼리는 연결이 강화된다"는 원리에 따라, 입력과 출력 뉴런의 동시 활성화를 기반으로 시냅스 가중치를 조정하는 **비지도 신경망 학습 규칙**입니다. 학습이란 시냅스 연결의 세기(strength)를 조정하는 것으로 정의했으며, 기본적인 학습 방법은 2개의 뉴런이 동시에 활성화하려면, 뉴런이 연결된 가중치(weight)를 높이면 됩니다.

-   **[[Delta Rule]]**
    -   **Sigmoid 함수의 부드러운 미분 가능성**과 **delta rule의 오차 기반 가중치 조정 방식**은 backpropagation에서 결합되어, 신경망이 입력과 정답 간의 오차를 각 계층에 걸쳐 효과적으로 학습하도록 돕습니다.

-   **[[Regularization]]**
    -   머신러닝 모델이 **과적합(overfitting)** 되는 것을 방지하기 위해 사용하는 **규제(regularization)** 기법입니다. 이 두 가지는 **모델의 복잡도를 제한하여 일반화 성능을 향상**시키는 데에 목적이 있습니다.

-   **[[Dropout; A Simple Way to Prevent Neural Networks from Overfitting|Dropout]]**
    -   **Dropout**은 **과적합(overfitting)을 방지하기 위한 정규화 기법**입니다. 2014년 Hinton 등이 제안한 방법으로, 훈련 과정에서 **신경망의 일부 뉴런을 랜덤하게 비활성화(drop)** 하여 모델이 특정 뉴런에 과도하게 의존하지 않도록 합니다.
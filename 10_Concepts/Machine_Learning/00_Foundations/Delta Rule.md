---
tags:
  - ML
  - learning_rule
  - optimization
aliases:
  - λΈνƒ€ κ·μΉ™
  - Widrow-Hoff Rule
---

# Delta Rule (λΈνƒ€ κ·μΉ™)

**λΈνƒ€ κ·μΉ™(Delta Rule)**μ€ μ§€λ„ ν•™μµ(Supervised Learning) ν™κ²½μ—μ„ μ‚¬μ©λλ” κ°€μ¤‘μΉ μ—…λ°μ΄νΈ κ·μΉ™μΌλ΅, μ‹¤μ  μ¶λ ¥κ³Ό λ©ν‘ μ¶λ ¥ κ°„μ **μ¤μ°¨(error)λ¥Ό μµμ†ν™”**ν•κΈ° μ„ν•΄ κ²½μ‚¬ ν•κ°•λ²•(Gradient Descent)μ„ μ‚¬μ©ν•©λ‹λ‹¤.

μ£Όλ΅ λ‹¨μΌ κ³„μΈµ μ‹ κ²½λ§, νΉν Adaline(Adaptive Linear Neuron) λ¨λΈμ ν•™μµμ— μ‚¬μ©λμ—μΌλ©°, μ΄ν›„ λ‹¤μΈµ νΌμ…‰νΈλ΅ μ„ ν•™μµμ‹ν‚¤λ” [[Backpropagation|μ—­μ „ν μ•κ³ λ¦¬μ¦]]μ ν•µμ‹¬μ μΈ κΈ°λ°μ΄ λμ—μµλ‹λ‹¤.

> **μ”μ•½**: λΈνƒ€ κ·μΉ™μ€ μ¶λ ¥κ°’κ³Ό λ©ν‘κ°’μ μ°¨μ΄(delta)λ¥Ό μ΄μ©ν•μ—¬, μ¤μ°¨λ¥Ό μ¤„μ΄λ” λ°©ν–¥μΌλ΅ μ‹ κ²½λ§μ κ°€μ¤‘μΉλ¥Ό μ μ§„μ μΌλ΅ μμ •ν•λ” ν•™μµ λ°©λ²•μ…λ‹λ‹¤.

## β™οΈ μ‘λ™ μ›λ¦¬

λΈνƒ€ κ·μΉ™μ€ κ° κ°€μ¤‘μΉ($w_i$)μ— λ€ν• μ—…λ°μ΄νΈ($\Delta w_i$)λ¥Ό λ‹¤μκ³Ό κ°™μ΄ μ •μν•©λ‹λ‹¤.

`Ξ”w_i = Ξ± * (t - y) * x_i`

-   `Ξ±` (alpha): ν•™μµλ¥  (Learning Rate), κ°€μ¤‘μΉλ¥Ό ν• λ²μ— μ–Όλ§λ‚ ν¬κ² λ³€κ²½ν• μ§€ κ²°μ •ν•©λ‹λ‹¤.
-   `t` (target): λ©ν‘ μ¶λ ¥κ°’μ…λ‹λ‹¤.
-   `y` (output): λ¨λΈμ μ‹¤μ  μ¶λ ¥κ°’μ…λ‹λ‹¤.
-   `(t - y)`: μ¤μ°¨(error), μ¦‰ 'λΈνƒ€'μ…λ‹λ‹¤.
-   `x_i`: ν•΄λ‹Ή κ°€μ¤‘μΉ($w_i$)μ— μ—°κ²°λ μ…λ ¥κ°’μ…λ‹λ‹¤.

μ΄ κ·μΉ™μ€ μ¤μ°¨κ°€ ν΄μλ΅, κ·Έλ¦¬κ³  ν•΄λ‹Ή μ¤μ°¨μ— κΈ°μ—¬ν• μ…λ ¥κ°’μ΄ ν΄μλ΅ κ°€μ¤‘μΉλ¥Ό λ” λ§μ΄ λ³€κ²½ν•λ„λ΅ λ§λ“­λ‹λ‹¤.

## β¨ Perceptronκ³Όμ μ°¨μ΄μ 

| κµ¬λ¶„         | Perceptronμ ν•™μµ κ·μΉ™                                 | Delta Rule (Adaline)                                     |
| ------------ | ------------------------------------------------------ | -------------------------------------------------------- |
| **ν™μ„±ν™” ν•¨μ**  | κ³„λ‹¨ ν•¨μ (Step Function)                              | μ„ ν• ν•¨μ (Linear Function, Identity)                    |
| **μ¤μ°¨ κ³„μ‚°**    | ν΄λμ¤ λ μ΄λΈ”μ΄ λ§μ•λ”μ§€ ν‹€λ Έλ”μ§€ (0 λλ” 1)           | μ‹¤μ  μ¶λ ¥κ³Ό λ©ν‘ μ¶λ ¥ κ°„μ μ—°μ†μ μΈ κ°’ μ°¨μ΄ `(t - y)`    |
| **κ°€μ¤‘μΉ μ—…λ°μ΄νΈ** | μλ» λ¶„λ¥ν–μ„ λ•λ§ μ—…λ°μ΄νΈλ©λ‹λ‹¤.                     | λ¨λ“  ν•™μµ λ°μ΄ν„°μ— λ€ν•΄ μ¤μ°¨μ— λΉ„λ΅€ν•μ—¬ ν•­μƒ μ—…λ°μ΄νΈλ©λ‹λ‹¤. |
| **κΈ°λ° μ›λ¦¬**    | μ¤λ¶„λ¥λ¥Ό μ¤„μ΄λ” λ°©ν–¥                                   | μ΄ μ¤μ°¨(μ: Mean Squared Error)λ¥Ό μµμ†ν™”ν•λ” κ²½μ‚¬ ν•κ°•λ²• |

## π”— Backpropagationκ³Όμ κ΄€κ³„

λ‹¤μΈµ νΌμ…‰νΈλ΅ (MLP)μ„ μ„ν• **[[Backpropagation|μ—­μ „ν]]** μ•κ³ λ¦¬μ¦μ€ λΈνƒ€ κ·μΉ™μ„ μΌλ°ν™”ν• κ²ƒμ…λ‹λ‹¤.

-   **μ¶λ ¥μΈµ**: μ¶λ ¥μΈµμ—μ„μ κ°€μ¤‘μΉ μ—…λ°μ΄νΈλ” λΈνƒ€ κ·μΉ™κ³Ό κ±°μ λ™μΌν• ν•νƒλ¥Ό κ°€μ§‘λ‹λ‹¤. λ‹¤λ§, λΉ„μ„ ν• ν™μ„±ν™” ν•¨μ(μ: [[Softmax|μ‹κ·Έλ¨μ΄λ“ ν•¨μ]])μ λ―Έλ¶„κ°’μ΄ μ¶”κ°€λ΅ κ³±ν•΄μ§‘λ‹λ‹¤.
-   **μ€λ‹‰μΈµ**: μ€λ‹‰μΈµμ μ¤μ°¨λ” μ§μ ‘ κ³„μ‚°ν•  μ μ—†μΌλ―€λ΅, μ¶λ ¥μΈµμ μ¤μ°¨('λΈνƒ€')λ¥Ό κ°€μ¤‘μΉλ¥Ό μ΄μ©ν•΄ μ—­μΌλ΅ μ „νμ‹μΌ κ³„μ‚°ν•©λ‹λ‹¤. μ΄ κ³„μ‚°λ μ€λ‹‰μΈµμ 'λΈνƒ€'λ¥Ό μ‚¬μ©ν•μ—¬ μ€λ‹‰μΈµμ κ°€μ¤‘μΉλ¥Ό μ—…λ°μ΄νΈν•©λ‹λ‹¤.

κ²°λ΅ μ μΌλ΅, **Sigmoid ν•¨μμ λ―Έλ¶„ κ°€λ¥μ„±**κ³Ό **λΈνƒ€ κ·μΉ™μ μ¤μ°¨ κΈ°λ° κ°€μ¤‘μΉ μ΅°μ • λ°©μ‹**μ€ μ—­μ „ν μ•κ³ λ¦¬μ¦μ ν•µμ‹¬ κµ¬μ„± μ”μ†λ΅ κ²°ν•©λμ–΄, κΉμ€ μ‹ κ²½λ§μ ν•™μµμ„ κ°€λ¥ν•κ² ν•©λ‹λ‹¤.

## π”— κ΄€λ ¨ κ°λ…
-   [[ANN]]
-   [[Perceptron]]
-   [[Backpropagation]]
-   [[Gradient Descent]]
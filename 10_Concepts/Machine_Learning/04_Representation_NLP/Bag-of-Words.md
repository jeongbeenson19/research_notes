---
tags:
  - NLP
  - feature_extraction
aliases:
  - BoW
  - 단어 가방
---

# Bag-of-Words (BoW)

**Bag-of-Words (BoW)**는 자연어 처리(NLP) 및 정보 검색(Information Retrieval) 분야에서 텍스트를 수치 벡터로 표현하는 간단하고 전통적인 방법입니다. 이름 그대로, 텍스트(문서)를 **'단어들의 가방'**으로 간주합니다.

BoW 모델은 텍스트에 포함된 단어들의 순서나 문법적 구조는 모두 무시하고, 오직 **각 단어의 출현 빈도(frequency)**만을 정보로 사용합니다.

## ⚙️ 생성 과정

1.  **어휘 집합(Vocabulary) 생성**: 전체 문서 집합(corpus)에 있는 모든 고유한 단어들의 집합을 만듭니다.
2.  **벡터화 (Vectorization)**: 각 문서를 어휘 집합의 크기만큼의 차원을 가지는 벡터로 변환합니다. 벡터의 각 차원은 어휘 집합의 특정 단어를 나타내며, 값은 해당 단어가 문서에 나타난 횟수를 의미합니다.

### 예시

-   **문서 1**: `John likes to watch movies. Mary likes movies too.`
-   **문서 2**: `John also likes to watch football games.`

**1. 어휘 집합 생성:**
`{ John, likes, to, watch, movies, Mary, too, also, football, games }` (총 10개 단어)

**2. 벡터화:**
-   **문서 1 벡터**: `[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]`
-   **문서 2 벡터**: `[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]`

## ✨ 장점 및 단점

### 장점

-   **단순함과 효율성**: 구현이 매우 간단하고, 텍스트를 빠르고 쉽게 수치 벡터로 변환할 수 있습니다.
-   **준수한 성능**: 많은 텍스트 분류 문제에서 기반 모델(baseline)로서 준수한 성능을 보입니다.

### 단점

-   **어순 무시**: 단어의 순서를 완전히 무시하므로, 문맥과 의미가 손실됩니다. (예: "I love you" vs "You love I" 를 동일하게 취급)
-   **희소성 (Sparsity)**: 어휘 집합의 크기가 커질수록, 각 문서 벡터는 대부분의 값이 0인 희소 벡터(sparse vector)가 됩니다. 이는 계산 비효율성과 메모리 문제를 야기할 수 있습니다.
-   **불용어 문제**: "the", "a", "is" 와 같이 자주 등장하지만 의미는 적은 단어(불용어)들이 높은 값을 갖게 되어, 핵심 단어의 중요도가 희석될 수 있습니다.

## 🔗 관련 개념 및 활용

BoW는 그 자체로 사용되기보다는, 이후에 등장한 더 정교한 텍스트 표현 기법들의 기초가 되는 중요한 개념입니다.

-   **주요 활용 분야**
    -   문서 분류 (Document Classification)
    -   스팸 메일 필터링
    -   감성 분석 (Sentiment Analysis)

-   **관련 개념**
    -   [[TF-IDF]]: BoW의 불용어 문제를 보완하기 위해 단어의 중요도에 가중치를 부여하는 방식입니다.
    -   [[Word Embedding]]: 단어의 의미와 문맥을 벡터 공간에 표현하는 기법으로, [[Word2Vec]], [[GloVe]] 등이 있습니다.
    -   [[Tokenization]]
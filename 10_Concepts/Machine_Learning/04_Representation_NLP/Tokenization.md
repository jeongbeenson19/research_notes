# 토큰화 (Tokenization)

#NLP #Preprocessing

---

## 1. 개요

**토큰화(Tokenization)** 는 자연어 처리(NLP)에서 주어진 텍스트(corpus)를 **토큰(token)** 이라고 불리는 의미 있는 단위의 연속으로 분할하는 과정입니다. 토큰은 NLP 모델이 텍스트를 이해하고 처리하기 위한 가장 기본적인 입력 단위가 됩니다.

- **Corpus:** "I love natural language processing."
- **Tokens:** `["I", "love", "natural", "language", "processing", "."]`

토큰의 단위는 단어, 문자, 또는 '단어의 일부(subword)'가 될 수 있으며, 어떤 단위를 사용하느냐에 따라 모델의 성능과 특성이 크게 달라집니다.

---

## 2. 토큰화의 종류

### 2.1. 단어 수준 토큰화 (Word-level Tokenization)

- **방식:** 텍스트를 공백(space), 쉼표, 마침표 등과 같은 구분자(delimiter)를 기준으로 분할합니다. 가장 직관적인 토큰화 방식입니다.
- **예시:**
    - "An apple a day keeps the doctor away."
    - `["An", "apple", "a", "day", "keeps", "the", "doctor", "away", "."]`
- **한계:**
    - **어휘 집합 크기 증가:** 학습 데이터에 등장하는 모든 고유 단어를 어휘 집합(vocabulary)에 추가해야 하므로, 어휘 집합의 크기가 매우 커질 수 있습니다.
    - **OOV (Out-of-Vocabulary) 문제:** 학습 시점에 어휘 집합에 없었던 새로운 단어(신조어, 오타 등)가 등장하면 모델이 이를 처리할 수 없는 문제가 발생합니다.

### 2.2. 문자 수준 토큰화 (Character-level Tokenization)

- **방식:** 텍스트를 개별 문자 단위로 분할합니다.
- **예시:**
    - "apple"
    - `["a", "p", "p", "l", "e"]`
- **장점:**
    - **OOV 문제 해결:** 모든 단어는 문자의 조합으로 이루어지므로 OOV 문제가 발생하지 않습니다.
    - **어휘 집합 크기 감소:** 어휘 집합의 크기가 문자 수(영어의 경우 알파벳, 특수문자 등)로 매우 작게 유지됩니다.
- **단점:**
    - **의미 손실:** 'a', 'p', 'p', 'l', 'e' 각 문자는 그 자체로 의미를 갖기 어렵습니다. 모델이 단어 수준의 의미를 학습하기 위해 더 많은 계산과 노력이 필요합니다.
    - **시퀀스 길이 증가:** 토큰 시퀀스의 길이가 매우 길어져 계산 비용이 증가하고, 모델이 장기 의존성을 학습하기 어려워집니다.

### 2.3. 단어 하위 단위 토큰화 (Subword-level Tokenization)

단어와 문자 수준 토큰화의 장점을 결합한 방식으로, 현대 NLP 모델에서 가장 널리 사용됩니다. 자주 사용되는 단어는 그대로 유지하고, 자주 사용되지 않는 단어는 의미 있는 하위 단위(subword)로 분할합니다.

- **방식:** 데이터의 통계적 특성을 기반으로 어휘 집합을 구축하고, 단어를 더 작은 단위로 분해합니다.
- **예시:**
    - "unhappiest" → `["un", "happi", "est"]`
    - "tokenization" → `["token", "ization"]`
- **장점:**
    - **OOV 문제 완화:** 처음 보는 단어도 학습된 하위 단위의 조합으로 표현할 수 있어 OOV 문제를 효과적으로 처리합니다. (e.g., "Newest" -> `["New", "est"]`)
    - **어휘 집합 크기 제어:** 어휘 집합의 크기를 합리적인 수준으로 제어할 수 있습니다.
    - **형태소 정보 활용:** 'un-', '-est', '-ization'과 같은 접사나 어미가 분리되어 모델이 단어의 형태학적 의미를 학습하는 데 도움이 됩니다.

#### 대표적인 Subword Tokenization 알고리즘

- **BPE (Byte Pair Encoding):** 가장 빈번하게 등장하는 문자 쌍(byte pair)을 새로운 하나의 토큰으로 병합하는 과정을 반복합니다.
- **WordPiece:** BERT에서 사용되는 방식으로, BPE와 유사하지만 문자 쌍을 병합할 때 전체 코퍼스의 우도(likelihood)를 가장 높이는 쌍을 선택합니다.
- **SentencePiece:** 구글에서 개발한 토크나이저 라이브러리. 텍스트를 공백으로 미리 분리할 필요 없이, 원시 텍스트(raw text)에서 직접 동작하도록 설계되었습니다. BPE 또는 유니그램(Unigram) 모델을 구현체로 사용합니다.

---

## 3. 결론

토큰화는 단순히 텍스트를 나누는 것을 넘어, NLP 모델의 성능, 어휘 집합의 크기, OOV 문제 처리 능력 등을 결정하는 매우 중요한 전처리 단계입니다. 어떤 토큰화 전략을 선택하느냐가 전체 NLP 파이프라인의 성공에 큰 영향을 미칩니다.

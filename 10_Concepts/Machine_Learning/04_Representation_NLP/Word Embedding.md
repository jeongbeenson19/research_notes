---
tags: [nlp, embedding, concept, machine-learning]
aliases: [워드 임베딩, 단어 임베딩]
---

# 단어 임베딩 (Word Embedding)

단어 임베딩은 텍스트를 구성하는 단어(word)들을 컴퓨터가 이해하고 효율적으로 처리할 수 있도록, 실수(real number)로 이루어진 저차원의 **밀집 벡터(dense vector)** 로 변환하는 기술 및 그 결과물을 의미합니다. 자연어 처리(NLP) 모델의 성능을 비약적으로 발전시킨 핵심적인 기반 기술입니다.

---

### **1. 왜 단어 임베딩이 필요한가?**

컴퓨터는 문자를 직접 이해하지 못하므로, 텍스트를 숫자로 변환하는 과정이 필수적입니다. 초기에는 다음과 같은 방식들이 사용되었지만 명확한 한계가 있었습니다.

#### **전통적인 표현 방식: 원-핫 인코딩 (One-Hot Encoding)**

-   **방식**: 전체 단어 집합(vocabulary)의 크기만큼의 차원을 가진 벡터를 만들고, 표현하려는 단어의 인덱스에만 1을, 나머지에는 모두 0을 채우는 방식입니다.
-   **문제점**:
    1.  **차원의 저주 (Curse of Dimensionality)**: 단어 집합의 크기가 커질수록 벡터의 차원이 한없이 커져, 계산 비효율성과 메모리 낭비가 심각해집니다.
    2.  **단어 간 관계 표현 불가**: 모든 단어 벡터가 서로 직교(orthogonal)하기 때문에, "강아지"와 "고양이"가 "책상"보다 의미적으로 더 가깝다는 관계를 전혀 표현하지 못합니다. 단어의 의미를 벡터에 담아내지 못하는 것입니다.

---

### **2. 단어 임베딩의 핵심 아이디어**

단어 임베딩은 **분포 가설(Distributional Hypothesis)** 이라는 언어학적 가정에 기반합니다.

> "단어의 의미는 그 단어 주변에 함께 나타나는 단어들에 의해 결정된다." (A word is characterized by the company it keeps.)

즉, 비슷한 문맥에서 자주 등장하는 단어들은 비슷한 의미를 가질 것이라는 아이디어입니다. 단어 임베딩은 이 가설을 수학적으로 구현하여, **비슷한 의미를 가진 단어들이 벡터 공간상에서 가까운 위치에 존재하도록** 단어들을 벡터로 표현합니다.

#### **벡터 연산을 통한 의미 표현**

잘 학습된 단어 임베딩은 단어 간의 복잡한 의미 관계를 벡터 연산으로 표현할 수 있습니다. 가장 유명한 예시는 다음과 같습니다.

`vector('왕') - vector('남자') + vector('여자') ≈ vector('여왕')`

이는 임베딩 벡터가 단순한 숫자 배열을 넘어, 단어의 의미적, 문법적 관계를 벡터 공간에 성공적으로 투영했음을 보여줍니다.

---

### **3. 주요 단어 임베딩 모델**

#### **3.1. [[Word2Vec]]**

-   Google에서 개발한 예측 기반의 임베딩 모델로, 단어 임베딩의 대중화를 이끌었습니다.
-   **CBOW (Continuous Bag-of-Words)**: 주변 단어들(context words)을 가지고 중심 단어(center word)를 예측하는 방식으로 학습합니다.
-   **Skip-gram**: 중심 단어를 가지고 주변 단어들을 예측하는 방식으로 학습합니다. 일반적으로 CBOW보다 성능이 좋다고 알려져 있습니다.

#### **3.2. [[GloVe]] (Global Vectors for Word Representation)**

-   Stanford에서 개발했으며, Word2Vec과 같은 예측 기반이 아닌 **카운트 기반(count-based)** 모델입니다.
-   전체 말뭉치(corpus)의 단어 동시 등장(co-occurrence) 통계 정보를 미리 계산하고, 이 정보를 기반으로 임베딩을 학습하여 단어 간의 전역적인(global) 관계를 더 잘 반영하는 경향이 있습니다.

#### **3.3. FastText**

-   Facebook에서 개발했으며, Word2Vec의 확장 모델입니다.
-   단어를 통째로 벡터로 만드는 것이 아니라, 단어를 여러 개의 **부분 단어(subword) 또는 문자 n-gram**으로 분해하여 학습합니다. (예: `embedding` -> `emb`, `mbe`, `bed`, `edd`, `ddi`, `din`, `ing`)
-   이러한 특징 덕분에, 학습 데이터에 없었던 **미등록 단어(Out-of-Vocabulary, OOV)**에 대해서도 그 단어를 구성하는 부분 단어 벡터들의 합으로 의미를 유추하여 벡터를 생성할 수 있는 큰 장점이 있습니다.

---

### **4. 문맥을 고려한 임베딩으로의 발전**

Word2Vec, GloVe 등은 단어의 의미를 하나의 고정된 벡터로 표현하기 때문에, 문맥에 따라 의미가 달라지는 **다의어(polysemy)** 문제를 해결할 수 없습니다. (예: "사과"는 과일일 수도, 용서를 구하는 행위일 수도 있음)

이러한 한계를 극복하기 위해, 문장의 전체적인 문맥을 고려하여 단어의 의미를 동적으로 결정하는 **문맥 의존적 임베딩(Contextualized Word Embedding)** 이 등장했습니다.

-   **ELMo (Embeddings from Language Models)**
-   **[[BERT]] (Bidirectional Encoder Representations from Transformers)**
-   **[[GPT]] (Generative Pre-trained Transformer)**

이러한 모델들은 트랜스포머(Transformer) 아키텍처를 기반으로, 같은 단어라도 문맥에 따라 다른 임베딩 벡터를 생성하여 언어 표현의 정확도를 한 차원 더 높은 수준으로 끌어올렸습니다.

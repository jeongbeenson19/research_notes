
# TF-IDF (Term Frequency-Inverse Document Frequency)
#NLP #FeatureExtraction

---

## 1. 개요

**TF-IDF(Term Frequency-Inverse Document Frequency)** 는 여러 문서로 이루어진 문서 집합(corpus)에서 특정 단어가 특정 문서 내에서 얼마나 중요한지를 나타내는 통계적 가중치입니다. 

[[Bag-of-Words]]가 단순히 단어의 출현 빈도만 사용하는 것의 한계를 보완하기 위해 등장했습니다. 즉, 모든 문서에 흔하게 나타나는 단어(e.g., a, the, is)에는 낮은 가중치를 주고, 특정 문서에만 집중적으로 나타나는 단어에는 높은 가중치를 부여하는 방식입니다.

TF-IDF는 **TF(Term Frequency)** 와 **IDF(Inverse Document Frequency)** 두 값의 곱으로 계산됩니다.

`TF-IDF(단어, 문서) = TF(단어, 문서) * IDF(단어, 전체 문서 집합)`

---

## 2. 구성 요소

### 2.1. TF (Term Frequency, 단어 빈도)

- **의미:** 특정 문서 내에서 특정 단어가 얼마나 자주 등장하는지를 나타내는 값.
- **계산:** `TF(t, d) = (문서 d에 단어 t가 나타난 횟수) / (문서 d의 전체 단어 수)`
- **목적:** 한 문서 내에서 자주 나타나는 단어일수록 그 문서를 설명하는 데 중요할 것이라는 가정.

### 2.2. IDF (Inverse Document Frequency, 역문서 빈도)

- **의미:** 특정 단어가 전체 문서 집합에서 얼마나 희귀하게 등장하는지를 나타내는 값. (희귀할수록 값이 커짐)
- **계산:** `IDF(t, D) = log( (전체 문서의 수) / (단어 t를 포함한 문서의 수 + 1) )`
    - 분모에 1을 더하는 것은 특정 단어가 모든 문서에 등장하지 않아 0으로 나누어지는 것을 방지하기 위함입니다. (Smoothing)
- **목적:** 모든 문서에 공통적으로 나타나는 단어(불용어 등)는 변별력이 없다고 보고, 이런 단어들의 중요도를 낮추기 위함.

---

## 3. 장점 및 단점

### 장점
- **단순하고 빠른 계산:** 구현이 간단하고 계산 속도가 빠릅니다.
- **중요도 추출:** 단순히 빈도만 보는 것보다 문서 내에서 핵심적인 역할을 하는 단어를 더 효과적으로 추출할 수 있습니다.
- **불용어 처리:** IDF를 통해 불용어나 관용구처럼 자주 쓰이지만 의미는 적은 단어들의 영향력을 자연스럽게 줄일 수 있습니다.

### 단점
- **의미/문맥 파악 불가:** 단어의 의미적 유사성이나 문장 내 순서를 고려하지 못합니다. (e.g., '사과'와 'apple'을 다른 단어로 취급)
- **새로운 단어에 대한 취약성:** 학습 데이터(문서 집합)에 없던 새로운 단어가 나타나면 IDF 값을 계산할 수 없습니다.
- **문서 길이에 따른 편향:** 긴 문서에서 단어 빈도가 높아지는 경향이 있을 수 있습니다.

---

## 4. 활용

- 정보 검색 및 검색 엔진의 순위 결정
- 문서 요약 및 핵심 단어 추출
- 텍스트 분류 시스템의 특징(feature) 벡터 생성

TF-IDF는 [[Word2Vec]]이나 [[BERT]]와 같은 최신 딥러닝 기반의 임베딩 기술이 나오기 전까지 매우 널리 사용되었으며, 여전히 간단하고 효과적인 텍스트 표현 방법으로 활용되고 있습니다.

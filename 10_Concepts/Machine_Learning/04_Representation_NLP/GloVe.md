---
tags:
  - NLP
  - embedding
aliases:
  - GloVe
---

# GloVe (Global Vectors for Word Representation)

**GloVe(Global Vectors for Word Representation)**는 2014년 스탠포드 대학 연구팀이 개발한 단어 임베딩(Word Embedding) 모델입니다. 이름에서 알 수 있듯이, 단어 표현을 위해 말뭉치(corpus) 전체의 **'전역적(Global)' 통계 정보**를 활용하는 것이 핵심입니다.

GloVe는 [[Word2Vec]]과 같은 지역 문맥 윈도우(local context window) 기반의 예측 방법론과, LSA(Latent Semantic Analysis)와 같은 전역 행렬 분해(global matrix factorization) 방법론의 장점을 결합하고자 했습니다.

## 📖 핵심 아이디어

GloVe의 기본 아이디어는 **"단어 임베딩 벡터 간의 관계가 전체 말뭉치에서 두 단어가 함께 등장하는 빈도(Co-occurrence)의 통계적 관계를 반영해야 한다"**는 것입니다.

-   [[Word2Vec]]이 특정 윈도우 내의 지역적 정보(local information)를 사용하여 주변 단어를 예측하는 방식이라면, GloVe는 말뭉치 전체의 **단어 동시 등장 행렬(Word Co-occurrence Matrix)** 이라는 전역적 통계 정보를 직접적으로 학습에 사용합니다.
-   즉, 임베딩된 공간에서 단어 벡터 간의 내적(dot product)이 두 단어의 동시 등장 확률(log-probability)과 유사해지도록 모델을 학습시킵니다.

## ⚙️ 손실 함수 (Loss Function)

GloVe의 손실 함수는 다음과 같이 정의됩니다.

$$J = Σ ( f(X_ij) * (w_i^T * w̃_j + b_i + b̃_j - log(X_ij))^2 )$$

-   $X_ij$: 단어 `i`와 단어 `j`가 함께 등장한 횟수입니다.
-   $w_i$, $w̃_j$: 각각 중심 단어 `i`와 문맥 단어 `j`의 임베딩 벡터 (학습 대상)입니다.
-   $b_i$, $b̃_j$: 각 단어의 편향(bias) (학습 대상)입니다.
-   $f(X_ij)$: 가중치 함수(weighting function)입니다. 동시 등장 빈도가 너무 높은 단어(e.g., 'the', 'is')의 영향력을 제한하고, 희귀한 단어에 너무 낮은 가중치를 주지 않기 위해 사용됩니다.

이 손실 함수는 임베딩 벡터의 내적이 두 단어의 동시 등장 빈도의 로그값에 가까워지도록 모델을 학습시킵니다.

## ✨ Word2Vec과의 비교

| 구분        | GloVe                                        | Word2Vec (Skip-gram)                         |
| :---------- | :------------------------------------------- | :------------------------------------------- |
| **학습 방식**   | 카운트 기반 (Count-based)                    | 예측 기반 (Prediction-based)                 |
| **활용 정보**   | 전역 통계 정보 (Global Co-occurrence Matrix) | 지역 문맥 정보 (Local Context Window)        |
| **학습 속도**   | 일반적으로 더 빠름 (행렬을 한 번만 구축)      | 데이터가 클 경우 느릴 수 있음                |
| **성능**        | 작은 데이터셋이나 희귀 단어에 대해 더 강건한 경향 | 큰 데이터셋에서 유사하거나 더 나은 성능을 보임 |

실질적으로 두 모델의 성능은 태스크, 데이터셋, 하이퍼파라미터 튜닝에 따라 달라지며, 어느 하나가 절대적으로 우월하다고 말하기는 어렵습니다.

## 🔗 관련 개념

-   [[Word Embedding]]
-   [[Word2Vec]]
## 🪙 단어 동시 등장 행렬 (Co-occurrence Matrix)

**단어 동시 등장 행렬(Co-occurrence Matrix)** 은 말뭉치(corpus)에 있는 단어들이 서로 얼마나 '가까이'에서 함께 등장하는지를 나타내는 행렬입니다. 이 행렬은 GloVe가 전역적인 통계 정보를 학습하는 데 사용하는 핵심적인 데이터 구조입니다.

### 구축 과정

1.  **어휘집(Vocabulary) 정의**: 말뭉치에 있는 모든 고유한 단어들의 집합을 만듭니다. 이 어휘집의 크기가 행렬의 차원(V x V)을 결정합니다.
2.  **윈도우 크기(Window Size) 설정**: 중심 단어(center word)를 기준으로 좌우 몇 개의 단어를 '문맥(context)'으로 볼 것인지 결정합니다. 예를 들어 윈도우 크기가 2이면, 중심 단어의 좌우 2개 단어까지를 문맥 단어로 봅니다.
3.  **행렬 채우기**:
    *   말뭉치의 모든 단어를 순회하면서 중심 단어로 설정합니다.
    *   해당 중심 단어의 윈도우 내에 등장하는 각 문맥 단어(context word)와의 동시 등장 횟수를 셉니다.
    *   행렬에서 (중심 단어, 문맥 단어)에 해당하는 위치의 값을 1 증가시킵니다.

**가중치 부여**: GloVe에서는 중심 단어와 문맥 단어 사이의 거리에 따라 다른 가중치를 부여하기도 합니다. 예를 들어, 중심 단어와 가까울수록 높은 가중치(e.g., 1/distance)를 주어 더 강한 연관성을 갖도록 할 수 있습니다.

### 예시

다음과 같은 문장이 있고, 윈도우 크기가 1이라고 가정해 봅시다.

> "I love NLP and I love deep learning"

어휘집: `{"I", "love", "NLP", "and", "deep", "learning"}`

| | I | love | NLP | and | deep | learning |
| :--- | :-: | :--: | :--: | :--: | :--: | :---: |
| **I** | 0 | 2 | 0 | 0 | 0 | 0 |
| **love** | 2 | 0 | 1 | 0 | 1 | 0 |
| **NLP** | 0 | 1 | 0 | 1 | 0 | 0 |
| **and** | 0 | 0 | 1 | 0 | 0 | 0 |
| **deep** | 0 | 1 | 0 | 0 | 0 | 1 |
| **learning**| 0 | 0 | 0 | 0 | 1 | 0 |

-   `love`는 `I`와 두 번, `NLP`와 한 번, `deep`과 한 번씩 인접해 있으므로 각 셀에 해당 횟수가 기록됩니다.
-   이 행렬은 대칭 행렬(symmetric matrix)이 됩니다.

### GloVe에서의 역할

GloVe는 이 동시 등장 행렬의 정보를 손실 함수에 직접 활용합니다. 구체적으로, 임베딩된 두 단어 벡터의 내적이 동시 등장 횟수($X_{ij}$)의 로그값과 유사해지도록 학습함으로써, 단어 벡터 공간에 말뭉치 전체의 통계적 패턴을 녹여냅니다. 이는 지역적인 문맥만 보는 Word2Vec과 달리, 전역적인 단어 관계를 학습할 수 있게 해주는 핵심적인 메커니즘입니다.

-   [[Latent Semantic Analysis|LSA]]
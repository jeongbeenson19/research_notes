# Vanishing Gradient Problem

## 한줄 요약
깊은 네트워크에서 기울기가 점점 작아져 학습이 어려워지는 현상.

## 핵심 개념
- 시그모이드/탄흐 같은 활성화 함수에서 자주 발생
- 깊어질수록 초기 층 업데이트가 미약
- [[LSTM]], [[GRU]] 등으로 완화

## 대표 증상
- 학습 초반 손실이 거의 줄지 않음
- 초기 층 가중치 변화가 미미

## 실무 팁/주의점
- ReLU 계열 활성화와 적절한 초기화 사용
- 스킵 연결(ResNet)으로 완화

## 관련 노트
- [[RNN]]
- [[LSTM]]
- [[GRU]]
- [[Backpropagation]]

아래 목록은 **CVPR 2025 메인 컨퍼런스(virtual poster/accepted list)에 실제로 올라온 항목들** 중, 제목/초록/트랙 표기에서 **Video Understanding(특히 Video-LLM, Long-form Video, VideoQA/Reasoning/Temporal Grounding)** 축에 확실히 걸리는 것들을 **CVPR 2025 공식 사이트(accepted papers / virtual poster 페이지)와 CVF OpenAccess 페이지**를 기반으로 모은 것입니다. 

---

## **Long-form Video Understanding / 효율화(프레임·토큰·메모리·검색)**

- **Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.pdf
    
- **BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_BOLT_Boost_Large_Vision-Language_Model_Without_Training_for_Long-form_Video_CVPR_2025_paper.pdf
    
- **Re-thinking Temporal Search for Long-Form Video Understanding** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_Re-thinking_Temporal_Search_for_Long-Form_Video_Understanding_CVPR_2025_paper.pdf
    
- **ReWind: Understanding Long Videos with Instructed Learnable Memory** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Diko_ReWind_Understanding_Long_Videos_with_Instructed_Learnable_Memory_CVPR_2025_paper.pdf
    
- **VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VideoTree_Adaptive_Tree-based_Video_Representation_for_LLM_Reasoning_on_Long_CVPR_2025_paper.pdf
    
- **Adaptive Keyframe Sampling for Long Video Understanding** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Adaptive_Keyframe_Sampling_for_Long_Video_Understanding_CVPR_2025_paper.pdf
    
- **M-LLM Based Video Frame Selection for Efficient Video Understanding** 
  - 핵심 기여: 프레임/토큰 선택 또는 압축을 통해 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Hu_M-LLM_Based_Video_Frame_Selection_for_Efficient_Video_Understanding_CVPR_2025_paper.pdf
    
- **Flexible Frame Selection for Efficient Video Reasoning** 
  - 핵심 기여: 프레임/토큰 선택 또는 압축을 통해 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Buch_Flexible_Frame_Selection_for_Efficient_Video_Reasoning_CVPR_2025_paper.pdf
    
- **AdaCM²: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_CVPR_2025_paper.pdf
    
- **BIMBA: Selective-Scan Compression for Long-Range Video Question Answering** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Islam_BIMBA_Selective-Scan_Compression_for_Long-Range_Video_Question_Answering_CVPR_2025_paper.pdf
    
- **Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Jang_Efficient_Long_Video_Tokenization_via_Coordinate-based_Patch_Reconstruction_CVPR_2025_paper.pdf
    
- **DrVideo: Document Retrieval Based Long Video Understanding** 
  - 핵심 기여: 긴 비디오 이해를 위해 메모리/토큰/검색 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_DrVideo_Document_Retrieval_Based_Long_Video_Understanding_CVPR_2025_paper.pdf
    
- **Efficient Chaptering in Hour-Long Videos with LLMs (Chapter-Llama)** 
  - 핵심 기여: 긴 비디오를 장(Chapter) 단위로 분절하는 효율적 이해 파이프라인을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Ventura_Chapter-Llama_Efficient_Chaptering_in_Hour-Long_Videos_with_LLMs_CVPR_2025_paper.pdf
    

---

## **Benchmarks / 평가셋(긴 비디오·온라인·환각·전문지식 등)**

- **MLVU: Benchmarking Multi-task Long Video Understanding** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_MLVU_Benchmarking_Multi-task_Long_Video_Understanding_CVPR_2025_paper.pdf
    
- **Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.pdf
    
- **MMVU: Measuring Expert-Level Multi-Discipline Video Understanding** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_MMVU_Measuring_Expert-Level_Multi-Discipline_Video_Understanding_CVPR_2025_paper.pdf
    
- **OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Niu_OVO-Bench_How_Far_is_Your_Video-LLMs_from_Real-World_Online_Video_CVPR_2025_paper.pdf
    
- **Online Video Understanding: OVBench and VideoChat-Online** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Online_Video_Understanding_OVBench_and_VideoChat-Online_CVPR_2025_paper.pdf
    
- **VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VidHalluc_Evaluating_Temporal_Hallucinations_in_Multimodal_Large_Language_Models_for_CVPR_2025_paper.pdf
    
- **VEU-Bench: Towards Comprehensive Understanding of Video Editing** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.pdf
    
- **EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_EgoTextVQA_Towards_Egocentric_Scene-Text_Aware_Video_Question_Answering_CVPR_2025_paper.pdf
    
- **Efficient Motion-Aware Video MLLM (EMA) + MotionBench** 
  - 핵심 기여: 새로운 벤치마크/데이터셋과 평가 프로토콜을 제안해 특정 문제군을 체계적으로 측정.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Efficient_Motion-Aware_Video_MLLM_CVPR_2025_paper.pdf
    

---

## **VideoQA / Video Reasoning / Temporal Grounding / Reasoning Segmentation**

- **Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Commonsense_Video_Question_Answering_through_Video-Grounded_Entailment_Tree_Reasoning_CVPR_2025_paper.pdf
    
- **Cross-modal Causal Relation Alignment for Video Question Grounding** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding_CVPR_2025_paper.pdf
    
- **VITED: Video Temporal Evidence Distillation** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_VITED_Video_Temporal_Evidence_Distillation_CVPR_2025_paper.pdf
    
- **VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection** 
  - 핵심 기여: 프레임/토큰 선택 또는 압축을 통해 효율을 높이는 방법을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.pdf
    
- **Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Enhancing_Video-LLM_Reasoning_via_Agent-of-Thoughts_Distillation_CVPR_2025_paper.pdf
    
- **Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Deng_Motion-Grounded_Video_Reasoning_Understanding_and_Perceiving_Motion_at_Pixel_Level_CVPR_2025_paper.pdf
    
- **The Devil is in Temporal Token: High Quality Video Reasoning Segmentation** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Gong_The_Devil_is_in_Temporal_Token_High_Quality_Video_Reasoning_CVPR_2025_paper.pdf
    
- **Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding** 
  - 핵심 기여: 비디오 QA/추론을 위한 증거 추출 및 추론 구조를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Deng_Seq2Time_Sequential_Knowledge_Transfer_for_Video_LLM_Temporal_Grounding_CVPR_2025_paper.pdf
    

---

## **Video-LLM 구성요소/시스템(스트리밍, 오브젝트 이해 등)**

- **Apollo: An Exploration of Video Understanding in Large Multimodal Models** 
  - 핵심 기여: 비디오 이해 성능을 향상시키는 새로운 모델/학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Zohar_Apollo__An_Exploration_of_Video_Understanding_in_Large_Multimodal_CVPR_2025_paper.pdf
    
- **STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding** 
  - 핵심 기여: 비디오 이해 성능을 향상시키는 새로운 모델/학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_STOP_Integrated_Spatial-Temporal_Dynamic_Prompting_for_Video_Understanding_CVPR_2025_paper.pdf
    
- **DynFocus: Dynamic Cooperative Network Empowers LLMs with Video Understanding** 
  - 핵심 기여: 비디오-LLM의 정렬/추론/적응을 개선하는 구성요소 또는 학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Han_DynFocus_Dynamic_Cooperative_Network_Empowers_LLMs_with_Video_Understanding_CVPR_2025_paper.pdf
    
- **VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding** 
  - 핵심 기여: 비디오 이해 성능을 향상시키는 새로운 모델/학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_VideoICL_Confidence-based_Iterative_In-context_Learning_for_Out-of-Distribution_Video_Understanding_CVPR_2025_paper.pdf
    
- **LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale** 
  - 핵심 기여: 비디오-LLM의 정렬/추론/적응을 개선하는 구성요소 또는 학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_LiveCC_Learning_Video_LLM_with_Streaming_Speech_Transcription_at_Scale_CVPR_2025_paper.pdf
    
- **VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM** 
  - 핵심 기여: 비디오-LLM의 정렬/추론/적응을 개선하는 구성요소 또는 학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan_VideoRefer_Suite_Advancing_Spatial-Temporal_Object_Understanding_with_Video_LLM_CVPR_2025_paper.pdf
    
- **Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding** 
  - 핵심 기여: 비디오-LLM의 정렬/추론/적응을 개선하는 구성요소 또는 학습 전략을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_Video-3D_LLM_Learning_Position-Aware_Video_Representation_for_3D_Scene_Understanding_CVPR_2025_paper.pdf
    

---

## **도메인/확장 축**

- **Towards Universal Soccer Video Understanding** 
  - 핵심 기여: 도메인 특화 비디오 이해(스포츠) 문제를 위한 모델/데이터를 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Rao_Towards_Universal_Soccer_Video_Understanding_CVPR_2025_paper.pdf
    

---

## **4D/Point Cloud “Video Understanding”**

- **Mamba4D: Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models** 
  - 핵심 기여: 4D 포인트클라우드 기반 비디오 이해를 위한 시공간 모델을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Mamba4D_Efficient_4D_Point_Cloud_Video_Understanding_with_Disentangled_Spatial-Temporal_CVPR_2025_paper.pdf
    
- **Adapting Pre-trained 3D Models for Point Cloud Video Understanding via Cross-frame Spatio-temporal Perception** 
  - 핵심 기여: 4D 포인트클라우드 기반 비디오 이해를 위한 시공간 모델을 제안.
  - 링크: https://openaccess.thecvf.com/content/CVPR2025/papers/Lv_Adapting_Pre-trained_3D_Models_for_Point_Cloud_Video_Understanding_via_CVPR_2025_paper.pdf
    

---

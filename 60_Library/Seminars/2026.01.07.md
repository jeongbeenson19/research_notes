# 세미나 정리: Attention & Transformer (슬라이드 순서 상세)

## 출처
- 발표자료: (2026.01.07_손정빈) Attention & Transformer
- 논문: *Attention Is All You Need* (Vaswani et al., 2017)
- 논문: *An Image Is Worth 16x16 Words* (Dosovitskiy et al., 2021)

## 0. 표지
- 주제: Attention & Transformer
- 참고 논문: 1706.03762, 2010.11929
- 발표자: 손정빈

## 1. Background & Motivation
- RNN/LSTM/GRU의 구조적 한계:
  - **내재적 계산 병목**: 순차적으로 $h_t$를 계산해야 하므로, 모든 시간 스텝이 직렬로 연결됨.
  - **병렬화 불가**: $h_t$가 $h_{t-1}$에 의존해 GPU 병렬 계산이 제한됨.
  - **기울기 소실/폭주**: 긴 시퀀스에서 연쇄 곱이 누적되어 기울기가 사라지거나 커짐.
  - **장거리 의존 학습 어려움**: 정보가 $O(n)$ 경로를 거쳐 전달되어 먼 토큰 정보가 희석됨.
- 결론: 순차 모델을 대체할 병렬 가능한 구조가 필요
## 2. Model Architecture (Transformer)
- Encoder–Decoder 프레임워크
- Encoder 6층, Decoder 6층
- Encoder 구성:
  - Multi-Head Self-Attention (MHSA)
  - Position-wise FFN
- Decoder 구성:
  - Masked MHSA
  - Encoder–Decoder Attention (Cross-Attention)
  - Position-wise FFN
- 공통 요소:
  - Residual Connection
  - Layer Normalization

## 3. Attention Mechanism (개념)
- Self-Attention 정의:
  - 모든 위치 쌍 (i, j)의 관계를 계산
  - i의 표현을 j들의 가중합으로 갱신
- 기본 연산:
  - **weighted sum of values**
  - softmax가 “얼마나 볼지”를 결정

## 4. Attention Mechanism (Q/K/V)
- Q/K/V는 입력 특징을 서로 다른 역할로 **선형 투영**한 것
  - $Q = XW_Q$, $K = XW_K$, $V = XW_V$
- Multi-Head:
  - 여러 세트의 Q/K/V를 병렬로 사용
  - 서로 다른 표현 하위공간의 관계 학습
  - 단일 head의 표현 손실을 완화

## 5. Positional Encoding (개요)
- Transformer는 recurrence/convolution이 없어 **순서 정보 없음**
- 해결: 입력 임베딩에 위치 인코딩을 더함
- 목표: 위치 정보를 직접 주입하면서 상대 거리 학습 가능

## 6. Positional Encoding (덧셈 정리)
- 사인/코사인 덧셈 정리로 위치 이동을 **선형 변환**으로 표현 가능
- 회전 행렬 형태:
$$  \[PE_{pos+k,2i}, PE_{pos+k,2i+1}]^T
   = R(\omega_i k) [PE_{pos,2i}, PE_{pos,2i+1}]^T$$
- 전개 과정(회전행렬 유도):
  $$ \sin(\omega_i(pos+k)) = \sin(\omega_i pos)\cos(\omega_i k) + \cos(\omega_i pos)\sin(\omega_i k)$$
  - $$\cos(\omega_i(pos+k)) = \cos(\omega_i pos)\cos(\omega_i k) - \sin(\omega_i pos)\sin(\omega_i k)$$
  - 위 두 식을 벡터로 모으면 $[PE_{pos+k,2i}, PE_{pos+k,2i+1}]^T = R(\omega_i k)[PE_{pos,2i}, PE_{pos,2i+1}]^T$
  - $R(\omega_i k)$는 2D 회전행렬로, 위치 이동이 **회전 변환**으로 표현됨
- 의미:
  - 모델이 **절대 위치값을 몰라도 상대 거리 k**를 학습하기 쉬움

## 7. Applications of Attention
- Cross-Attention(Encoder–Decoder Attention):
  - Query는 Decoder에서 생성
  - Key/Value는 Encoder 출력 사용
- Self-Attention:
  - Encoder는 동일 시퀀스 내부
  - Decoder도 동일 시퀀스 내부
- Decoder는 auto-regressive 속성 때문에 **Masking** 필요

## 8. Vision Transformer (ViT)
- 이미지를 패치로 나눈 뒤 시퀀스로 변환
- Transformer 인코더를 그대로 적용

## 9. Experiments (ViT: 규모 의존성)
- 데이터셋이 커질수록 ResNet 대비 성능이 더 크게 상승
- 대규모 데이터가 ViT 성능에 핵심적

## 10. Experiments (ViT: 계산 효율성)
- 더 나은 성능을 달성하면서도 **계산 자원 요구가 낮음**

## 11. Experiments (ViT: 벤치마크 성능)
- JFT-300M 사전학습 모델이 여러 벤치마크에서 SOTA 달성

## 12. Interpretability (저수준 특징)
- ViT 초기 레이어는 CNN과 유사하게
  - 색상, 엣지 등 저수준 특징을 학습

## 13. Interpretability (위치 임베딩)
- 가까운 패치, 같은 행/열 패치일수록 위치 임베딩 유사도가 높음
- 모델이 2D 구조를 스스로 학습했음을 시사

## 14. Interpretability (헤드 분석)
- 일부 헤드는 로컬 영역, 일부는 전역 영역을 봄
- 레이어가 깊어질수록 전역 정보 활용 증가
- CNN의 구조적 제약 없이도 공간 구조를 학습 가능

## 15. Impact & Significance
- CNN이 필수적이지 않음을 입증 → 패러다임 전환
- 단일 아키텍처가 다양한 모달리티에 적용될 가능성
- **Scale**이 설계 원칙으로 중요하다는 점 강조

## 16. Research Direction
- Object Detection, Segmentation, Video Understanding 확장
- ViT 기반 자기지도 학습 연구 확산
